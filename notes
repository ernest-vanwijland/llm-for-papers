Technique pour optimiser les prompts:
je veux trouver la meilleure prompt pour optimiser un probleme, on fixe le pb

J'ai un juge qui quand je lui donne une solution dit si c'est bon ou pas bon
qd t'entraines un modele ce qu'il se passe c'est que tu donnes en entree des token
ou des mots, et ces mots sont geles, et la solution que tu veux est gelee

tu modifies les poids du modele dans ta backpropagation pour avoir les meilleurs
poids possibles

on gele les poids au milieu, si on gele tout il se passe plus rien
mais si on degele la prompt (pas la partie description du probleme)
on peut backpropager et updater les token d'entree plutot que le modele
(donc optimiser la prompt)

on peut faire trois choses :
premiere chose : hard juge couteux et soft juge qui donne du feedback
pour n'importe quel probleme en entree, je peux optimiser la prompt

deuxieme truc :
partie fixee de la prompt (le pb), l'enrobage autour il est pas frozen,
si tu le fais pour un certain pb tu decouvres un enrobage, pour un autre
pb un autre enrobage, en prenant un echantillion de pbs sur lesquels on
fait ces iterations, on peut avoir un enrobage qu'on optimise sur tous les pb
en meme temps

troisieme truc : plus ambitieuse
backpropager a la fois sur les prompts et sur les solutions input des noeuds
ca definit des nouvelles target et on peut backpropager plus haut

soft tokens, hard tokens
f : mots -> tokens
hard token : f-1 est un mot

backpropagation ne distingue pas entre soft et hard tokens
pb a gerer
















































