{
  "paper": "2502.08328",
  "toprove": 27,
  "validity": 0,
  "comment": "error",
  "proof": "Let $\\mathcal{E}$ be the event that there exists $F \\in \\Omega^{\\mathrm{dis}}$ and $\\gamma \\in \\Gamma^{\\mathrm{dis}}(F)$ such that $|V_G(\\gamma)| \\ge \\frac{A \\log n}{d}$ and $w^{\\mathrm{dis}}(\\gamma) > q^{1-|V_G(\\gamma)|/10}$. We seek to show $P(\\mathcal{E}) \\to 0$ by a union bound over all possible contours $\\gamma$.\n\nLet $s = |\\gamma|$ be the size of the support of a contour $\\gamma$. From the model's definition, the weight $w^{\\mathrm{dis}}(\\gamma)$ is of the form $\\exp(-c(\\beta, d) s)$ where $c(\\beta, d) = \\Theta(d)$. For simplicity, we write $w^{\\mathrm{dis}}(\\gamma) = \\exp(-c_1 d s)$ for some constant $c_1 > 0$. The number of contours of size $s$ is at most $\\binom{n}{s} \\alpha^s$ for some constant $\\alpha > 0$.\n\nLet $k = |V_G(\\gamma)|$. The condition $w^{\\mathrm{dis}}(\\gamma) > q^{1-k/10}$ (for $0 < q < 1$) implies $\\exp(-c_1 d s) > q^{1-k/10}$. Taking logarithms, we get $-c_1 d s > (1-k/10)\\log q$, which simplifies to $s < \\frac{|\\log q|}{c_1 d} (k/10 - 1)$. This requires $k > 10$. Let $c_2 = \\frac{|\\log q|}{10 c_1}$. The condition becomes $s < \\frac{c_2}{d}(k - 10)$.\n\nLet $\\mathcal{E}_{\\gamma}$ be the event that $\\gamma$ is \"bad\", i.e., $k(\\gamma, G) \\ge k_0 := \\frac{A \\log n}{d}$ and $s < \\frac{c_2}{d}(k(\\gamma, G) - 10)$. The second inequality is equivalent to $k(\\gamma, G) > \\frac{sd}{c_2} + 10$. Thus, $\\mathcal{E}_{\\gamma}$ is the event that $k(\\gamma, G) \\ge T_s := \\max(k_0, \\frac{sd}{c_2} + 10)$.\n\nThe random variable $k(\\gamma, G) = |V_G(\\gamma)|$ is the number of vertices in the support of $\\gamma$ with at least one neighbor outside the support. For a fixed support of size $s$, this variable follows a binomial distribution $Bin(s, p_d)$, where $p_d = 1 - (1-d/n)^{n-s} \\approx 1-e^{-d}$. Let $\\mu_s = s p_d$ and $\\sigma_s^2 = s p_d(1-p_d)$ be its mean and variance.\n\nThe probability of the bad event is bounded by the expected number of bad contours:\n$P(\\mathcal{E}) \\le \\sum_{s=1}^n \\binom{n}{s} \\alpha^s P\\left(Bin(s, p_d) \\ge T_s\\right)$.\n\nTo bound the binomial tail probability, we use a normal approximation. Let $X \\sim Bin(s, p_d)$. The distribution of $(X-\\mu_s)/\\sigma_s$ is close to a standard normal distribution $Z \\sim N(0,1)$. The tail probability $P(Z > z)$ is bounded by $e^{-z^2/2}$.\nThus, $P(X \\ge T_s) = P\\left(\\frac{X-\\mu_s}{\\sigma_s} \\ge \\frac{T_s-\\mu_s}{\\sigma_s}\\right) \\le \\exp\\left(-\\frac{(T_s-\\mu_s)^2}{2\\sigma_s^2}\\right)$.\nThis gives $P(X \\ge T_s) \\le \\exp\\left(-\\frac{(T_s-sp_d)^2}{2sp_d(1-p_d)}\\right)$.\n\nLet's analyze the exponent. For large $d$, $p_d \\approx 1$ and $1-p_d \\approx e^{-d}$.\nThe threshold $T_s$ is at least $\\frac{sd}{c_2}$, which is much larger than the mean $\\mu_s \\approx s$.\nSo the term $(T_s-sp_d)^2$ is at least $(\\frac{sd}{c_2} - s)^2 = s^2(d/c_2-1)^2 \\approx s^2(d/c_2)^2$.\nThe exponent is approximately $-\\frac{s^2(d/c_2)^2}{2s e^{-d}} = -s \\frac{d^2}{2c_2^2}e^d$. Let this be $-s C_d$, where $C_d$ is a very large constant.\n\nSo, $P(\\mathcal{E}) \\le \\sum_{s=1}^n \\binom{n}{s} \\alpha^s e^{-s C_d}$.\nWe can bound this sum by $\\sum_{s=1}^n \\left(\\frac{ne\\alpha}{s}\\right)^s e^{-s C_d} = \\sum_{s=1}^n \\left(\\frac{ne\\alpha e^{-C_d}}{s}\\right)^s$.\nSince $C_d$ grows very fast with $d$, for $d$ sufficiently large we have $ne\\alpha e^{-C_d} < 1$.\nThe term in the parenthesis is less than $1/s$, and the sum converges to a value that tends to 0 as $n \\to \\infty$.\nFor instance, for $s=1$, the term is $ne\\alpha e^{-C_d}$, which goes to 0. For $s \\ge 1$, the terms are even smaller.\nThus, $P(\\mathcal{E}) \\to 0$. This completes the proof.",
  "timestamp": "2025-09-15T13:27:28.834136"
}