\documentclass[12pt]{article}
\usepackage[margin=1in, centering]{geometry}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{dsfont}
\usepackage{amssymb}
\usepackage{libertine}
\usepackage{newtxmath}
\usepackage[table]{xcolor}
\usepackage{esvect}
\usepackage{subcaption}
\usepackage{tikz}
\usetikzlibrary{angles,quotes}
\usepackage[normalem]{ulem}
\definecolor{TSUYUKUSA}{RGB}{46, 169, 223}
\definecolor{KURENAI}{RGB}{203, 27, 69}
\usepackage[pdfstartview=FitH,pdfpagemode=UseNone,colorlinks=true,citecolor=KURENAI,linkcolor=TSUYUKUSA,backref=page,linktocpage=true]{hyperref}
\usepackage[capitalise]{cleveref}
\usepackage[nottoc]{tocbibind}
\usepackage{appendix}
\usepackage{makecell}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{remark}[theorem]{Remark}
\newcommand{\fig}[1]{\hyperref[fig:#1]{Figure~\ref*{fig:#1}}}
\newcommand{\eq}[1]{\hyperref[eq:#1]{(\ref*{eq:#1})}}
\newcommand{\lem}[1]{\hyperref[lem:#1]{Lemma~\ref*{lem:#1}}}
\newcommand{\thm}[1]{\hyperref[thm:#1]{Theorem~\ref*{thm:#1}}}
\newcommand{\defi}[1]{\hyperref[def:#1]{Definition~\ref*{def:#1}}}
\newcommand{\app}[1]{\hyperref[sec:#1]{Appendix~\ref*{sec:#1}}}
\newcommand{\fct}[1]{\hyperref[fact:#1]{Fact~\ref*{fact:#1}}}
\newcommand{\sect}[1]{\hyperref[sec:#1]{Section~\ref*{sec:#1}}}
\newcommand{\subsec}[1]{\hyperref[subsec:#1]{Subsection~\ref*{subsec:#1}}}
\newcommand{\itm}[2]{\hyperref[itm:#1]{#2}}
\newcommand{\clm}[1]{\hyperref[clm:#1]{Claim~\ref*{clm:#1}}}
\newcommand{\rmk}[1]{\hyperref[rmk:#1]{Remark~\ref*{rmk:#1}}}
\newcommand{\tbl}[1]{\hyperref[tbl:#1]{Table~\ref*{tbl:#1}}}
\setcounter{tocdepth}{2}

\makeatletter
\newtheorem*{rep@theorem}{\rep@title}
\newcommand{\newreptheorem}[2]{\newenvironment{rep#1}[1]{\def\rep@title{#2 \ref*{##1}}\begin{rep@theorem}}{\end{rep@theorem}}}
\makeatother

\newreptheorem{fact}{Fact}


\definecolor{lightcyan}{RGB}{0.88,1,1}
\definecolor{darkgreen}{RGB}{0, 128, 0}
\definecolor{darkblue}{RGB}{0, 0, 128}
\newcommand{\tablecolor}{lightcyan}
\newcommand{\hlight}[1]{\textcolor{darkgreen}{#1}}

\newcommand{\urls}[1]{\href{#1}{#1}}

\newcommand{\N}{\mathbb{N}}  \newcommand{\R}{\mathbb{R}} \newcommand{\C}{\mathbb{C}} \newcommand{\U}{\mathbb{U}} \renewcommand{\d}{\mathrm{d}} \DeclareMathOperator*{\E}{\mathbb{E}}  \newcommand{\so}{\mathrm{SO}} \newcommand{\s}{\mathrm{S}} \newcommand{\su}{\mathrm{SU}} \renewcommand{\i}{\mathrm{i}} \newcommand{\A}{\mathcal{A}}  \newcommand{\B}{\mathcal{B}} \newcommand{\CC}{\mathcal{C}} \newcommand{\D}{\mathcal{D}} \newcommand{\F}{\mathcal{F}} \renewcommand{\H}{\mathcal{H}} \newcommand{\K}{\mathcal{K}} \newcommand{\NN}{\mathcal{N}} \newcommand{\V}{\mathcal{V}} \newcommand{\X}{\mathcal{X}} \newcommand{\Y}{\mathcal{Y}} \renewcommand{\S}{\mathcal{S}} \newcommand{\SR}{\mathcal{S}_{\R}} \newcommand{\SC}{\mathcal{S}_{\C}} \newcommand{\EE}{\mathcal{E}}  \newcommand{\PP}{\mathcal{P}} \newcommand{\KK}{\widetilde{K}} \newcommand{\LL}{\widetilde{L}} \newcommand{\W}{\widehat{W}} \newcommand{\f}{\hat{f}} \newcommand{\g}{\hat{g}} \newcommand{\h}{\hat{h}} \newcommand{\bit}[1]{\{0,1\}^{#1}} \newcommand{\wrt}{w.r.t.~} \newcommand{\us}{\overset{\$}{\leftarrow}} \newcommand{\set}[1]{\left\{#1\right\}} \newcommand{\lhs}{\mathrm{LHS}} \newcommand{\expec}[1]{\E\!\Br{#1}} \newcommand{\expect}[2]{\E_{\substack{#1}}\!\Br{#2}} \newcommand{\prob}[2]{\underset{#1}{\mathrm{Pr}}\!\Br{#2}} \newcommand{\cf}{\widetilde{f}} \newcommand{\cg}{\widetilde{g}} \newcommand{\ch}{\widetilde{h}} \newcommand{\ck}{\widetilde{K}} \newcommand{\rep}[2]{\br{#1}_{#2}} \newcommand{\AND}[1]{\mathrm{AND}\!\br{#1}}
\newcommand{\sign}[1]{\mathrm{sign}\!\br{#1}}
\newcommand{\grad}[2]{\nabla^{#1}{#2}}

\newcommand{\br}[1]{\left(#1\right)} \newcommand{\Br}[1]{\left[#1\right]} \newcommand{\st}[1]{\left\{#1\right\}} \newcommand{\tr}[1]{\mathrm{Tr}\!\Br{#1}} \newcommand{\abs}[1]{\left|#1 \right|} \newcommand{\norm}[1]{\left\lVert #1 \right\rVert} \newcommand{\agl}[2]{\theta^{\br{#1}}_{#2}} \newcommand{\aglp}[2]{{\theta'}^{\br{#1}}_{#2}} \newcommand{\lint}[1]{\left\lfloor#1\right\rfloor} \newcommand{\poly}[1]{\mathrm{poly}\!\br{#1}} \newcommand{\negl}[1]{\mathrm{negl}\!\br{#1}} \newcommand{\de}[1]{\mathrm{d}#1} \newcommand{\val}[1]{\mathrm{val}\!\br{#1}} \newcommand{\vall}[1]{\mathrm{val}\br{#1}} \newcommand{\nd}[1]{\mathcal{N}\!\br{#1}} \newcommand{\ketbratwo}[2]{\ket{#1} \hspace{-0.4em}\bra{#2}} \newcommand{\ketbra}[1]{\ketbratwo{#1}{#1}} \newcommand{\id}{\ensuremath{\mathds{1}}} \newcommand{\ogroup}[1]{\mathrm{O}\!\br{#1}} \newcommand{\ugroup}[1]{\mathrm{U}\!\br{#1}} \newcommand{\td}{\mathrm{TD}} \newcommand{\tv}[1]{\norm{#1}_{\mathrm{TV}}} \newcommand {\defeq} {\ensuremath{ \stackrel{\mathrm{def}}{=} }} \newcommand{\vdim}{\ensuremath{N}} \newcommand{\dimin}{\ensuremath{n}} \newcommand{\dimout}{\ensuremath{m}} \newcommand{\ncopy}{\ell} \newcommand{\hspacein}{\H_\mathrm{in}} \newcommand{\hspaceout}{\H_\mathrm{out}} \newcommand{\Sin}{\S(\hspacein)} \newcommand{\Sout}{\S(\hspaceout)} \newcommand{\haar}{\ensuremath{\mu}} \newcommand{\tensorhaar}{\ensuremath{\eta}} \newcommand{\tensorsrss}{\ensuremath{\nu}} \newcommand{\qadvice}{\ensuremath{\rho}} \newcommand{\tp}{\otimes} \newcommand{\wone}[2]{W_1\!\br{#1,#2}} 




\usepackage[normalem]{ulem}
\newcommand{\todo}[1]{\noindent \textcolor{red}{$\triangleright$ \textbf{TODO: #1} $-$\emph{Mingnan}}}
\newcommand{\com}[1]{\textcolor{brown}{(#1)}}
\newcommand{\mk}[1]{\textcolor{blue}{\uline{#1}}}

\newcommand{\add}[1]{\textcolor{red}{[}  #1 \textcolor{red}{]}}

 


\setlength\parindent{20pt}
\allowdisplaybreaks

\begin{document}



\title{A Pseudorandom Generator for Functions of Low-Degree Polynomial Threshold Functions}

\author{Penghui Yao\thanks{\scriptsize State Key Laboratory for Novel Software Technology, New Cornerstone Science Laboratory, Nanjing University, Nanjing 210023, China. Email:
    \texttt{phyao1985@gmail.com}.}~\thanks{\scriptsize Hefei National
    Laboratory, Hefei 230088, China.} \and Mingnan Zhao\thanks{\scriptsize State Key Laboratory for Novel Software Technology, New Cornerstone Science Laboratory, Nanjing University, Nanjing 210023, China. Email:
    \texttt{mingnanzh@gmail.com}.}  }
\date{April 15, 2025}
\maketitle


\thispagestyle{empty}
\begin{abstract}
  Developing explicit \emph{pseudorandom generators} (PRGs) for prominent categories of Boolean functions is a key focus in computational complexity theory.
In this paper, we investigate the PRGs against the
functions of degree-$d$
\emph{polynomial threshold functions} (PTFs)
over Gaussian space.
Our main result is an explicit construction of PRG
with seed length $\poly{k,d,1/\epsilon}\cdot\log n$
that can fool \emph{any} function of $k$
degree-$d$ PTFs with probability at least $1-\varepsilon$.
More specifically, we show that the summation of
$L$ independent $R$-moment-matching Gaussian vectors
$\epsilon$-fools functions of $k$ degree-$d$ PTFs,
where $L=\poly{ k, d, \frac{1}{\epsilon}}$ and $R = O({\log \frac{kd}{\epsilon}})$.
The PRG is then obtained by applying an appropriate discretization
to Gaussian vectors with bounded independence.

 \end{abstract}




\setcounter{page}{1}



\section{Introduction}
\label{sec:intro}
In computational complexity theory,
derandomization is a powerful technique that aims to
reduce randomness in algorithms without sacrificing efficiency or accuracy.
A versatile approach for derandomization is to design explicit \emph{pseudorandom generators} (PRGs) for notable families of Boolean functions.
A PRG for a family of Boolean functions is able to consume few random bits
and produce a distribution over high-dimensional vectors,
which is indistinguishable from a target distribution, such as the uniform distribution over Boolean cube, by any function in the family. In this paper, we concern ourselves with the Gaussian distribution over $\R^n$.
Formally,
\begin{definition}
	Let $\F\subseteq\st{f:\R^n \to \{0,1\}}$ be a family of Boolean functions.
	A function $G:\{0,1\}^r \to \R^n$ is a pseudorandom generator for $\F$ with error $\epsilon$ over Gaussian distribution $\NN\br{0,1}^n$ if for any $f\in\F$,
	\[
		\abs{ \expect{s\sim_u\st{0,1}^r}{f(G(s))} 
		 - \expect{x\sim\NN\br{0,1}^n}{f(x)}		
		} \leq \epsilon \enspace. 
	\]
	We call $r$ the seed length of $G$. We also say $G$ $\epsilon$-fools $\F$ over the Gaussian distribution.
\end{definition}

There has been a considerable amount of research
developing PRGs for various Boolean function families,
including halfspaces, polynomial threshold functions and intersections of halfspaces.
Let $\mathrm{sign}:\R\to\st{0,1}$ be the function such that $\sign{x}=1$ iff $x\geq0$.
A \emph{halfspace} is a Boolean function of the form $f(x) = \sign{a_1x_1+\cdots+a_nx_n-b}$ for some $a_1,\cdots,a_n,b\in\R$.
Halfspaces are a fundamental class of Boolean functions which have found significant applications in machine learning, complexity theory, theory of approximation and more.
A very successful series of work produced PRGs that $\epsilon$-fools halfspaces with seed length poly-logarithmic in $n$ and $\epsilon^{-1}$ over both Boolean space \cite{Ser06, DGJ+10, MZ13, GKM18} and Gaussian space \cite{KM15}.
\emph{Polynomial threshold functions} (PTFs) are functions of the form $f(x) = \sign{p(x)}$ where $p$ is a polynomial. We call $f$ is a degree-$d$ PTF if $p$ is a degree-$d$ polynomial. PTFs are natural generalization for halfspaces since a halfspace is a degree-$1$ PTF.
An explicit PRG that $\epsilon$-fools PTFs over Boolean space has been achieved with seed length
$ (d/\epsilon)^{O(d)}\cdot\log n $ \cite{MZ13}.
As for Gaussian space, a sequence of work \cite{DKN10, Kan11a, Kan11b, Kan12, MZ13, Kan14, Kan15, OST20, KM22} succeeds in giving a PRG with seed length polynomial in $d$, $\epsilon^{-1}$ and $\log n$ \cite{OST20, KM22}.
Another extension for halfspaces is \emph{intersections} of $k$ halfspaces
which are polytopes with $k$ facets.
A line of work \cite{GOWZ10, HKM13, ST17, CDS19, OST22} results in PRGs with seed length polynomial in $\log k$, $\log n$ and $1/\epsilon$ over Boolean space \cite{OST22} and over Gaussian space \cite{CDS19}.



Considering the prosperity of PRGs for these functions families, we commence designing PRGs for
\emph{functions of degree-$d$ polynomial threshold functions}.
\begin{definition}
	We say a function $F:\R^n\to\{0,1\}$ is a function of $k$ degree-$d$ PTFs if there exist $k$ polynomials $p_1,\dots,p_k:\R^n \to \R$ of degree $d$ and a Boolean function $f:\bit{k}\to\st{0,1}$ such that
	\[
		F(x) = f\!\br{\sign{p_1\!(x)},\dots,\sign{p_k\!(x)}}\enspace .
	\]
\end{definition}
\noindent This family consumes all three function families we discussed above. For example, it includes intersections of halfspaces by setting $d=1$ and $f(x)=x_1\cdots x_k$.
The research on PRGs for functions of PTFs is driven by several motivations beyond its fundamental role in derandomization tasks.
For instance, the collection of satisfying assignments of an intersection of $k$ degree-$2$ PTFs corresponds to the feasible solutions set of an $\{0,1\}$-integer quadratic programing \cite{NW06} with $k$ constraints.
The investigation into the structure of these sets has been a central focus of extensive research in areas including learning theory, counting, optimization, and combinatorics.

In this work, we consider building explicit PRGs for functions of degree-$d$ PTFs over Gaussian space. Before presenting our main result, we briefly revisit relevant prior work on fooling functions of halfspaces.

\begin{table}[t]
	\centering
	\caption{Related Work on PRGs for Intersections of PTFs}
	\label{tbl:related}
	\renewcommand{\arraystretch}{1.8}
	\newcolumntype{C}[1]{>{\centering\arraybackslash}p{#1}}
	
	\begin{tabular}{C{2.5cm}cc}
	\hline
	\noalign{\vskip 0mm}
	\hline
	\noalign{\vskip 0mm}
	\hline
	Reference & Function Family  & Seed length \\
	\hline
	\cite{GOWZ10}   & Monotone functions of $k$ halfspaces & $O((k\log (k/\epsilon) + \log n)\cdot \log (k/\epsilon))$ \\
	\hline
	\cite{HKM13}   & Intersections of $k$ $\delta$-regular halfspaces & \makecell{$O(\log n\log k/\epsilon)$\\for $\delta \leq \epsilon^5/(\log^{8.1}\! k \log (1/\epsilon)) $} \\
	\hline
	\cite{ST17}   & Intersections of $k$ weight-$t$ halfspaces & $\poly{\log n, \log k, t, 1/\epsilon}$ \\
	\hline
	\cite{OST22}    & Intersections of $k$ halfspaces & \makecell{$\mathrm{polylog}\ m\cdot{\epsilon}^{-(2+\delta)}\cdot\log n$\\for any absolute constant $\delta\in(0,1)$}\\
	\hline
	\cite{CDS19}    & \makecell{Intersections of $k$ halfspaces\\Arbitrary functions of $k$ halfspaces} & \makecell{$O(\log n + \poly{\log k,1/\epsilon})$\\$O(\log n + \poly{ k,1/\epsilon})$}\\
	\hline
	\cite{DKN10}    & Intersections of $k$ degree-$2$ PTFs & $O(\log n\cdot\poly{k,1/\epsilon})$ \\
	\hline
	\noalign{\vskip 0mm}
	\hline
	\noalign{\vskip 0mm}
	\hline
	\end{tabular}
\end{table}

\subsection{Prior Work}

The related work is summarized in \tbl{related}.
Gopalan, O’Donnell, Wu and Zuckerman \cite{GOWZ10} constructed PRGs for \emph{monotone functions of halfspaces}.
They modified the PRG for halfspaces in \cite{MZ13} and showed the modified PRG $\epsilon$-fools any monotone function of $k$ halfspaces over a broad class of \emph{product distributions} with seed length $O((k\log (k/\epsilon) + \log n)\cdot \log (k/\epsilon))$. When $k/\epsilon \leq \log^c n$ any $c>0$, the seed length can be further improved to $O(k\log (k/\epsilon) + \log n)$.

Harsha, Klivans and Meka \cite{HKM13} considered designing PRGs for intersections of \emph{regular} halfspaces (i.e., halspaces with low influence). 
A halfspace $f(x) = \sign{a_1x_1+\cdots+a_nx_n-b}$ is \emph{$\delta$-regular} if $\sum_i a_i^4\leq \delta^2 \sum_{i} a_i^2$.
They gave an explicit PRG construction for intersections of $k$ $\delta$-regular halfspaces over \emph{proper} and \emph{hypercontractive} distributions with seed length $O(\log n\log k/\epsilon)$ when $\delta$ is no more than a threshold.
Their proof is based on developing an invariance principle for intersections of regular halfspaces via a generalization of the well-known Lindeberg method \cite{Lin22} and an anti-concentration result of polytopes in Gaussian space from \cite{KOS08}.

By extending the approach of \cite{HKM13} and combing the results on bounded independence fooling CNF formulas \cite{Baz09, Raz09}, Servedio and Tan \cite{ST17} designed an explicit PRG that $\epsilon$-fools intersections of $k$ \emph{weight}-$t$ halfspaces over Boolean space with $\poly{\log n, \log k, t, 1/\epsilon}$ seed length. A halfspace $f(x) = \sign{a_1x_1+\cdots+a_nx_n-b}$ is said to be weight-$t$ if each $a_i$ is an integer in $[-t,t]$.

As for intersections of $k$ general halfspaces, O’Donnell, Servedio and Tan \cite{OST22} gave a PRG construction over Boolean space with a polylogarithmic seed length dependence on $k$ and $n$. Their proof involves a novel invariance principle for intersections of arbitrary halfspaces and a Littlewood–Offord style anticoncentration inequality for polytopes over Boolean space.

Concurrently, Chattopadhyay, De and Servedio \cite{CDS19} proposed a simple PRG that $\epsilon$-fools intersections of $k$ general halfspaces over Gaussian space, building upon the concept of
\emph{Johnson-Lindenstrauss transform} \cite{JL86, KMN11}. The seed length is $O(\log n + \poly{\log k,1/\epsilon})$. Additionally, they show that the same PRG with seed length $O(\log n + \poly{k,1/\epsilon})$ is able to fool arbitrary functions of $k$ halfspaces.

Speaking of fooling functions of PTFs, the study by Diakonikolas, Kane and Nelson \cite{DKN10} stands out as the sole work that constructs a PRG for intersections of $k$ degree-$2$ PTFs. Their PRG is specific to degree $d\leq 2$ with a $O(\log n\cdot\poly{k,1/\epsilon})$ seed length. 







\subsection{Main Result}





In this work, we investigate the PRGs fooling any function of low-degree PTFs. The main result is the following.

\begin{theorem}\label{thm:main}(Informal version of \thm{main_formal})
	There exists an explicit PRG
	$\epsilon$-fools any function of $k$ degree-$d$ PTFs over Gaussian space
	with seed length $\poly{k,d,1/\epsilon}\cdot\log n$.
\end{theorem}

The proof is inspired by the PRG proposed in \cite{Kan11b} and the work \cite{KM22}. This theorem follows from two components.

\paragraph{(1) Bounded independence fools functions of $k$ degree-$d$ PTFs.}
Consider the continuous random vector
$Y = \frac{1}{\sqrt{L}} \sum_{i=1}^{L} Y_i$
where $Y_i$ is a $R$-wise independent standard Gaussian vector of length $n$.
Every $Y_{i,j}$ is a standard Gaussian variable and for any degree-$R$ polynomial $f$,
$\expec{f(Y_i)} = \expect{y\sim\NN(0,1)^n}{f(y)}$.
We will prove that
\begin{theorem}(Informal version of \thm{cont})
	With $R = O({\log \frac{kd}{\epsilon}})$ and $L=\poly{k, d, \frac{1}{\epsilon}}$, the distribution of $Y$
	$\epsilon$-fools any function of $k$ degree-$d$ PTFs over Gaussian space.
\end{theorem}
\noindent The prior work \cite{KM22} shows that bounded independence fools a single low-degree polynomial threshold function. This generalizes their work to the case of functions of $k$ low-degree PTFs.


\paragraph{(2) Discretization of bounded independence Gaussians.}
An explicit PRG construction requires a discrete approximation to Gaussian vectors with bounded independence.
The idea is to use a finite entropy random variable $X$ to approximate $Y$.
Previous work \cite{Kan11b} uses the idea that a single Gaussian variable can be produced by two uniform random variables in $[0,1]$ through the Box–Muller transform \cite{Bm58}.
Therefore bounded independence Gaussian variables $Y_i$ can be generated by using bounded independence uniform random variables.
Then by truncating these uniform $[0,1]$ random variables to a sufficient precision, we obtain vectors $X_i$ that serve as a discrete approximation of $Y_i$.
We prove that $X$ also fools functions of $k$ degree-$d$ PTFs as long as $X$ is a good approximation to $Y$.
\begin{lemma}(Informal version of \lem{discrete})
	If $X_{i,j}$ and $Y_{i,j}$ are sufficiently close with high probability, then $X$ also fools functions of $k$ degree-$d$ PTFs.
\end{lemma}
















 

\paragraph{Acknowledgment.} PY and MZ were supported by
National Natural Science Foundation of China (Grant No. 62332009 and 12347104),
Innovation Program for Quantum Science and Technology (Grant No. 2021ZD0302901),
NSFC/RGC Joint Research Scheme (Grant No. 12461160276),
Natural Science Foundation of Jiangsu Province (Grant No. BK20243060),
and the New Cornerstone Science Foundation.

\section{Preliminary}
\label{sec:prelim}
\paragraph{Basic Notation.}
For $n\in \N$, $[n]$ denotes the set $\st{1,2,\cdots,n}$.
For $\alpha\in \R^n$ and $i\in[n]$, $\alpha_i$ denotes the $i$-th coordinate of $\alpha$, $\abs{\alpha} = \sum_{i=1}^n \abs{\alpha_i}$ and $\norm{\alpha}_{\infty} = \max_{1\leq i\leq n} \abs{\alpha_i}$.
For $\alpha, \beta\in \R^n$, $\alpha-\beta$ denotes the vector $v$ such that $v_i = \alpha_i-\beta_i$ for all $i\in[n]$, and $\alpha^{\beta} = \prod_{i=1}^n \alpha_i^{\beta_i}$.
For $\alpha\in \N^n$, $\alpha! = \prod_{i=1}^n \alpha_i!$.
When it is clear from the context, we will use both subscript and superscript as indices.
\vspace{-1em}
\paragraph{Derivatives and Multidimensional Taylor Expansion.}
For a function $f:\R^n \to \R$ and $\alpha\in \N^n$, we use $\partial^{\alpha}\!f$ to denote the partial derivative taken $\alpha_i$ times in the $i$-th coordinate and define $\norm{\grad{t}{f(x)}} = \sqrt{\sum_{\alpha\in\N^n, \abs{\alpha}=t} \br{\partial^{\alpha} f(x)}^2}$.
For $f(a,b): \R^n \times \R^n \to \R$ and $\alpha,\beta\in \N^n$, we use $\partial^{\alpha}_a\partial^{\beta}_b\!f$ to denote the partial derivative taken $\alpha_i$ times in $a_i$ and $\beta_i$ times in $b_i$.
Using these notations, one has:
\begin{theorem}[Multidimensional Taylor's Theorem]\label{thm:taylor}
	Let $d\in \N$ and $f:\R^n \to \R^n$ be a $\mathcal{C}^{d+1}$ function.
	Then for all $x,y\in\R^n$,
	\[
		f(y) = \sum_{\alpha\in\N^n, \abs{\alpha}\leq d}	\frac{\partial^{\alpha} f(x)}{\alpha!} (y-x)^{\alpha} + \sum_{\alpha\in\N^n, \abs{\alpha}= d+1}	\frac{\partial^{\alpha} f(z)}{\alpha!} (y-x)^{\alpha}
	\]
	where $z = cx+(1-c)y$ for some $c\in(0,1)$.
\end{theorem}

\vspace{-1em}
\paragraph{Bump Function.}
Consider the bump function $\Psi:\R\to\R$ defined by
$
	\Psi(x) = \begin{cases}
		e^{\frac{1}{x^2-1}}, &\text{ if } \abs{x}<1,\\
		0, &\text{ if } \abs{x}\geq 1.
	\end{cases}
$
It is well known that this function is infinitely differentiable and the derivatives are bounded.
\begin{fact}\label{fact:deri1}
	For all $t\in \N$, $\abs{\Psi^{(t)}(x) } \leq t^{(3+o(1))t}$.
\end{fact}
\noindent Let $\rho$ be the smooth univariate function defined by
$
	\rho(x) = \begin{cases}
		1, &\text{ for }x\geq 1,\\
		e\cdot e^{\frac{1}{(t-1)^2-1}} &\text{ for } 0<x<1,\\
		0, &\text{ for }x\leq 0.
	\end{cases}
$

\noindent It is easy to see $\rho$ is obtained from $\Psi$ via translation, stretch, and concatenation.
We have
\begin{fact}\label{fact:deri2}
	For all $t\in \N$, $\abs{\rho^{(t)}(x)} \leq t^{(3+o(1))t}$.
\end{fact}

\begin{fact}\label{fact:deri}
	Let $r(u,v) \coloneq \rho( \log u-\log v + c )$ for some constant $c$. Then we have that for all $n,m\in \N$,
	$ \abs{ \frac{ \partial^n \partial^m r(u,v) }{ \partial u^n \partial v^m } } \leq \frac{(n+m)^{6(n+m)}}{\abs{u}^n \abs{v}^m} $.
\end{fact}
\noindent We include the proof for the above three facts in \app{fact_bump} for self-containment.


\paragraph{Gaussian Space and the Gaussian Noise Operator}
We denote by $y\sim \NN(0,1)^n$ that $y = (y_1,\dots,y_n)\in\R^n$ is a random vector
whose components are independent standard Gaussian variables (i.e., with mean $0$ and variance $1$).
We say a random vector $Y\in\R^n$ is a $k$-wise independent standard Gaussian vector if every component of $Y$ is a standard Gaussian variable and
$\E[p(Y)] = \E_{y\sim \NN(0,1)^n}[p(y)]$ for all polynomials $p:\R^n\to \R$ with degree at most $k$.
For a function $f:\R^n\to \R$ on Gaussian space and $1\leq p\leq \infty$, the $p$-norm is denoted by
$
	\norm{f}_p = \br{\expect{y\sim \NN(0,1)^n}{\abs{f(y)}^p}}^{1/p}.
$
For $\rho\in[0,1]$, the \emph{Gaussian noise operator} $U_{\rho}$ is the operator on the space of functions $f:\R^n\to\R$ defined by
$
	U_{\rho}f(x) = \expect{y\sim \NN(0,1)^n}{f(\rho x+\sqrt{1-\rho^2}y)}.
$

The \emph{probabilists' Hermite polynomials}~\cite[Section 11]{O14} $\st{H_j}_{j\in\N}$ are defined by
\[
	H_j(y) = \frac{(-1)^{j}}{\varphi(y)}\cdot \frac{\mathrm{d}^j\varphi(y)}{\mathrm{d}\ y^j}
\]
where $\varphi(y)= \frac{1}{\sqrt{2\pi}}e^{-\frac{y^2}{2}}$.
The \emph{univariate Hermite polynomials} $\st{h_j}_{j\in\N}$ are defined by normalization: $h_{j} = \frac{1}{\sqrt{j!}}H_j$.
For a multi-index $\alpha\in\N^n$, the \emph{(multivariate) Hermite polynomial} $h_\alpha:\R^n\to \R$ is
\[
	h_\alpha(y) = \prod_{j=1}^n h_{\alpha_j}(y_j) \enspace.
\]
The degree of $h_\alpha$ is $\abs{\alpha}$.
The Hermite polynomials $\st{h_\alpha}_{\alpha\in\N^n}$ form an orthonormal basis for the functions over Gaussian space: $\expect{y\sim \NN(0,1)^n}{h_\alpha(y)h_\beta(y)} = 1$ iff $\alpha=\beta$, and every degree-$d$ polynomial $f:\R^n\to \R$ can be uniquely expanded as
\[
	f(y) = \sum_{\alpha\in\N^n, \abs{\alpha}\leq d} \widehat{f}(\alpha) h_{\alpha}(y)\enspace.
\]
We can also expand the function $f(x+\sqrt{\lambda} y)$ in the Hermite basis in a manner similar to Taylor expansion.
\begin{lemma}[Lemma 16 in \cite{KM22}]\label{lem:expanding}
Suppose $f(y) = \sum_{\alpha\in\N^n} \widehat{f}(\alpha) h_{\alpha}(y)$, we have
\[
	f(x+\sqrt{\lambda} y) =
	\sum_{\alpha\in\N^n} \frac{\partial^{\alpha}\phi(x)}{\sqrt{\alpha!}} \lambda^{\abs{\alpha}/2} h_{\alpha}(y) \enspace,
\]
where $\phi(x) = U_{\sqrt{1-\lambda}} f\br{\frac{x}{\sqrt{1-\lambda}}}$.
\end{lemma}

The function $U_{\rho}f$ has the following expansion:
\[
	U_\rho f(y) = \sum_{\alpha\in\N^n, \abs{\alpha}\leq d} \rho^{\abs{\alpha}}\widehat{f}(\alpha) h_{\alpha}(y)\enspace.
\]
The definition of $U_\rho$ can be extended to $\rho>1$ by its action on the Hermite polynomials: $U_{\rho}h_{\alpha}(y) = \rho^{\abs{\alpha}}h_{\alpha}(y)$.
We will use the following hypercontractive inequality:
\begin{theorem}\label{thm:hc}
	Let $f:\R^n\to\R$ and $2\leq p\leq \infty$, $\norm{f}_p \leq \norm{U_{\sqrt{p-1}}f}_2$.
\end{theorem}
For more details on analysis over Gaussian space, readers may refer to \cite{O14}. 

\paragraph{Low-Degree Polynomials.}
Low-degree polynomials are extensively studied in the literature.
We list some results used in this paper.
It is well-know that low-degree polynomials have the following anti-concentration property:
\begin{lemma}[Theorem 8 in \cite{CW01}]\label{lem:anti}
	Let $p:\R^n\to\R$ be a polynomial of degree $d$ with $\norm{p}_2=1$. Then 
	\[
		\Pr_{x\sim \NN(0,1)^n} [\abs{p(x)}\leq \epsilon] = O(d\epsilon^{1/d}) \enspace .
	\]
\end{lemma}

Suppose $p$ is a low-degree polynomial, the following gives an estimation on the deviation of $p(x)$ caused by a small perturbation.
\begin{lemma}[Lemma 22 in \cite{Kan11b}] \label{lem:close}
	Let $p:\R^n\to\R$ be a polynomial of degree $d$ with $\norm{p}_2=1$. Suppose $x\in\R^n$ be a vector with $\norm{x}_\infty\leq B(B>1)$. Let $x'$ be another vector such that $\norm{x-x'}_\infty\leq \delta<1$. Then 
	\[
		\abs{p(x)-p(x')}\leq \delta n^{d/2}O(B)^d \enspace .
	\]
\end{lemma}

The magnitudes of the derivatives of a low-degree polynomial are likely to grow at a moderate rate with high probability. Formally,
\begin{lemma}[Lemma 6 in \cite{KM22}]\label{lem:good_event}
	Let $p:\R^n \to \R$ be an arbitrary polynomial of degree $d$ and $y\sim\NN\br{0,1}^n$,
	the following holds with probability at least $1-\epsilon d^3$:
	\[
		\norm{\grad{t}{p(y)}} \leq O\br{\frac{1}{\epsilon}} \norm{\grad{t-1}{p(y)}}
		\text{ for all } 1\leq t\leq d.
	\]
\end{lemma}

The following lemma gives quantitative bounds on how much
the derivatives $\grad{t}{p(x+\sqrt{\lambda}y)}$ are concentrated around those of $\phi(x) =\expect{y\sim \NN(0,1)^n}{p(x+\sqrt{\lambda} y)}$ when $y\sim \NN(0,1)^n$.

\begin{lemma}[Lemma 23 in \cite{KM22}]\label{lem:concentrate}
	Let $0\leq \lambda<1$ and $p:\R^n \to \R$ be an arbitrary polynomial of degree $d$ and $\phi(x) = U_{\sqrt{1-\lambda}} p\!\br{\frac{x}{\sqrt{1-\lambda}}}=\expect{y\sim \NN(0,1)^n}{p(x+\sqrt{\lambda} y)}$. For $0\leq t\leq d$ and $y\sim\NN\br{0,1}^n$,
	\[
	\br{\expect{y\sim\NN\br{0,1}^n}{\norm{ \grad{t}{p(x+\sqrt{\lambda}y)} - \grad{t}{\phi(x)}}^R}}^{\frac{1}{R}} \leq
	\sqrt{ \sum_{j=t+1}^d (\lambda dR)^{j-t} \norm{ \grad{j}{\phi(x)}}^2 } \enspace.
	\]
\end{lemma} 

\section{Fooling the Functions of PTFs via Bounded Independence}
\label{sec:fool}
In this section, we show that a random Gaussian vector matching certain moments fools \emph{any} function of low-degree polynomial threshold functions. Formally, we prove
\begin{theorem}\label{thm:cont}
	Fix a small constant $0<\epsilon<1$ and let $R\in\N$ be an integer. Let $p_1,\dots,p_k:\R^n \to \R$ be arbitrary polynomials of degree $d$ and $f:\bit{k}\to\st{0,1}$ be an arbitrary Boolean function.
	Define function
	\[
		F(x)\coloneqq f\!\br{\sign{p_1\!(x)},\dots,\sign{p_k\!(x)}}
	\]
	Let $Y = \frac{1}{\sqrt{L}} \sum_{i=1}^{L} Y_i $ where $Y_i$ is
	 a $2dR$-wise independent standard Gaussian vector of length $n$ and $L=\Omega\br{\frac{k^{2}d^{3}R^{15}}{\epsilon^2}}$. Then, we have
	\[
	\abs{
		\expect{Y}{
			F(Y)	
		}
		-
		\expect{y\sim\NN\br{0,1}^n}{
			F(y)
		}
	} = O(\epsilon kd^3) + kdL\cdot2^{-\Omega(R)} \enspace .
	\]
	\end{theorem}

The key idea in the proof of \thm{cont} is 
to analyze the derivatives of the disturbed function
$\phi_i(x) = \E_{y\sim\NN\br{0,1}^n}[p_i(x+\sqrt{\lambda}y)]$.
We will see that
once the derivatives of $\phi_i$ are well-controlled by its preceding order derivative at $x$,
$\grad{t}{p_i(x+\sqrt{\lambda}y)}$ is concentrated around
$\grad{t}{\phi_i(x)}$
for a random $y$,
and $p_i(x+\sqrt{\lambda}y)$ and $\phi_i(x)$
share the same sign with high probability.
Starting from this point, we use the mollifier introduced in~\cite{KM22}
\begin{align}
	G(x)\coloneqq \prod_{i=1}^{k} \prod_{t=0}^{d-1} \rho\!\br{\log\!\br{\frac{\norm{\grad{t}{p_i(x)}}^2}{16\epsilon^2 \norm{\grad{t+1}{p_i(x)}}^2 }}} \label{eq:molli}
\end{align}
to judge whether the derivatives are all well-controlled for all $k$ polynomials.
$G(x)=0$ as long as a certain order of derivative that is not controlled by its preceding order derivative.
Our proof consists of following steps:
\begin{itemize}
	\item \textbf{Approximation using the mollifier $G$:}
	We first establish that
	\[
	\abs{
		\expect{Y}{
			F(Y)	
		}
		-
		\expect{y}{
			F(y)
		}
	} \approx 
	\abs{
		\expect{Y}{
			F(Y)G(Y)	
		}
		-
		\expect{y}{
			F(y)G(y)
		}
	}\enspace.
	\]
	This approximation enables us to focus primarily on the analysis of $F(y)G(y)$ in the subsequent steps.
	\item \textbf{Hybrid argument:} Let $\lambda = L^{-1}$, $y = \sqrt{\lambda}\sum_{i=1}^L y_i$ where $y_i\sim \NN(0,1)^n$ and $Z^i =\sqrt{\lambda}(y_1 + \cdots +y_{i-1} +Y_{i+1} +\cdots Y_L)$. We will show 
	\begin{align}
		\expect{ }{
			F(Z^i+\sqrt{\lambda}Y_i)G(Z^i+\sqrt{\lambda}Y_i)	
		}
		\approx
		\expect{ }{
			F(Z^i+\sqrt{\lambda}y_i)G(Z^i+\sqrt{\lambda}y_i)
		} \enspace.\label{eq:singlestep}
	\end{align}
	Therefore by the triangle inequality, we have
	\begin{align*}
		\expect{Y}{
			F(Y)G(Y)	
		} &= \expect{ }{
			F(Z^1+\sqrt{\lambda}Y_1)G(Z^1+\sqrt{\lambda}Y_1)	
		}\\
		&\approx
		\expect{ }{
			F(Z^1+\sqrt{\lambda}y_1)G(Z^1+\sqrt{\lambda}y_1)
		}
		= \expect{ }{
			F(Z^2+\sqrt{\lambda}Y_2)G(Z^2+\sqrt{\lambda}Y_2)	
		}\\
		&\approx \cdots \approx
		\expect{ }{
			F(Z^L+\sqrt{\lambda}y_L)G(Z^L+\sqrt{\lambda}y_L)
		}= \expect{ }{
			F(y)G(y)
		} \enspace.
	\end{align*}
	To prove \eq{singlestep}, we show for any fixed $x$,
	\[
		\expect{}{
			F(x+\sqrt{\lambda}Y_i)G(x+\sqrt{\lambda}Y_i)
		}
		\approx
		\expect{}{
			F(x+\sqrt{\lambda}y_i)G(x+\sqrt{\lambda}y_i)
		} \enspace .
	\]
	This is done by a case analysis:
	\begin{itemize}
	\item The derivatives of all $k$ polynomials $\phi_j(x)$ are well-controlled at point $x$. In this case, all $p_j(x+\sqrt{\lambda} Y_i)$ and $p_j(x+\sqrt{\lambda} y_i)$ share the same sign with high probability. Thus, it is highly likely that $F(x+\sqrt{\lambda}Y_i)$ and $F(x+\sqrt{\lambda}y_i)$ are nearly the same constant. It suffices to show $Y_i$ fools the mollifier function $G(x+\sqrt{\lambda}y_i)$.
	\item At least one derivative is not controlled. In this case, we will show that $G(x+\sqrt{\lambda}Y_i)$ and $G(x+\sqrt{\lambda}y_i)$ are $0$ with high probability. This implies that $F(x+\sqrt{\lambda}Y_i)G(x+\sqrt{\lambda}Y_i) = F(x+\sqrt{\lambda}y_i)G(x+\sqrt{\lambda}y_i) = 0$ with overwhelming probability.
	\end{itemize}
	\end{itemize}
	
	In the subsequent sections, \sect{molli} first demonstrates that $Y_i$ is able to fool the mollifier function $G$ when $x$ is a well-behaved point.
	\sect{single} shows the closeness of a single step in the hybrid argument.
	Lastly, we prove \thm{cont} using approximation and the hybrid argument in \sect{main}.
	





\subsection{Fooling the Mollifier $G$} \label{sec:molli}
We begin with proving that a $2dR$-wise independent standard Gaussian vector $Y$
fools the mollifier function $G(x+\sqrt{\lambda}y)$.
To achieve this, we utilize the Taylor expansion to expand
the mollifier function
$G(x+\sqrt{\lambda}y)$ up to a specified order.
As a result, $G(x+\sqrt{\lambda}y)$ is decomposed into two parts:
a degree-$d(R-1)$ polynomial $l(y)$ and a remainder term $\Delta(y)$.
We mainly show that $\E[\Delta]$ is negligible under both
pseudorandom distribution and true Gaussian distribution.
This leads us to the conclusion that
$\E[G(x+\sqrt{\lambda}y)]\approx \E[l(y)]$ and
$\E[G(x+\sqrt{\lambda}Y)]\approx \E[l(Y)]$.
Furthermore, since $l(y)$ has degree at most $dR$,
it follows that $\E[l(y)] = \E[l(Y)]$.
Thus, we conclude that $\E[G(x+\sqrt{\lambda}y)]\approx\E[G(x+\sqrt{\lambda}Y)]$.


\begin{lemma}\label{lem:molli}
		Fix a small constant $0<\epsilon<1$ and let $R\in\N$ be an integer.
		Let $p_1,\dots,p_k:\R^n \to \R$ be arbitrary polynomials of degree $d$.
	Define $\phi_i(x) \coloneq U_{\sqrt{1-\lambda}} p_i\!\br{\frac{x}{\sqrt{1-\lambda}}}=\E_{y\sim\NN\br{0,1}^n}[p_i(x+\sqrt{\lambda}y)]$ for all $p_i$.
Suppose that a fix point $x\in\R^n$ satisfies $ \norm{\grad{t+1}{\phi_i(x)}} \leq \frac{1}{\epsilon}\norm{\grad{t}{\phi_i(x)}}$ for any $1\leq i\leq k$ and $0\leq t\leq d-1$.
	Let $Y$ be a $2dR$-wise independent standard Gaussian vector of length $n$.
	For $\lambda = O(k^{-2}d^{-3}R^{-15}\epsilon^2)$, we have 
	\[
	\abs{
		\expect{Y}{
			G(x+\sqrt{\lambda}Y)
		}
		-
		\expect{y\sim\NN\br{0,1}^n}{
			G(x+\sqrt{\lambda}y)
		}
	} = kd\cdot 2^{-\Omega(R)} \enspace ,
	\]
	where $G$ is defined in \eq{molli}.
	\end{lemma}
	
\begin{proof}\textcolor{red}{TOPROVE 0}\end{proof}	

\subsection{A Single Step in the Hybrids} \label{sec:single}
In this section,
we analyze one single step in the entire hybrid argument.
We will show that for any $x$, we have that
$\E_Y[F(x+\sqrt{\lambda}Y)G(x+\sqrt{\lambda}Y)]\approx
\E_y[F(x+\sqrt{\lambda}y)G(x+\sqrt{\lambda}y)]$ for $2dR$-wise independent Gaussian $Y$ and true Gaussian $y$.

Let $\phi_i(x) = U_{\sqrt{1-\lambda}} p_i\!\br{\frac{x}{\sqrt{1-\lambda}}} = \E_y[p_i(x+\sqrt{\lambda}y)]$.
The proof proceeds through a case analysis
based on the behavior of $\phi_i$ at the fixed point $x$.
Specifically, we define $x$ as well-behaved if
$\norm{\grad{t+1}{\phi_i(x)}} \leq \frac{1}{\epsilon}\norm{\grad{t}{\phi_i(x)}}$ for all $t\in[d]$ and $i\in[k]$.
In other words, for each function $\phi_i$,
its $t$-th order derivatives are controlled by its
$(t-1)$-th order derivatives.
\begin{itemize}
	\item In the scenario where $x$ is not well-behaved,
	we can identify an $i_0$ and a $t_0$ such that
	with at least probability $1-2^{-R+1}$,
	\[
	\norm{ \grad{t_0+1}{p_{i_0}(x+\sqrt{\lambda}y)}}> \frac{1}{4\epsilon} \norm{ \grad{t_0}{p_{i_0}(x+\sqrt{\lambda}y)}} \enspace .
	\]
	Thus, it is highly probable that the mollifier function
	$G(x+\sqrt{\lambda}y)=0$.
	So, the expectation of $F(x+\sqrt{\lambda}y)G(x+\sqrt{\lambda}y)$ is no more that $2^{-R+1}$.
	The same argument works for $Y$ as well.
	\item For the case that $x$ is well-behaved,
	we will show that for all $p_i$,
	${ p_i(x+\sqrt{\lambda}y) }$ and ${ p_i(x+\sqrt{\lambda}Y) }$ are nearly the same constant. This implies
	$F(x+\sqrt{\lambda}y)$ and $F(x+\sqrt{\lambda}Y)$ are equal in most situations.
	Then it suffices to show $Y$ fools the mollifier,
	as discussed in the previous section.
\end{itemize}

\begin{lemma}\label{lem:hybrid}
	Fix a small constant $0<\epsilon<1$ and let $R\in\N$ be an integer. Let $p_1,\dots,p_k:\R^n \to \R$ be arbitrary polynomials of degree $d$ and $f:\bit{k}\to\st{0,1}$ be an arbitrary Boolean function.
	Define function
	\[
		F(x)\coloneqq f\!\br{\sign{p_1\!(x)},\dots,\sign{p_k\!(x)}}\enspace.
	\]
Let $Y$ be a $2dR$-wise independent standard Gaussian vector of length $n$.
	For any $x\in\R^n$ and $\lambda = O(k^{-2}d^{-3}R^{-15}\epsilon^2)$
	\[
	\abs{
		\expect{Y}{
			F(x+\sqrt{\lambda}Y)G(x+\sqrt{\lambda}Y)
		}
		-
		\expect{y\sim\NN\br{0,1}^n}{
			F(x+\sqrt{\lambda}y)G(x+\sqrt{\lambda}y)
		}
	} = kd2^{-\Omega(R)} \enspace ,
	\]
	where $G$ is defined in \eq{molli}.
\end{lemma}

\begin{proof}\textcolor{red}{TOPROVE 1}\end{proof}




\subsection{Proof of \thm{cont}} \label{sec:main}	
\begin{proof}\textcolor{red}{TOPROVE 2}\end{proof}
	
	
	
	
	
	
	
	
	
	
	
	 

\section{Discretization}
\label{sec:discrete}
To give an explicit construction of a PRG,
we need a discretization of $R$-wise independent Gaussian distributions. 
In this section, we show an algorithm which outputs $L$ vectors $\st{X_i}_{1\leq i\leq L}$ approximating $Y_i$, that is, $\abs{X_{i,j}-Y_{i,j}}$ is sufficiently small. Before that, we first prove that if $X$ and $Y$ are close enough, then $X$ also fools any function of low-degree polynomial threshold functions.



\begin{lemma}\label{lem:discrete}
	Let $0<\epsilon,\delta<1$, and $R\in\N$ be an integer.
	Let $Y = \frac{1}{\sqrt{L}}\sum_{i=1}^L Y_i$ where $Y_i$ is an $R$-wise independent Gaussian vector of length $n$ for $1\leq i\leq L$.
	Let $p_1,\dots,p_k:\R^n \to \R$ be arbitrary polynomials of degree $d$ and $f:\bit{k}\to\st{0,1}$ be an arbitrary Boolean function.
	Define functions
	\[
		F(x)\coloneqq f\!\br{\sign{p_1\!(x)},\dots,\sign{p_k\!(x)}}
	\]
Suppose that for any such function $F$,
	\[
	\abs{
		\expect{Y}{
			F(Y)	
		}
		-
		\expect{y\sim\NN\br{0,1}^n}{
			F(y)
		}
	} \leq \epsilon \enspace .
	\]
	Suppose that $\st{X_{i}}_{1\leq i\leq L}$ are random vectors of length $n$ and there is a joint distribution over $X$ and $Y$ such that
	for each $1\leq i\leq L,1\leq j\leq n$, 
	$\Pr\Br{\abs{X_{i,j}-Y_{i,j}}\leq \delta}\geq 1-\delta$.
	
	Let $X = \frac{1}{\sqrt{L}}\sum_{i=1}^L X_i$ and we have that for any such function $F$
	\[
	\abs{
		\expect{X}{
			F(X)	
		}
		-
		\expect{y\sim\NN\br{0,1}^n}{
			F(y)
		}
	} \leq \epsilon + k 2^{2k}d \delta^{1/d} \sqrt{nL} \log\frac{1}{\delta}  + O(2^{2k}nL\delta) \enspace .
	\]
\end{lemma}

\begin{proof}\textcolor{red}{TOPROVE 3}\end{proof}




We now prove the main theorem for constructing an explicit pseudorandom generator .
The idea is that a standard Gaussian variable can be generated using two uniform $[0,1]$ random variables
through the Box–Muller transform \cite{Bm58}.
Let
$
	Y_{i,j} = \sqrt{-2 \log u_{i,j}}\cos(2\pi v_{i,j})
$ 
where $u_{i,j}$ and $v_{i,j}$ are uniform in $[0,1]$. Then $Y_{i,j}$ is a Gaussian variable.
Thus, if we truncate $u_{i,j}$ and $v_{i,j}$ to a certain precision and produce $X_{i,j}$ in a similar manner, $X$ approximates $Y$ with high probability.

\begin{theorem}\label{thm:main_formal}
	There exists an explicit PRG which $\epsilon$-fools any functions of any $k$ degree-$d$ polynomial threshold functions over $\NN(0,1)^n$ with seed length
	$
		O\br{\frac{k^5d^{11}}{\epsilon^{2}} \mathrm{log}\frac{kdn}{\epsilon} } .	
	$
\end{theorem}

\begin{proof}\textcolor{red}{TOPROVE 4}\end{proof}








 
\newpage
\bibliographystyle{alpha}
\bibliography{ref}

\newpage
\begin{appendices}
\addappheadtotoc
\addtocontents{toc}{\protect\setcounter{tocdepth}{1}}

\section{Facts about Bump Function}
\label{sec:fact_bump}

\begin{repfact}{fact:deri1}
		For all $t\in \N$, $\abs{\Psi^{(t)}(x) } \leq t^{(3+o(1))t}$.
\end{repfact}

\begin{proof}\textcolor{red}{TOPROVE 5}\end{proof}


\begin{repfact}{fact:deri2}
	For all $t\in \N$, $\abs{\rho^{(t)}(x)} \leq t^{(3+o(1))t}$.
\end{repfact}

\begin{proof}\textcolor{red}{TOPROVE 6}\end{proof}

\begin{repfact}{fact:deri}
	Let $r(u,v) \coloneq \rho( \log u-\log v + c )$ for some constant $c$. Then we have that for all $n,m\in \N$,
	$ \abs{ \frac{ \partial^n \partial^m r(u,v) }{ \partial u^n \partial v^m } } \leq \frac{(n+m)^{6(n+m)}}{\abs{u}^n \abs{v}^m} $.
\end{repfact}

\begin{proof}\textcolor{red}{TOPROVE 7}\end{proof}


%
 
\end{appendices}

\end{document}
