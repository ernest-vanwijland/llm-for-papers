%!TEX root = mainEV.tex


\section{Algorithms against an Oblivious Adversary}\label{sec:Obl}

To prove \Cref{thm:upper}, we first reduce Problem~\ref{prob:dyn} to solving a normalized threshold version of the problem where we assume that initially, the maximum eigenvalue is not much bigger than one. Then we want to maintain a certificate that 
the maximum eigenvalue is not much less than one until no such certificate exists. This is formalized below.
%
%
\begin{problem}[DecMaxEV($\epsilon,\AA_0,\vv_1,\cdots,\vv_T$)]\label{def:DecMaxEval} Let $\AA_0$ be an $n\times n$ symmetric PSD matrix such that $ \lambda_{\max}(\AA_0) \leq 1 + \frac{\epsilon}{\log n}$. The {\sc DecMaxEV}($\epsilon,\AA_0,\vv_1,\cdots,\vv_T$) problem asks to find for every $t$, a vector $\ww_t$ such that 
\[
\|\ww_t\| = 1 \quad \text{and} \quad \ww_t^{\top}\AA_t \ww_t \geq 1-40\epsilon,
\]
or return {\sc False} indicating that $\lambda_{\max}(\AA_t)\le 1- \frac{\eps}{\log n}$.
\end{problem} 
We defer the proof of the standard reduction stated below to the appendix.
%
\begin{restatable}{lemma}{Decision}\label{lem:Decision}
Given an algorithm $\mathcal{A}$ that solves the decision problem {\sc DecMaxEV}($\epsilon,\AA_0,\vv_1,\cdots,\vv_T$) (Definition~\ref{def:DecMaxEval}) for any $\epsilon>0$, $\AA_0 \succeq 0$ and vectors $\vv_1,\cdots,\vv_T$ in time $\mathcal{T}$, we can solve Problem~\ref{prob:dyn} in total time $O\left(\frac{\log^2 n\log\frac{n}{\epsilon}}{\epsilon}\cdot nnz(\AA_0) + \frac{\log n}{\epsilon}\log \frac{\lambda_{\max}(\AA_0)}{\lambda_{\max}(\AA_T)}\mathcal{T}\right)$.
\end{restatable}
%
%
Next, we describe \Cref{alg:PowerMethod} which can be viewed as an algorithm for \Cref{def:DecMaxEval} when there are no updates. 
Our algorithm essentially applies the {\it power iteration}, which is a standard algorithm used to find an approximate maximum eigenvalue and eigenvector of a matrix. In the algorithm, we make $R = O(\log n)$ copies to boost the probability.
%
%
\begin{algorithm}
\caption{{\sc DecMaxEV} with no update}\label{alg:PowerMethod}
 \begin{algorithmic}[1]
\Procedure{PowerMethod}{$\epsilon, \AA$}
\State $R \leftarrow 10\log n, r_0 \leftarrow 1$
\State $K \leftarrow \frac{4\log \frac{n}{\epsilon}}{\epsilon}$
\For{$r = 1,\cdots , R$}
\State $\vv^{(0,r)} \leftarrow $ random vector with coordinates chosen from $N(0,1)$\label{algline:randomInit}
\For{$k = 1:K$}
\State $\vv^{(k,r)}\leftarrow \AA\vv^{(k-1,r)}$
\EndFor
\State $\ww^{(r)} \leftarrow \frac{\vv^{(K,r)}}{\|\vv^{(K,r)}\|}$ 
\EndFor
\State $\WW = [\ww^{(1)},\dots,\ww^{(R)}]$\label{line:before case}
\If{ $(\ww^{(r)})^{\top}\AA\ww^{(r)} < 1-\epsilon$ for all $r\le R$}\label{line: if all w 1-eps}
\State \Return {\sc False}
\Else
\State $r_0\leftarrow$ smallest $r$ such that $(\ww^{(r)})^{\top}\AA\ww^{(r)} \geq 1-5\epsilon$
\State \Return $[r_0,\WW]$
\EndIf
\EndProcedure 
 \end{algorithmic}
\end{algorithm}
%
%
Below, we state the guarantees of the power method.

\begin{restatable}{lemma}{PowerMethod}\label{thm:StaticPowerMain}
Let $\epsilon>0$ and $\AA \succeq 0$. Let $\WW$ be as defined in Line~\ref{line:before case} in the execution of {\sc PowerMethod}($\epsilon,\AA$). With probability at least $1-1/n^{10}$, for some $\ww \in \WW$, it holds that $\ww^{\top}\AA\ww \geq (1-\frac{\epsilon}{2})\lambda_{\max}(\AA)$. The total time taken by the algorithm is at most $O\left(\frac{nnz(\AA)\log n\log \frac{n}{\epsilon}}{\epsilon}\right)$.

Furthermore, let $\lambda_i$ and $\uu_i$ denote the eigenvalues and eigenvectors of $\AA$. For all $i$ such that $\lambda_i (\AA) \leq \frac{\lambda_{\max}(\AA)}{2}$, with probability at least $1-2/n^{10}$, $\left[\ww^{\top}\uu_i\right]^2\leq \frac{1}{n^8}\cdot \frac{\lambda_i}{\lambda_1}$.
\end{restatable}
 We note that the last line of the above lemma is saying that the vectors returned by the power method satisfy \Cref{def:super}, which we state for completeness but is not required by our algorithm. The following result is a direct consequence of Lemma~\ref{thm:StaticPowerMain}.

 \begin{corollary}\label{thm:StaticPower}
 Let $\epsilon>0, \AA \succeq 0$. Let $\WW$ be as defined in Line~\ref{line:before case} in the execution of {\sc PowerMethod}($\epsilon,\AA$). If $\lambda_{\max}(\AA) \ge 1-\epsilon$, then with probability at least $1-1/n^{10}$, $\ww^{\top}\AA\ww\geq 1-5\epsilon$ for some $\ww\in \WW$. Furthermore, if $\lambda_{\max}(\AA) \ge 1-\epsilon/\log n$, then with probability at least $1-1/n^{10}$, $\ww^{\top}\AA\ww\geq 1-\epsilon$ for some $\ww\in \WW$. The total time taken by the algorithm is at most $O\left(\frac{nnz(\AA)\log n\log \frac{n}{\epsilon}}{\epsilon}\right)$.
 \end{corollary}

Observe that, if the algorithm returns $[r_0,\WW]$, then $(\ww^{(r)})^{\top}\AA\ww^{(r)}\geq 1-5\epsilon$ for $r=r_0$, and $\ww^{(r_0)}$ is therefore a solution to Problem~\ref{def:DecMaxEval} when there is no update. The power method and its analysis are standard, and we thus defer the proof of \Cref{thm:StaticPowerMain} to the appendix. 




Next, in \Cref{alg:Init,alg:DynamicMaxPM} we describe an algorithm for Problem~\ref{def:DecMaxEval} when we have an online sequence of updates $\vv_1,\cdots, \vv_T$. 
%
The algorithm starts by initializing $R = O(\log n)$ copies of the approximate maximum eigenvectors from the power method. Given a sequence of updates, as long as one of the copies is the witness that the current matrix $\AA_t$ still has a large eigenvalue, i.e., there exists $r$ where $(\ww^{(r)})_t^{\top}\AA_t\ww^{(r)}_t\geq 1-40\epsilon$, we can just return $\ww^{(r)}$ as the solution to Problem~\ref{def:DecMaxEval}. 
Otherwise, $(\ww^{(r)})_t^{\top}\AA_t\ww^{(r)}_t< 1-40\epsilon$ for all $r \le R$ and none of the vectors from the previous call to the power method are a witness of large eigenvalues anymore. In this case, we simply recompute these vectors by calling the power method again. If the power method returns that there is no large eigenvector, then we return {\sc False} from now. Otherwise, we continue in the same manner. 
%
Note that our algorithm is very simple, but as we will see, the analysis is not straightforward.
%
\begin{algorithm}
\caption{Initialization}\label{alg:Init}
 \begin{algorithmic}[1]
 \Procedure{Init}{$\epsilon, \AA_{0}$}
\State $\WW \leftarrow$ {\sc PowerMethod}($\epsilon,\AA_0$)\label{algline:PMInit}
\State \Return $\WW$
\EndProcedure
\end{algorithmic}
\end{algorithm}


\begin{algorithm}
\caption{Update algorithm at time $t$ ($A_{t-1},r_t,\WW_{t-1}= [w^{(r)}_{t-1}: r= 1,\cdots R], \eps$ are maintained)}\label{alg:DynamicMaxPM}
 \begin{algorithmic}[1]
\Procedure{Update}{$\vv_t$}
\State $\AA_t \leftarrow \AA_{t-1}-\vv_t\vv_t^{\top}$
\If{$ (\ww^{(r)}_{t-1})^{\top}\AA_t\ww^{(r)}_{t-1} <1-40\epsilon$ for all $r \le R$}\label{algline:Check}
\State $[r_t,\WW_t]\leftarrow$ {\sc PowerMethod}($\epsilon,\AA_t$)\label{algline:PM}
\If{{\sc PowerMethod}($\epsilon,\AA_t$) returns {\sc False}}
\State \Return {\sc False} for all further updates
\EndIf
\Else
\State $r_t\leftarrow $ smallest $r$ such that $(\ww^{(r)}_{t-1})^{\top}\AA_t\ww^{(r)}_{t-1} \geq 1-40\epsilon$
\State $\WW_t \leftarrow \WW_{t-1}$
\EndIf
\State \Return $[r_t,\WW_t]$
\EndProcedure 
 \end{algorithmic}
\end{algorithm}

\subsection{Proof Overview}
The overall proof of \Cref{thm:upper}, including the proof of correctness and the runtime depends on the number of executions in Line~\ref{algline:PM} in Algorithm~\ref{alg:DynamicMaxPM}. If the number of executions of Line~\ref{algline:PM} is bounded by $\poly(\log n/\epsilon)$, then the remaining analysis is straightforward. Therefore, the majority of our analysis is dedicated to proving this key lemma, i.e., $\poly(\log n/\epsilon)$ bound on the number of calls to the power method:
\begin{lemma}[Key Lemma]\label{lem:BoundW}
The number of executions of Line~\ref{algline:PM} over all updates is bounded by $O(\log n\log^5\frac{n}{\epsilon}/\epsilon^2)$ with probability at least $1-\frac{1}{n}$.
\end{lemma}
Given the key lemma, the correctness and runtime analyses are quite straightforward and are presented in \Cref{sec:correct}. We now give an overview of the proof of \Cref{lem:BoundW}.


Let us consider what happens between two consecutive calls to Line~\ref{algline:PM}, say at $\AA$ and $\AAtil = \AA - \sum_{i=1}^k\vv_i\vv_i^{\top}$. We first define the following subspaces of $\AA$ and $\AAtil$.
Recall \Cref{def:subspace}, which we use to define the following subspaces.

\begin{definition}[Subspaces of $\AA$ and $\AAtil$]\label{def:SpaceA} Given $\epsilon>0$, $\AA$, and $\AAtil$ define for $\nu = 0,1,\cdots, 15\log \frac{n}{\epsilon}-1$:
\[
T_{\nu} = \Span\left(\frac{(\nu+1)\epsilon}{5\log \frac{n}{\epsilon}},\AA\right) -\Span\left(\frac{\nu\epsilon}{5\log \frac{n}{\epsilon}},\AA\right),
\]
and,
\[
\tilde{T}_{\nu} = \Span\left(\frac{(\nu+1)\epsilon}{5\log \frac{n}{\epsilon}},\AAtil\right)-\Span\left(\frac{\nu\epsilon}{5\log \frac{n}{\epsilon}},\AAtil\right).
\]
That is, the space $T_{\nu}$ and $\Ttil_\nu$ are spanned by eigenvectors of $\AA$ and $\AAtil$, respectively, corresponding to eigenvalues between $\left(1-(\nu+1)\frac{\epsilon}{5\log \frac{n}{\epsilon}}\right)\lambda_0$ and $\left(1-\nu\frac{\epsilon}{5\log \frac{n}{\epsilon}}\right)\lambda_0$.

Let $d_{\nu} = \dim{T_{\nu}}$ and $\tilde{d}_{\nu} = \dim{\tilde{T}_{\nu}}$. Also define,
\[
\tilde{T} = \Span(3\epsilon,\AAtil),\quad  T = \Span(3\epsilon,\AA),
\]
and let $d = \dim{T}$, $\tilde{d} = \dim{\tilde{T}}$.
\end{definition}

Observe that $T = \sum_{\nu = 0}^{15\log\frac{n}{\eps}-1}T_\nu$ and similarly $\Ttil = \sum_{\nu = 0}^{15\log\frac{n}{\eps}-1}\Ttil_\nu$. We next define some indices/levels corresponding to large subspaces, which we call ``important levels''.
\begin{definition}[Important $\nu$]\label{def:impNu} We say a level $\nu$ is important if,
\[d_{\nu} \geq \frac{\epsilon}{600\log^3 \frac{n}{\epsilon}} \sum_{\nu'<\nu}d_{\nu'}.
\]
We will use $\mathcal{I}$ to denote the set of $\nu$ that are important.
\end{definition}
%

The main technical lemma that implies \Cref{lem:BoundW} is the following:


\begin{restatable}[Measure of Progress]{lemma}{progress}\label{lem:EigenspaceChange}
	Let $\epsilon>0$ and let $\WW=[\ww^{(1)},\dots,\ww^{(R)}]$ be as defined in Line~\ref{line:before case} in the execution of {\sc PowerMethod}($\epsilon,\AA$). Let $\vv_{1},\cdots,\vv_{k}$ be a sequence of updates generated by an oblivious adversary and define $\AAtil=\AA-\sum_{i=1}^{k}\vv_{i}\vv_{i}^{\top}$.
	
	Suppose that $\lambda_{\max}(\AA)\ge 1-\epsilon$ and $\ww^{\top}\AAtil\ww<1-40\epsilon$ for all $\ww\in\WW$. Then, with probability at least $1-\frac{50\log\frac{n}{\epsilon}}{n^{2}}$, for some $\nu\in\mathcal{I}$,
	\begin{itemize}
		\item $\dim{T_{\nu}-\tilde{T}}\geq\frac{\epsilon}{300\log\frac{n}{\epsilon}}d_{\nu}$ if $d_{\nu}\geq\frac{3000\log n\log\frac{n}{\epsilon}}{\epsilon}$, or
		\item $\dim{T_{\nu}-\tilde{T}}\geq1$ if $d_{\nu}<\frac{3000\log n\log\frac{n}{\epsilon}}{\epsilon}$.
	\end{itemize}
\end{restatable}


We prove this lemma in \Cref{sec:progress}. Intuitively speaking, it means that, whenever  Line~\ref{algline:PM} of \Cref{alg:DynamicMaxPM} is executed, there is some important level $\nu$ such that an $\Omega(\eps/\poly \log(n/\eps))$-fraction of eigenvalues of $\AA$ at level $\nu$ have decreased in value. This is the crucial place where we exploit an oblivious adversary.

Given \Cref{lem:EigenspaceChange}, the remaining proof of \Cref{lem:BoundW} follows a potential function analysis which is presented in detail in Section~\ref{sec:proof key}. We consider potentials $\Phi_j = \sum_{\nu=0}^jd_{\nu}$ for $j = 0,\cdots,15\log\frac{n}{\epsilon}-1$. The main observation is that $\Phi_j$ is non-increasing over time for all $j$, and whenever there exists $\nu_0\in \mathcal{I}$ that satisfies the condition of Lemma~\ref{lem:EigenspaceChange}, $\Phi_{\nu_0}$ decreases by $\dim{T_{\nu_0}-\tilde{T}}$. Since $\dim{T_{\nu_0}-\tilde{T}} \geq \Omega(\epsilon/\poly\log(n/\epsilon))d_{\nu_0}$ and $\nu_0\in \mathcal{I}$, i.e., $\Phi_{\nu_0} = \sum_{\nu<\nu_0}d_{\nu} + d_{\nu_0} \leq d_{\nu_0} \left(\frac{O(\log^3\frac{n}{\epsilon})}{\epsilon}+1\right) $,  we can prove that $\Phi_{\nu_0}$ decreases by a multiplicative factor of $\Omega(1-\epsilon^2/\poly\log(n/\epsilon))$. As a result, every time our algorithm executes Line~\ref{algline:PM}, $\Phi_{j}$ decreases by a multiplicative factor for some $j$, and since we have at most $15\log\frac{n}{\epsilon}$ values of $j$, we can only have $\poly(\log n/\epsilon)$ executions of Line~\ref{algline:PM}.

It remains to describe how we prove Lemma~\ref{lem:EigenspaceChange} at a high level. We can write $\ww^{\top}\AAtil\ww$ for any $\ww \in \WW$ as
\[
\ww^{\top}\AAtil\ww = \ww^{\top}\AA\ww - \ww^{\top}\VV\ww,
\]
for $\VV = \sum_{i=1}^k\vv_i\vv_i^{\top}$. 
Our strategy is to show that:
\begin{align*}\label{star}
        &\text{If } \dim{T_{\nu}-\tilde{T}}\text{ does not satisfies the inequalities in \Cref{lem:EigenspaceChange} for all }\nu\in \mathcal{I},\\ 
        &\text{then }\ww^{\top}\VV\ww \leq 35\epsilon \text{ for all } \ww \in \WW. \tag{$\star$}
\end{align*}    
Given \eqref{star} as formalized later in \Cref{cl:progress}, we can conclude \Cref{lem:EigenspaceChange} because, from the definition of $\AA$ and $\AAtil$, we have that for some $\ww\in \WW$, $\ww^{\top}\AA\ww \geq 1-5\epsilon$ by \Cref{thm:StaticPower} and $\ww^{\top}\AAtil\ww <1-40\epsilon$. As a result for this $\ww$, $\ww^{\top}\VV\ww >35\epsilon$. Now, by contra-position of \eqref{star}, we have that $\dim{T_{\nu}-\tilde{T}}$ is large for some $\nu \in \mathcal{I}$.

To prove \eqref{star}, we further decompose $\ww^{\top}\VV\ww$ as
\[
\ww^{\top}\VV\ww = \ww^{\top}\VV_{\tilde{T}}\ww+\sum_{\nu = 0}^{15\log\frac{n}{\epsilon}-1}\ww^{\top}\VV_{T_{\nu} -\tilde{T}}\ww+ \ww^{\top}\VV_{\overline{T}}\ww.
\]

In the above equation, $\VV_{\tilde{T}}= \Pi_{\tilde{T}}\VV\Pi_{\tilde{T}},\VV_{T_{\nu} -\tilde{T}}=\Pi_{\nu}\VV\Pi_{\nu}$, and $\VV_{\overline{T}} = \Pi_{\overline{T}}\VV\Pi_{\overline{T}}$ where $\Pi_{\tilde{T}},\Pi_{\nu},\Pi_{\overline{T}}$ denote the projections matrices that project any vector onto the spaces $\tilde{T}$, $T_{\nu}-\tilde{T}$, and $\overline{T}$ respectively\footnote{Suppose a subspace $S$ is spanned by vectors $\uu_1,\dots,\uu_k$. Let $\UU = [\uu_1,\dots,\uu_k]$. Recall that the projection matrix onto $S$ is $\UU(\UU^\top \UU)^{-1} \UU^\top$.}. Refer to Section~\ref{sec:progress} for proof of why such a split is possible. Our proof of \eqref{star} then bounds the terms on the right-hand side. Let us consider each term separately.
\begin{enumerate}
	\item $\ww^{\top}\VV_{\tilde{T}}\ww$: We prove that this is always at most $10\epsilon(1+\epsilon)$ (Equation~\eqref{eq:V2}). From the definition of $\VV,$ 
	\[
	\ww^{\top}\VV_{\tilde{T}}\ww = \ww^{\top}\Pi_{\tilde{T}}\AA\Pi_{\tilde{T}}\ww - \ww^{\top}\Pi_{\tilde{T}}\AAtil\Pi_{\tilde{T}}\ww.
	\] 
	Since $\Pi_{\tilde{T}}\ww$ is the projection of $\ww$ along the large eigenspace of $\AAtil$, the second term on the right-hand side above is large, i.e. $\geq (1- 10\epsilon)\lambda_0\|\Pi_{\tilde{T}}\ww\|^2$. The first term on the right-hand side can be bounded as, $\ww^{\top}\Pi_{\tilde{T}}\AA\Pi_{\tilde{T}}\ww \leq \|\AA\|\|\Pi_{\tilde{T}}\ww\|^2 \leq \lambda_0 \|\Pi_{\tilde{T}}\ww\|^2$.
 Therefore the difference on the right-hand side is at most $10\epsilon\lambda_0\|\Pi_{\tilde{T}}\ww\|^2 \leq 10 \epsilon\lambda_0\|\ww\|^2 = 10\epsilon \lambda_0 \leq 10\epsilon(1+\epsilon)$.
	\item $\ww^{\top}\VV_{\overline{T}}\ww$: Observe that this term corresponds to the projection of $\ww$ along the space spanned by the eigenvalues of $\AA$ of size at most $1-3\epsilon$. Let $\uu_i$ and $\lambda_i$ denote an eigenvector and eigenvalue pair with $\lambda_i<1-3\epsilon$. Since the power method can guarantee that $\ww^{\top}{\uu_i}\approx\lambda_i^{2K}$, we have  $\lambda_i^{2K} \leq (1-3\epsilon)^{2K}\leq \poly\left(\frac{\epsilon}{n}\right)$ is tiny. So we have that $\ww^{\top}\VV_{\overline{T}}\ww\leq \epsilon$ (Lemma~\ref{lem:boundLowEV}).

	\paragraph{}Before we look at the final case, we define a basis for the space $T_{\nu}$.
	\begin{definition}[Basis for $T_{\nu}$]\label{def:Basis} Let $T_{\nu}$ be as defined in Definition~\ref{def:SpaceA}.  Define indices $a_{\nu}$ and $b_{\nu}$ with $b_{\nu}-a_{\nu}+1 = d_{\nu}$ such that the basis of $T_{\nu}$ is given by $\uu_{a_{\nu}},\cdots, \uu_{b_{\nu}}$, where $\uu_1,\uu_2,\cdots,\uu_n$ are the eigenvectors of $\AA$ in decreasing order of eigenvalues.
	\end{definition}

    \item $\ww^{\top}\VV_{T_{\nu} -\tilde{T}}\ww$: For this discussion, we will ignore the constant factors and assume that the high probability events hold. Let $\Pi_{\nu}$ denote the projection matrix for the space $T_{\nu}-\Ttil$. Observe that $\ww^{\top}\VV_{T_{\nu}-\Ttil}\ww=\ww^{\top}\Pi_{\nu}\VV\Pi_{\nu}\ww\le\|\VV\|\|\Pi_{\nu}\ww\|^2 \le (1+\epsilon)\|\Pi_{\nu}\ww\|^2$, where the last inequality is because $\AAtil=\AA-\VV\succeq0$, and therefore, $\|\VV\|\le\|\AA\|\le(1+\epsilon)$. Hence, it suffices to bound $\|\Pi_{\nu}\ww\|^2 = O(\epsilon)$. 

We can write $\ensuremath{\ww=\frac{\sum_{i=1}^{n}\lambda_{i}^{K}\alpha_{i}\uu_{i}}{\sqrt{\sum_{i=1}^{n}\lambda_{i}^{2K}\alpha_{i}^{2}}}}$ where $\lambda_{i},\uu_{i}$'s are the eigenvalues and eigenvectors of $\AA$ and $\alpha_{i}\sim N(0,1)$. 
Define $\zz = \sum_{i=1}^n z_i \uu_i$ where $z_i = \lambda_i^K\alpha_i$. That is, 
$\ww=\frac{\zz}{\|\zz\|}$. Since $\|\Pi_{\nu}\ww\| = \|\Pi_{\nu}\zz\|/\|\zz\|$, it suffices to show that $\|\Pi_{\nu}\zz\|^2 \le O(\epsilon) \| \zz\|^2$. We show this in two separate cases. In both cases, we start with the following bound
\[
\|\Pi_{\nu}\zz\|^{2}\le\lambda_{a_{\nu}}^{2K}\cdot\dim{T_{\nu}-\tilde{T}},
\]
which holds with high probability. To see this, let $\boldsymbol{g}_{\nu}\sim N(0,1)$ be a gaussian vector in the space $T_{\nu}-\tilde{T}$. 
We can couple $\textbf{g}_\nu$ with $\Pi_{\nu}\zz$ so that $\Pi_{\nu}\zz$ is dominated by $\lambda_{a_{\nu}}^{K}\cdot \boldsymbol{g}_{\nu}$. So $\|\Pi_{\nu}\zz\|^{2}\le\lambda_{a_{\nu}}^{2K}\|\boldsymbol{g}_{\nu}\|^{2}$. By \Cref{lem:NormG}, the norm square of gaussian vector is concentrated to its dimension so $\|\boldsymbol{g}_{\nu}\|^{2}\le\dim{T_{\nu}-\tilde{T}}$ with high probability, thus proving the inequality. Next, we will bound $\dim{T_{\nu}-\tilde{T}}$ in terms of $\|\zz\|$ in two cases. 

\paragraph{When $\nu\protect\notin{\cal I}$ (\Cref{lem:NotImp}):}

From the definition of the important levels, we have 
\[
\dim{T_{\nu}-\tilde{T}}\leq d_{\nu}\leq\frac{O(\epsilon)}{\log^{3}\frac{n}{\epsilon}}\sum_{\nu'<\nu}d_{\nu'}.
\]
Now, we have $\sum_{\nu'<\nu}d_{\nu'}\approx\sum_{i=1}^{b_{\nu-1}}\alpha_{i}^{2}$ because $\alpha_{i}\sim N(0,1)$ is gaussian and the norm square of gaussian vector is concentrated to its dimension (\Cref{lem:NormG}). Since $\alpha_{i}=z_{i}/\lambda_{i}^{K}$, we have that
\[
\sum_{\nu'<\nu}d_{\nu'}\approx\sum_{i=1}^{b_{\nu-1}}\alpha_{i}^{2}=\sum_{i=1}^{b_{\nu-1}}\frac{z_{i}^{2}}{\lambda_{i}^{2K}}\le\|\zz\|^{2}/\lambda_{b_{\nu-1}}^{2K}.
\]
Therefore, we have 
\[
\|\Pi_{\nu}\zz\|^2 \le\lambda_{a_{\nu}}^{2K}\dim{T_{\nu}-\tilde{T}}\le\left(\frac{\lambda_{a_{\nu}}}{\lambda_{_{b_{\nu-1}}}}\right)^{2K}\frac{O(\epsilon)}{\log^{3}\frac{n}{\epsilon}}\|\zz\|^{2}\le O(\epsilon)\|\zz\|^{2}
\]
where the last inequality is trivial because $\lambda_{b_{\nu-1}}\ge\lambda_{a_{\nu}}$ by definition.

\paragraph{When $\nu\in{\cal I}$ (\Cref{lem:GaussianProjD}):}

In this case, according to \eqref{star}, we can assume $$\dim{T_{\nu}-\tilde{T}}\lesssim\epsilon d_{\nu}.$$
Again, by \Cref{lem:NormG}, we have that $d_{\nu}\approx\sum_{i=a_{\nu}}^{b_{\nu}}\alpha_{i}^{2}$ because $\alpha_{i}\sim N(0,1)$ is gaussian. Since $\alpha_{i}=z_{i}/\lambda_{i}^{K}$, we have
\[
d_{\nu}\approx\sum_{i=a_{\nu}}^{b_{\nu}}\alpha_{i}^{2}=\sum_{i=a_{\nu}}^{b_{\nu}}\frac{z_{i}^{2}}{\lambda_{i}^{2K}}\le\|\zz\|^{2}/\lambda_{b_{\nu}}^{2K}.
\]
Therefore,
\[
\|\Pi_{\nu}\zz\|^2\le\lambda_{a_{\nu}}^{2K}\dim{T_{\nu}-\tilde{T}}\le\left(\frac{\lambda_{a_{\nu}}}{\lambda_{_{b_{\nu}}}}\right)^{2K}\epsilon\|\zz\|^{2}\le O(\epsilon)\|z\|^{2}
\]
where the last inequality is because $\ensuremath{\left(\frac{\lambda_{a_{\nu}}}{\lambda_{b_{\nu}}}\right)^{2K}\leq\left(\frac{1-\frac{\nu\epsilon}{5\log\frac{n}{\epsilon}}}{1-\frac{(\nu+1)\epsilon}{5\log\frac{n}{\epsilon}}}\right)^{2K}\approx\left(1+\frac{\epsilon}{2\log\frac{n}{\epsilon}}\right)^{2K}\approx O(1)}.$

\end{enumerate}
From these three cases, we can conclude that if $\dim{T_{\nu}-\tilde{T}}$ is small for all $\nu\in \mathcal{I}$, then $\ww^{\top}\VV\ww \leq 35\epsilon$, proving our claim.

In the remaining sections, 
we give formal proofs of the claims made in this section. In \Cref{sec:correct}, we prove the main result, \Cref{thm:upper}, assuming the key lemma. In \Cref{sec:proof key}, we prove the key lemma, \Cref{lem:BoundW}, assuming the \Cref{lem:EigenspaceChange}. Finally, we prove \Cref{lem:EigenspaceChange} in \Cref{sec:progress}


\subsection{Proof of the Main \Cref{thm:upper} assuming the Key \Cref{lem:BoundW}}\label{sec:correct}

Here, we formally prove \Cref{thm:upper} assuming the key \Cref{lem:BoundW}. We will first prove the correctness and then bound the total runtime.


\paragraph{Correctness.}
The following formalizes the correctness guarantee of  Algorithm~\ref{alg:DynamicMaxPM}. 

\begin{lemma}\label{lem:DynCorrectAns}
Let $\epsilon>1/n$. With probability at least $1-1/n$, the following holds for all time step $t \ge 1$: 
if the maximum eigenvalue of $\AA_t$ is at least $1-\frac{\epsilon}{\log n}$, {\sc Update}($\vv_t$) returns $[r_t,\WW_t]$.
\end{lemma}
\begin{proof}\textcolor{red}{TOPROVE 0}\end{proof}



\paragraph{Runtime.}\label{sec:runtime}

Next, we bound the runtime of the various lines of Algorithm~\ref{alg:DynamicMaxPM}. 
%
\begin{lemma}\label{lem:SameW}
For a fixed $\ww$ and any $t$, we can update $\ww^{\top}\AA_{t-1}\ww$ to $\ww^{\top}\AA_t\ww$ in time $O(nnz(\vv_t))$.
\end{lemma}
\begin{proof}\textcolor{red}{TOPROVE 1}\end{proof}
%
\begin{lemma}\label{lem:DiffW}
Fix time $t$. Given $\ww$ as input, we have that $\ww^{\top}\AA_t\ww$ and $\AA_t\ww$ can be computed $O\left(nnz(\AA_0) +\sum_{i=1}^t nnz(\vv_i)\right)$ time.
\end{lemma}
\begin{proof}\textcolor{red}{TOPROVE 2}\end{proof}
%
\begin{lemma}\label{lem:PMt}For any time $t$, we can implement {\sc PowerMethod}($\epsilon,\AA_t$) in time at most 
\[
O\left(\frac{\log n\log \frac{n}{\epsilon}}{\epsilon}\left(nnz(\AA_0) +\sum_{i=1}^t nnz(\vv_i)\right)\right).
\]
\end{lemma}
\begin{proof}\textcolor{red}{TOPROVE 3}\end{proof}

 Given the above results, we can now prove Theorem~\ref{thm:upper}.
\subsubsection*{Proof of Theorem~\ref{thm:upper}}
\begin{proof}\textcolor{red}{TOPROVE 4}\end{proof}


\input{KeyLemma}

\section{Conditional Lower Bounds for an Adaptive Adversary}\label{sec:Adap}

In this section, we will prove a conditional hardness result for algorithms against adaptive adversaries. In particular, we will prove \Cref{thm:lower}.
Consider \Cref{alg:red} for solving \Cref{prob:factor}. 
The only step in \Cref{alg:red} whose implementation is not specified is Line~\ref{line:approx eig}. We will implement this step using an algorithm for \Cref{prob:dyn}.



\begin{algorithm}
\caption{Algorithm for Checking PSDness}\label{alg:red}
 \begin{algorithmic}[1]
\Procedure{CheckPSD}{$\delta,\kappa,\AA$}
\State $\epsilon \leftarrow \min\{1- n^{-o(1)}, (1-\delta)/(1+\delta)\}$
\State $T \leftarrow \frac{2n}{\epsilon(1-\epsilon)^2}\log \frac{\kappa}{\delta}$
\State $\AA_0 \leftarrow \AA$
\State $\mu_0 = 0, \ww_0 = 0$
\For{$t  = 1,2,\cdots,T$}
\State $\AA_t = \AA_{t-1} - \frac{\mu_{t-1}}{10}\ww_{t-1}\ww_{t-1}^{\top}$\label{line:red update}
\State $(\mu_t,\ww_t)\leftarrow \epsilon$-approximate maximum eigenvalue and eigenvector of $\AA_t$ (Equations~\eqref{eq:epsEigvalue},\eqref{eq:epsEigvec})\label{line:approx eig}
\If{$\mu_t<0$}
\State \Return {\sc False}:$\AA$ is not PSD
\EndIf
\EndFor
\State {$\sigma^2 \gets $PowerMethod($\epsilon,\AA_T^{\top}\AA_T$)}\label{line:last check}
\If{$0\leq \sigma \leq \frac{(1+\epsilon)\mu_1\delta}{\kappa}$}\label{line:red lastcheck}
\State \Return $\XX=\frac{1}{\sqrt{10}}\begin{bmatrix}\sqrt{\mu_1}\ww_1 & \sqrt{\mu_2}\ww_2 & \cdots & \sqrt{\mu_T}\ww_T\end{bmatrix}$
\Else
\State \Return {\sc False}:$\AA$ is not PSD
\EndIf
\EndProcedure 
 \end{algorithmic}
\end{algorithm}

\paragraph{High-level idea.}
 Overall for our hardness result, we use the idea that an adaptive adversary can use the maximum eigenvectors returned to perform an update. This can happen $n$ times and in the process, we would recover the entire eigen-decomposition of the matrix, which is hard. Now consider Algorithm~\ref{alg:red}. We claim that \Cref{alg:red} solves \Cref{prob:factor}. 
At the first glance, this claim looks suspicious because the input matrix for \Cref{prob:factor} might not be PSD, but the dynamic algorithm for \Cref{prob:dyn} at Line~\ref{line:approx eig} has any guarantees only when the matrices remain PSD. 
However, the reduction does work by crucially exploiting \Cref{prop:assume}. The high-level idea is as follows. 
\begin{itemize}
    \item If the input matrix $\AA$ is initially PSD, then we can show that $\AA_t$ remains PSD for all $t$  by exploiting \Cref{prop:assume}, (see \Cref{lem:orthoUpdate}). So, the approximation guarantee of the algorithm at Line~\ref{line:approx eig} is valid at all steps.
    From this guarantee, $\|\AA_T\|$ must be tiny since we keep decreasing the approximately maximum eigenvalues (see \Cref{lem:RedAns}). At the end, the reduction will return $\XX$.
    \item If the input matrix $\AA$ is initially \emph{not} PSD, there must exist a direction $\vv$ such that $\vv^{\top}\AA\vv <0$. Since in the reduction, we update $\AA_T = \AA - \WW$ for some $\WW \succeq 0$, we must have that $\vv^{\top}\AA_T\vv <\vv^{\top}\AA\vv$. That is, this negative direction remains negative or gets even more negative. It does not matter at all what guarantees the algorithm at Line~\ref{line:approx eig} has. We still have that $\|\AA_T\|$ cannot be tiny. We can distinguish whether $\|\AA_T\|$ is tiny or not using the static power method at Line~\ref{line:last check}, and, hence, we will return {\sc False} in this case (see \Cref{lem:RedAns}).
\end{itemize}



% {\color{blue} We now give a high-level reasoning to why our \Cref{alg:red} returns the right answer before we formally prove the reduction. When the input matrix $\AA$ is PSD, then \Cref{def:super} guarantees that $\AA_t$ remains PSD for all $t$ (see Lemma~\ref{lem:orthoUpdate}) and $\|\AA_T\|$ is small in the end (see Lemma~\ref{lem:RedAns}) as required. When $\AA$ is not PSD, there must exist a direction $\vv$ such that $\vv^{\top}\AA\vv <0$. Since in the algorithm, we create $\AA_T = \AA - \WW$ for $\WW \succeq 0$, we must have that $\vv^{\top}\AA_T\vv <\vv^{\top}\AA\vv.$ Now, since there is a large negative eigenvalue of $\AA$ and $\AA_T$, the eigenvalue with the largest magnitude ($\sigma$ computed in the end) must be large. Therefore, the algorithm must return that $\AA$ is not PSD. We now give the formal reduction.}


\paragraph{Analysis.}
We prove the guarantees of the output of Algorithm~\ref{alg:red} when $\ww_t$'s satisfy \Cref{def:super} for all $t$.


\begin{lemma}\label{lem:orthoUpdate}
In Algorithm~\ref{alg:red}, let $\ww_t$'s, $t=1,\cdots, T$ be generated such that they additionally satisfy \Cref{def:super}. If $\AA_0\succeq 0$, then $\AA_t \succeq 0$ for all $t$.
\end{lemma}
 We would like to point out that our parameter $\epsilon$ is quite large. This just implies that our reduction can work even if we find crude approximations to the maximum eigenvector as long as this is along the directions with large eigenvalue, since $\ww$ also has to satisfy \Cref{def:super}.
\begin{proof}\textcolor{red}{TOPROVE 5}\end{proof}


\begin{lemma}\label{lem:RedAns}
In Algorithm~\ref{alg:red}, let $\ww_t$'s, $t=1,\cdots, T$ be generated such that they additionally satisfy \Cref{def:super}. 
\begin{itemize}
    \item If $\AA\succeq 0$, then \Cref{alg:red} returns $\XX$ such that $\|\AA-\XX\XX^{\top}\|\leq \delta \min_{\|\xx\|=1}\|\AA\xx\|$. 
    \item If $\AA$ is not psd, then \Cref{alg:red} returns {\sc False}.
\end{itemize}
\end{lemma}
\begin{proof}\textcolor{red}{TOPROVE 6}\end{proof}


\paragraph{Proof of Theorem~\ref{thm:lower}.} 
We are now ready to prove our conditional lower bound.
\begin{proof}\textcolor{red}{TOPROVE 7}\end{proof}








