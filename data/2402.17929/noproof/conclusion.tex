

\section{Conclusion and Open Problems}\label{sec:open}


\paragraph{Upper Bounds.}
We have presented a novel extension of the power method to the dynamic setting. Our algorithm from \Cref{thm:upper} maintains a multiplicative $(1+\epsilon)$-approximate maximum eigenvalue and eigenvector of a positive semi-definite matrix that undergoes decremental updates from an oblivious adversary. The algorithm has polylogarithmic amortized update time per non-zeros in the updates. 

Our algorithm is simple, but our analysis is quite involved. While we believe a tighter analysis that improves our logarithmic factors is possible, it is an interesting open problem to give a simpler analysis for our algorithm. 
%
Other natural questions are whether we can get similar algorithms in incremental or fully dynamic settings and whether one can get a worst-case update time.


\paragraph{Lower Bounds.}
We have shown a conditional lower bound for a class of algorithms against an adaptive adversary in \Cref{thm:lower}. It would also be very exciting to generalize our lower bound to hold for any algorithm against an adaptive adversary, as that would imply an oblivious-vs-adaptive separation for a natural dynamic problem. 

\paragraph{Incremental Updates.} We believe that the corresponding incremental updates problem, i.e., we update the matrix as $\AA_t \gets \AA_{t-1}+\vv_t\vv_t^{\top}$ cannot be solved in polylogarithmic amortized update time, even when the update sequence $\vv_t$'s are from an oblivious adversary. 
At a high level, the incremental version of our problem seems significantly harder for the following reasons. When we perform decremental updates to a matrix, the new maximum eigenvector must be a part of the eigenspace spanned by the original maximum eigenvectors. Furthermore, it is easy to detect whether the maximum eigenvalue has gone down as we have shown in our paper. For the incremental setting, it is possible that after an update the maximum eigenvalue has gone up and the new maximum eigenvector is a direction that was not the previous one or the update direction and in such cases we cannot really detect this quickly with known information on previous eigenvectors and update directions. This can also happen $n$ times and in every such case, we have to compute the eigenvalue and eigenvector from scratch. Therefore, we leave lower bounds and algorithms for incremental setting as an open problem. 

\paragraph{Dynamic SDPs.}
As discussed in \Cref{sec:dyn psd}, \Cref{thm:upper} can be viewed as a starting point towards a dynamic algorithm for general positive semi-definite programs. Can we make further progress? The dynamic semi-definite program problem, even with just two matrix constraints, already seems to be challenging. 

One promising approach to attack this problem is to dynamize the matrix multiplicative weight update (MMWU) method for solving a packing/covering SDP \cite{peng2012faster} since the corresponding approach was successful for linear programs -- the near-optimal algorithms of \cite{bhattacharya2023dynamic} are essentially dynamized multiplicative weight update methods for positive linear programs. However, in our preliminary study exploring this approach, we could only obtain an algorithm that solves Problem~\ref{prob:dynsdp}, which has a single matrix constraint, and  solves~\Cref{prob:dyn} partially, i.e., maintains an approximate eigenvalue only. The main barrier in this approach is that the algorithm requires maintaining the exponential of the sum of the constraint matrices, and to do this fast, we require that for any two constraint matrices $\AA$ and $\BB$, $e^{\AA+\BB}= e^{\AA} e^{\BB}$ which only holds when $\AA$ and $\BB$ commute i.e., $\AA\BB = \BB\AA$. Note that when $\AA$ and $\BB$ are diagonal, this is true; therefore, we can obtain the required algorithms for positive LPs. Even when we have just two constraint matrices where one of them is a diagonal matrix, this remains an issue as the matrices still do not commute. 
