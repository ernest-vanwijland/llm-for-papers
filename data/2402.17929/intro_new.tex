%!TEX root = mainEV.tex



\section{Introduction}

Computing eigenvalues and eigenvectors of a matrix is predominant in several applications, including principle component analysis, clustering in high-dimensional data, semidefinite programming, spectral graph partitioning, and algorithms such as Google's PageRank algorithm. In the era of massive and dynamic datasets, developing algorithms capable of efficiently updating spectral information with changing inputs has become indispensable.

The study of the change in the spectrum of a matrix undergoing updates spans over five decades, with \textcite{golub1973some} providing the first algebraic characterization of the change for positive semi-definite matrices via the {\it secular equation}. This result offered an explicit formula for exactly computing all new eigenvalues when a matrix undergoes a single rank-one update, assuming knowledge of the entire eigen-decomposition of the initial matrix. Subsequently, \textcite{bunch1978rank} showed how to additionally compute eigenvectors explicitly, and handle the case of repeated eigenvalues of the initial matrix. A decade later, \textcite{arbenz1988restricted} extended the works of \cite{golub1973some,bunch1978rank} to compute all new eigenvalues and eigenvectors of a positive semi-definite matrix undergoing an update of the form of a small-rank matrix, or a single batch update. Further works extend these techniques to computing singular values \cite{stange2008efficient}, as well as computing new eigenvalues and eigenvectors when only the partial eigen-decomposition of the initial matrix is known \cite{mitz2019symmetric}. However, all these works require a full eigen-decomposition of the initial matrix and only handle a single update to the matrix. 

Another independent line of work aims at finding a small and sparse matrix that has eigenvalues close to the original matrix. 
A recent work in this direction by \textcite{bhattacharjee2023universal} provides a universal sparsifier $\SS$ for positive semi-definite matrices $\AA$, such that $\SS$ has at most $n/\epsilon^4$ non-zero entries and $\|\AA-\AA\circ\SS\|\leq \epsilon n$. \textcite{swartworth2023optimal} give an alternate technique by using {\it gaussian sketches} with $O(1/\epsilon^2)$ rows to approximate all eigenvalues of $\AA$ to an additive error of $\epsilon\|\AA\|_F$. The algorithms that find sparse matrices approximating the eigenvalues in these works may be extended to handle updates in the initial matrix quickly. However, the approximation of the eigenvalues achieved is quite crude, i.e., at least $\epsilon n$ additive factor. This approximation is also shown to be tight for such techniques.



Now consider the simpler task of only maintaining the maximum eigenvalue and eigenvector of a matrix $\AA$ as it undergoes updates. As we have seen above, known methods based on algebraic techniques require full spectral information before computing the new eigenvalues, and maintaining the entire eigen-decomposition can be slow. Sparsification-based algorithms are fast but only achieve large additive approximations of at least $\epsilon n$, which is not quite satisfactory. Works on streaming PCA, which have a similar goal of maintaining large eigenvectors focus on optimizing the space complexity instead of runtimes \cite{allen2017first}.
Previous works based on dynamically maintaining the matrix inverse can maintain $(1+\eps)$-multiplicative approximations to the maximum eigenvalues of an $n \times n$ symmetric matrix undergoing single-entry updates with an update time of $O(n^{1.447}/\poly(\eps))$ \cite{sankowski2004dynamic}. This was further improved to $O(n^{1.407}/\poly(\eps))$\cite{van2019dynamic}. The update time is even slower for row, column, or rank-one updates. In any case, it takes polynomial time per non-zeros of the updates. This leads to the following natural question:

\begin{center}
    \emph{Is there a dynamic algorithm that can maintain a multiplicative approximation of the maximum eigenvalue of a matrix using polylogarithmic update time per non-zeros in the update?}\par
\end{center}

In this paper, we study the problem of maintaining a $(1+\epsilon)$-multiplicative approximation to the maximum eigenvalue and eigenvector of a positive semi-definite matrix as it undergoes a sequence of rank-one updates that may only decrease the eigenvalues. We note that this is equivalent to finding the minimum eigenvalue and eigenvector of a positive semi-definite matrix undergoing rank-one updates that may only increase the eigenvalues.
%
We now formally state our problem.
\begin{problem}
	[Decremental Approximate Maximum Eigenvalue and Eigenvector]\label{prob:dyn}We are given a size parameter $n$, an accuracy parameter $\epsilon\in(0,1)$, a psd matrix $\AA_{0}\succeq0$ of size $n\times n$, and an online sequence of vectors $\vv_{1},\vv_{2},\dots,\vv_{T}$ that update $\AA_{t}\gets \AA_{t-1}-\vv_{t}\vv_{t}^{\top}$ with a promise that $\AA_{t}\succeq0$ for all $t$. 
	The goal is to maintain an $\epsilon$-approximate eigenvalue $\lambda_{t}\in\mathbb{R}$ and $\epsilon$-approximate eigenvector $\ww_{t}\in\mathbb{R}^{n}$ of $\AA_{t}$ for all time $t$. That is $\lambda_t$ and unite vector $\ww_t$ satisfying,
	\begin{equation}\label{eq:epsEigvalue}
	\max_{\uu:\textrm{unit}}\uu^{\top}\AA_{t}\uu \ge \lambda_{t} \ge (1-\epsilon)\max_{\uu:\textrm{unit}}\uu^{\top}\AA_{t}\uu,
	\end{equation}
	and
	\begin{equation}\label{eq:epsEigvec}
	\ww_{t}^{\top}\AA_{t}\ww_{t}\ge(1-\epsilon)\max_{\uu:\textrm{unit}}\uu^{\top}\AA_{t}\uu.
	\end{equation}
\end{problem}



\subsection{Our Results}

We give an algorithm for the Decremental Approximate Maximum Eigenvalue and Eigenvector Problem that has an amortized update time of $\approx \Otil(nnz(\vv_t))\leq \Otil(n)$\footnote{$\Otil$ hides polynomials in $\log n$.}. In other words, the total time required by our algorithm over $T$ updates is at most $\Otil(nnz(\AA_0) + \sum_{i=1}^T nnz(\vv_i))$. Observe that our algorithm only requires time that is $\Otil(1) \times$ (time required to read the input) and can handle any number of decremental updates. Our algorithm works against an {\it oblivious} adversary, i.e., the update sequence is fixed from the beginning. This is the first algorithm that can handle a sequence of updates while providing a multiplicative approximation to the eigenvalues and eigenvectors in a total runtime of $\leq \Otil(n^2 + n\cdot T)$ and an amortized update time faster than previous algorithms by a factor of $n^{\Omega(1)}$. Formally, we prove the following:
\begin{theorem}
	\label{thm:upper}There is an algorithm for \Cref{prob:dyn} under a sequence of $T$ decreasing updates, that given parameters $n$, $\AA_0$, and $\epsilon>1/n$ as input, with probability at least $1-1/n$ works against an oblivious adversary in total time,
 \[
 O\left(\frac{\log^{3}n \log^6 \frac{n}{\epsilon}\log \frac{\lambda_{\max}(\AA_0)}{\lambda_{\max}(\AA_T)}}{\epsilon^{4}}\left(nnz(\AA_0) + \sum_{i=1}^T nnz(\vv_i)\right) \right).
 \]
\end{theorem}

 Our algorithm is a novel adaptation of the classical {\it power method} (see, e.g., \cite{trefethen2022numerical}) 
 to the dynamic setting, along with a new analysis that may be of independent interest. 

 Our work can also be viewed as the first step towards generalizing the dynamic algorithms for solving positive linear programs of \cite{bhattacharya2023dynamic} to solving dynamic positive semi-definite programs. We discuss this connection in detail in \Cref{sec:dyn psd}.

\subsection{Towards Separation between Oblivious and Adaptive adversaries}
 We also explore the possibility of removing the assumption of an oblivious adversary and working against {\it adaptive adversaries}. Recall that update sequences are given by adaptive adversaries when the update sequence can depend on the solution of the algorithm. 
 
 We show that if there is an algorithm for \Cref{prob:dyn} with a total running time of at most $\Otil(n^2)$ such that the output $\ww_t$'s satisfy an additional natural property (which is satisfied by the output of the power method, but \emph{not} by our algorithm), then it contradicts the hardness of a well-known barrier in numerical linear algebra, therefore ruling out such algorithms.
 
 We first state the barrier formally, and then state our result for adaptive adversaries.
Recall that, given a matrix $\AA$, the condition number of $\AA$ is $\frac{\max_{x:\textrm{unit}}\|\AA\xx\|}{\min_{x:\textrm{unit}}\|\AA\xx\|}$.

\begin{problem}[Checking psdness with certificate]\label{prob:factor}Given $\delta\in(0,1)$, parameter $\kappa$, and a symmetric matrix $\AA$ of size $n\times n$ with condition number at most $\kappa$, either 
	\begin{itemize}
		\item Compute a matrix $\XX$ where $\|\AA-\XX\XX^{T}\|\le\delta\min_{\|\xx\|=1}\|\AA\xx\|$, certifying that $\AA$ is a psd matrix, or
		\item Report that $\AA$ is not a psd matrix.
	\end{itemize}
\end{problem}

Recall that $A$ is a psd matrix iff there exists $\XX$ such that $\AA=\XX\XX^{\top}$. The matrix of $\XX$ is called a \emph{vector realization} of $\AA$, and note that $\XX$ is not unique. The problem above asks to compute a (highly accurate) vector realization of $\AA$. Both eigendecomposition and Cholesky decomposition of $\AA$ give a solution for \Cref{prob:factor}.\footnote{Given an eigendecomposition $\AA=\QQ\Lambda \QQ^{\top}$ where $\Lambda$ is a non-negative diagonal matrix and $\QQ$ is an orthogonal matrix, we set $\XX=\QQ\Lambda^{1/2}$. Given a Cholesky decomposition $\AA=\LL\LL^{\top}$ where $\LL$ is a lower triangular matrix, we set $\XX=\LL$.} Banks et al~\cite{banks2022pseudospectral} showed how to compute with high accuracy an eigendecomposition of a psd matrix in $O(n^{\omega+\eta})$ time for any constant $\eta>0$, where $\omega > 2.37$ is the matrix multiplication exponent. Hence, a vector realization of $\AA$ can be found in the same time. Observe that, when $\delta < \kappa$, \Cref{prob:factor} is \emph{at least as hard as certifying that a given matrix $\AA$ is a psd matrix}.
It is a notorious open problem whether certifying the psd-ness of a matrix can be done faster than $o(n^\omega)$ even when the condition number is polynomial in $n$.
Therefore, we view \Cref{prob:factor} as a significant barrier and formalize it as follows.%



\begin{hypothesis}
	[PSDness Checking Barrier]\label{conj:decomp is hard}There is a constant $\eta>0$ such that, every randomized algorithm for solving \Cref{prob:factor} for instances with $\frac{\kappa}{\delta}\leq \poly(n)$, correctly with probability at least $2/3$ requires $n^{2+\eta}$ time. 
\end{hypothesis}


Our negative result states that, assuming  Hypothesis~\ref{conj:decomp is hard}, there is no  algorithm for Problem~\ref{prob:dyn} against adaptive adversaries with sub-polynomial update time
that maintains $\ww_t$ satisfying an additional property stated below.

\begin{property}\label{def:super}\label{prop:assume} For every $t$ let $\uu_i(\AA_t)$ denote the eigenvectors of $\AA_t$ and $\lambda_i(\AA_t)$ denote the eigenvalues. For all $i$ such that $\lambda_i(\AA_t)\leq \lambda_1(\AA_t)/2$,
\[
\left(\ww_t^{\top}\uu_i\left(\AA_{t}\right)\right)^2 \leq \frac{1}{n^2}\cdot \frac{\lambda_i\left(\AA_{t}\right)}{\lambda_1(\AA_t)}.
\]
\end{property}

\begin{theorem}
	\label{thm:lower}Assuming \Cref{conj:decomp is hard}, there is no algorithm for \Cref{prob:dyn} that maintains $\ww_t$'s additionally satisfying \Cref{def:super}, and given parameters $n$ and $\epsilon=\min\left\{1-\frac{1}{n^{o(1)}},\frac{1-\delta}{1+\delta}\right\}$ as input, works against an adaptive adversary in time $n^{o(1)}\cdot \left(nnz(\AA_0) + \sum_{t=1}^T nnz(\vv_i)\right)$. 
\end{theorem}

Let us motivate \Cref{def:super} from several aspects below. First, in the static setting, this property can be easily satisfied since the static power method strongly satisfies it (see \Cref{thm:StaticPowerMain}). 
Second, the statement of the property itself is a natural property that we might expect from an algorithm. Consider a ``bad'' eigenvector $\uu_i$ whose corresponding eigenvalue is very small, i.e., less than half of the maximum one.
It states that the projection of the output $\ww_t$ along $\uu_i$ should be very small, i.e., a polynomial factor smaller than the projection along the maximum eigenvector. This is intuitively useful because we do not want the output $\ww_t$ to direct to the ``bad'' direction. It should mainly direct along the approximately maximum eigenvector.
Third, we formalize this intuition and crucially exploit \Cref{def:super} to prove a reduction in \Cref{thm:lower}. 
Specifically, \Cref{def:super} allows us to decrease eigenvalues of a psd matrix while maintaining the psd-ness, which is crucial for us. See \Cref{sec:Adap} for more details. 
Lastly, our current algorithm from \Cref{thm:upper}, a dynamic version of the power method, actually maintains $\ww_t$'s that satisfy this property for certain snapshots, but not at every step. Unfortunately, we do not see how to strengthen our algorithm to satisfy \Cref{prop:assume} nor how to remove it from the requirement of \Cref{thm:lower}. We leave both possibilities as open problems.


Understanding the power and limitations of dynamic algorithms against an oblivious adversary vs.~an adaptive adversary has become one of the main research programs in the area of dynamic algorithms. However, finding a natural dynamic problem that separates oblivious and adaptive adversaries is still a wide-open problem.\footnote{
Beimel et al.~\cite{beimel2022dynamic} gave the first separations for artificial problems assuming a strong cryptographic assumption. Bateni et al.~\cite{bateni2023optimal} shows a separation between the adversaries for the {\it $k$-center clustering problem}, however, their results are based on the existence of a black box which the adaptive adversary can control.}
%
% An algorithm is said to be {\it robust} if it works against an adaptive adversary. Recently, Beimel et al.~\cite{beimel2022dynamic} took a complexity-theoretic approach to understanding the gap between the two models of adversaries. From the upper bound side, they showed the first black-box transformation that can transform any non-robust dynamic algorithm for any \emph{estimation }problem\footnote{In estimation problems, an algorithm needs to output one number for each query, such as the size of minimum cut or the distance of any given pair of vertices.} to the corresponding robust algorithm with some overhead. From the lower bound side, they also gave the first separation by showing the two problems that admit fast non-robust algorithms, but any robust algorithms require polynomially slower update time under a strong cryptographic assumption. Further work by Bateni et al.~\cite{bateni2023optimal} shows a separation between the adversaries for the {\it $k$-center clustering problem}, however, their results are based on the existence of a black box which the adaptive adversary can control. Therefore, finding a natural dynamic problem that separates oblivious and adaptive adversaries is still a wide-open problem. 
%
In this paper, we suggest the problem of maintaining approximate maximum eigenvectors as a natural candidate for the separation and 
give some preliminary evidence for this.
% towards this goal, and if one could show the lower bound without requiring \Cref{def:super}, we would have a solution to this long-studied open problem. 

\paragraph{Organization.} In the following sections, we begin with some preliminaries required to prove our results in Section~\ref{sec:prelims}, followed by our algorithm and its analysis in Section~\ref{sec:Obl}, and our conditional lower bound in Section~\ref{sec:Adap}. In Section~\ref{sec:dyn psd} we show connections with positive semi-definite programs, and finally, in Section~\ref{sec:open}, we present some open problems.

