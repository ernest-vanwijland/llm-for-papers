%!TEX root = mainEV.tex

\section{Omitted Proofs}
\Decision*
\begin{proof}
We first estimate the maximum eigenvalue of $\AA_0$. This can be done via the standard power method, or running {\sc PowerMethod}$(\epsilon/4\log n, \AA_0)$ until Line~\ref{line:before case} and returning $\ww\in \WW$ such that $\ww= \arg\max_{\ww\in \WW}\ww^{\top}\AA_0\ww$. From Lemma~\ref{thm:StaticPowerMain}, with probability at least $1-1/n^{10}$, $\ww$ satisfies $\nu = \ww^{\top}\AA_0\ww\geq (1-\epsilon/\log n)\lambda_{\max}(\AA_0)$. Now, $\AA_0/\nu$ has maximum eigenvalue at most $1+\frac{\epsilon}{\log n}$. This procedure takes time at most $O\left(\frac{\log^2 n\log\frac{n}{\epsilon}}{\epsilon}\cdot nnz(\AA_0)\right)$.

   For the sequence of updates, let $t_1,t_2,\cdots,t_k$ denote the time steps where the maximum eigenvalue decreases by a factor of at least $1-\epsilon/\log n$. We begin with solving {\sc DecMaxEV}$(\epsilon,\frac{\AA_0}{\nu},\frac{\vv_1}{\sqrt{\nu}},\cdots, \frac{\vv_T}{\sqrt{\nu}})$ using $\mathcal{A}$.  The algorithm returns $\ww_1,\ww_2,\cdots,\ww_{t_1-1},${\sc False},$\cdots$. Now $\nu$ is an $\epsilon$-approximate maximum eigenvalue for $\AA_1,\cdots \AA_{t_1-1}$, and the vectors $\ww_1,\cdots,\ww_{t_1-1}$ are the required eigenvectors for $\AA_1,\cdots \AA_{t_1-1}$ respectively. We also know that, $\lambda_{\max}(\AA_{t_1})\leq \nu\left(1-\frac{\epsilon}{\log n}\right)$. The total time taken so far is, $O(\frac{\log^2 n\log\frac{n}{\epsilon}}{\epsilon}\cdot nnz(\AA_0) + \mathcal{T})$. 

   Since, $\lambda_{\max}\left(\frac{\AA_{t_1}}{\nu(1-\frac{\epsilon}{\log n})}\right)\leq 1$, again solve {\sc DecMaxEV}$(\epsilon,\frac{\AA_{t_1}}{\nu(1-\epsilon/\log n)},\frac{\vv_{t_1}}{\sqrt{\nu(1-\frac{\epsilon}{\log n})}},\cdots, \frac{\vv_T}{\sqrt{\nu(1-\frac{\epsilon}{\log n})}})$ using $\mathcal{A}$. This time, the algorithm returns $\ww_{t_1+1},\ww_{t_1+2},\cdots, \ww_{t_2-1},${\sc False},$\cdots$. We can now repeat this process starting at $t_2$ until $t_k$.

   The total number of calls to $\mathcal{A}$ can be bounded by the number of times the maximum eigenvalue decreases by a factor of $1-\frac{\epsilon}{\log n}$. This can happen at most $\frac{\log n}{\epsilon}\log \frac{\lambda_{\max}(\AA_0)}{\lambda_{\max}(\AA_T)}$ times. Therefore, the total time required is at most $O\left(\frac{\log^2 n\log\frac{n}{\epsilon}}{\epsilon}\cdot nnz(\AA_0) + \frac{\log n}{\epsilon}\log \frac{\lambda_{\max}(\AA_0)}{\lambda_{\max}(\AA_T)}\mathcal{T}\right)$.
\end{proof}

\subsection*{Power Method}

The following lemma proves that at least one initial random vector has a component along the eigenspace of the maximum eigenvalue.
\begin{lemma}\label{lem:PMHighComp}
Let $\uu_1,\cdots \uu_l$ denote the maximum eigenvectors of $\AA$ and let $\vv^{(0)} \in \mathbb{R}^n$ denote the vector with entries sampled indenpendently from $N(0,1)$, i.e., $\vv^{(0)} = \sum_{i=1}^n\alpha_i\uu_i$, where $\alpha_i \sim N(0,1)$, $\uu_i$'s are eigenvectors of $\AA$. Then, with probability at least $3/4$, $\sum_{i=1}^l\alpha_i^2 \geq \frac{1}{25}$.
\end{lemma}
\begin{proof}
Let $\lambda_1,\lambda_2,\cdots, \lambda_n$ denote the eigenvalues of $\AA$ in decreasing order corresponding to the eigenvectors $\uu_1,\cdots, \uu_n$.  Let $\uu$ be a vector in the span of $\uu_1,\cdots, \uu_l$. We can write, $\uu=\sum_{i=1}^l \beta_i\uu_i$ with $\|\beta\| = 1$. We note that the inner product $\langle \vv^{(0)},\uu\rangle = \sum_{i=1}^l \alpha_i \beta_i$ has mean $0$ and variance $\sum_i \Var(\alpha_i)\beta_i^2 = 1$ (since $\Var(\alpha_i) =1$ and $\sum_i \beta_i^2 = 1$), and is also a gaussian variable, and thus follows the distribution $N(0,1)$. Therefore from the standard gaussian tables, 
\[
\Pr\left[|\langle \vv^{(0)},\uu\rangle|\geq \frac{1}{5}\right] \geq \frac{3}{4}.
\]
Using Cauchy-Schwarz, $|\langle \vv^{(0)},\uu\rangle| \leq \left(\sum_{i=1}^l\alpha_i^2\right)^{1/2}$. As a result, we also have,
\[
\Pr\left[\left(\sum_{i=1}^l\alpha_i^2\right)^{1/2}\geq \frac{1}{5}\right] \geq \frac{3}{4}.
\] 
\end{proof}

\PowerMethod*
\begin{proof}

Algorithm~\ref{alg:PowerMethod} starts with a random vector $\vv^{(0,r)}$ (Line~\ref{algline:randomInit}). Let $l$ denote the dimension of the eigenspace corresponding to the maximum eigenvalue. We denote $\vv^{(0,r)} = \sum_i \alpha^{(r)}_i \uu_i$ and from Lemma~\ref{lem:PMHighComp}, we have that for all $r$, initial vectors,
\[
\Pr\left[\sum_{i=1}^l(\alpha^{(r)})_i^2 \geq \frac{1}{25}\right]\geq 3/4.
\]
%
We now analyze Algorithm~\ref{alg:PowerMethod}. After $K$ iterations, the algorithm computes,
\begin{align*}
\AA^K\vv^{(0,r)} & =\sum_{i =1 }^{n}\alpha^{(r)}_i \lambda_i^K \uu_i .
\end{align*}
  Note that, since $K = \frac{4\log \frac{n}{\epsilon}}{\epsilon}$, for every $\lambda_i \leq (1-\epsilon/4)\lambda_1,$ we have $\lambda_i^{2K} \leq \frac{\epsilon^{2}}{n^{2}}\lambda_1^{2K}$. Let $\tilde{l}$ denote the dimension of the space spanned by all eigenvectors of $\AA$ with corresponding eigenvalues at least $(1-\epsilon/4)\lambda_1$, we split the sum as,
\[
{\vv^{(0,r)}}^{\top}\AA^{2K+1}\vv^{(0,r)}  =\sum_{i =1 }^{\tilde{l}}(\alpha^{(r)})_i^2 \lambda_i^{2K+1} + \sum_{i >\tilde{l}}(\alpha^{(r)})_i^2 \lambda_i^{2K+1} .
\]
 Now,
 \begin{equation}\label{eq:pm1}
 (\ww^{(r)})^{\top}\AA\ww^{(r)} = \frac{(\AA^K\vv^{(0,r)})^{\top}\AA(\AA^K\vv^{(0,r)})}{\|\AA^K\vv^{(0,r)}\|^2}  = \frac{{\vv^{(0,r)}}^{\top}\AA^{2K+1}\vv^{(0,r)}}{\|\AA^K\vv^{(0,r)}\|^2}  \geq \frac{\sum_{i =1 }^{\tilde{l}}(\alpha^{(r)})^2_i \lambda_i^{2K+1} }{\|\AA^K\vv^{(0,r)}\|^2}.
 \end{equation}
We now want to bound $\|\AA^K\vv^{(0,r)}\|^2$. We again expand this quantity and use the bounds on $\lambda_i$'s to get, 
\[
\|\AA^K\vv^{(0,r)}\|^2 = \sum_{i =1 }^{n}(\alpha^{(r)})_i^2 \lambda_i^{2K} \leq \sum_{i =1 }^{\tilde{l}}(\alpha^{(r)})_i^2 \lambda_i^{2K} + \left(\frac{\epsilon}{n}\right)^{2}\lambda_1^{2K}\sum_{i>\tilde{l}}(\alpha^{(r)})_i^2 \leq \sum_{i =1 }^{\tilde{l}}(\alpha^{(r)})_i^2 \lambda_i^{2K}+\lambda_1^{2K}\left(\frac{\epsilon}{n}\right)^2\|\vv^{(0,r)}\|^2.
\]
Since $\vv^{(0,r)}$ has entries in $N(0,1)$, from \Cref{lem:NormG}, with probability at least $1-1/n^2$, $\|\vv^{(0,r)}\|^2 \leq 2n$ and as a result, with probability at least $1-1/n^2$, $\|\AA^K\vv^{(0,r)}\|^2 \leq \sum_{i =1 }^{\tilde{l}}(\alpha^{(r)})_i^2 \lambda_i^{2K}+\left(\frac{\epsilon}{n}\right)\lambda_1^{2K}$ when $n\geq 5$. Using this bound in \eqref{eq:pm1} and $\epsilon<1/20$,
\[
(\ww^{(r)})^{\top}\AA\ww^{(r)} \substack{(a)\\ \geq}  \frac{\lambda_{\tilde{l}}\sum_{i =1 }^{\tilde{l}}(\alpha^{(r)})^2_i \lambda_i^{2K}}{\sum_{i =1 }^{\tilde{l}}(\alpha^{(r)})^2_i \lambda_i^{2K} + \epsilon n^{-1}\lambda_1^{2K}} \substack{(b)\\ \geq} \frac{\lambda_{\tilde{l}}}{1+\frac{\epsilon n^{-1}}{\sum_{i=1}^l(\alpha^{(r)})_i^2} }\substack{(c)\\ \geq} \frac{(1-\epsilon/4)\lambda_1}{1+\epsilon/4}\substack{(d)\\\geq} (1-\epsilon/2)\lambda_1,
\]
with probability at least $3/5$. To see this, in $(a)$ we used that $\|\AA^K\vv^{(0,r)}\|^2 \leq \sum_{i =1 }^{\tilde{l}}(\alpha^{(r)})_i^2 \lambda_i^{2K}+\left(\frac{\epsilon}{n}\right)\lambda_1^{2K}$ with probability at least $1-1/n^2 $, in $(b)$ we used that $\sum_{i =1 }^{\tilde{l}}(\alpha^{(r)})^2_i \lambda_i^{2K} \geq \lambda_1^{2K}\sum_{i=1}^l (\alpha^{(r)})_i^2$, and in $(c)$ we used that $\sum_{i=1}^l(\alpha^{(r)})_i^2 \geq 1/25$ with probability at least $3/4$.

We next prove our concentration bound, i.e., for at least one $r \in \{1,\cdots, 10\log n\}$, $(\ww^{(r)})^{\top}\AA\ww^{(r)}\geq (1-\epsilon/2)\lambda_1$ with probability at least $1-1/n$. Observe that the probability that $(\ww^{(r)})^{\top}\AA\ww^{(r)}< (1-\epsilon/2)\lambda_1$ for one $r$ is at most $2/5$. Since we have $R = 10\log n$ values of $r$, the probability that all of them are such that $(\ww^{(r)})^{\top}\AA\ww^{(r)}< (1-\epsilon/2)\lambda_1$ is $\left(\frac{2}{5}\right)^{R} \leq \frac{1}{n^{10}}.$
%
Therefore, we must have $(\ww^{(r)})^{\top}\AA\ww^{(r)}\geq (1-\epsilon/2)\lambda_1$ for at least one $r$ with probability at least $1-1/n^{10}$.

It remains to prove the runtime. In every iteration, we are multiplying $\AA$ with a vector. This takes time $\nnz(\AA)$. The total number of iterations is $O(\log \frac{n}{\epsilon}/\epsilon)$ for every initial random vector $\vv^{(0,r)}$. We run the entire method $10\log n$ times, giving us a total runtime of $O(\nnz(\AA) \frac{\log^2 \frac{n}{\epsilon}}{\epsilon})$. In the end we compute $(\ww^{(r)})^{\top}\AA\ww^{(r)}$ for $r = 1,\cdots, 10\log n$. This takes an additional time of $O(\log n\cdot  nnz(\AA))$.

We now prove that for $i$ such that $\lambda_i \leq \lambda_1/2$, for some $r$, with probability at least $1-1/n^{10}$, ${\ww^{(r)}}^{\top}\uu_i \leq \frac{1}{n^8}\cdot \frac{\lambda_i}{\lambda_1}$. We use the same argument as above to get that for at least one $r$, $\sum_{i=1}^d(\alpha^{(r)})_i^2 \geq 1/25$ with probability at least $1-1/n^{10}$. For this $r$, and $\epsilon<1/20$,
\begin{align*}
(\ww^{(r)})^{\top}\uu_i & = \frac{\alpha_i\lambda_i^K}{\sqrt{\sum_{i=1}^n\alpha_i^2\lambda_i^{2K}}} \leq  \frac{\alpha_i\lambda_i^{K-1/2}\lambda_i^{1/2}}{\lambda_1^K\sqrt{\sum_{i=1}^d\alpha_i^2}}\leq 5\alpha_i \left(\frac{\lambda_i}{\lambda_1}\right)^{1/2}\left(\frac{1}{2}\right)^{K-1/2} \leq 5\alpha_i \left(\frac{\lambda_i}{\lambda_1}\right)^{1/2}(1-10\epsilon)^{K-1/2}.
\end{align*}
Now, $\alpha_i\leq 10\log n$ with probability at least $1-1/n^{10}$. Therefore, we have with probabiltiy at least $1-2/n^{10}$,
\[
\left[(\ww^{(r)})^{\top}\uu_i\right]^2 \leq 2500\frac{\lambda_i}{\lambda_1} \log^2 n (1-10\epsilon)^{2K-1} \leq \frac{2500 \epsilon^{10}\log n}{n^{10}}\frac{\lambda_i}{\lambda_1} \leq \frac{1}{n^8} \cdot \frac{\lambda_i}{\lambda_1}.
\]
\end{proof}

