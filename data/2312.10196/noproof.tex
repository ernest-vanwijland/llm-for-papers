\documentclass[11pt]{article}

\usepackage{array,fullpage,multirow}
\usepackage{amssymb,amsmath,amsthm,sectsty,url,nicefrac,color}
\usepackage[bookmarks=true,pdfstartview=FitH,colorlinks,linkcolor=blue,filecolor=blue,citecolor=blue,urlcolor=blue]{hyperref}
\usepackage{cleveref}
\usepackage[pdftex]{pict2e}
\usepackage{aliascnt}
\usepackage[numbers]{natbib} \usepackage[inline]{enumitem}


\usepackage[boxed]{algorithm}
\usepackage{algpseudocode}

\numberwithin{equation}{section}



\newtheorem{theorem}{Theorem}[section]

\newaliascnt{lemma}{theorem}
\newtheorem{lemma}[lemma]{Lemma}
\aliascntresetthe{lemma}
\crefname{lemma}{Lemma}{Lemmas}

\newaliascnt{example}{theorem}
\newtheorem{example}[example]{Example}
\aliascntresetthe{example}
\crefname{example}{Example}{Examples}

\newaliascnt{claim}{theorem}
\newtheorem{claim}[claim]{Claim}
\aliascntresetthe{claim}
\crefname{claim}{Claim}{Claims}

\newaliascnt{corollary}{theorem}
\newtheorem{corollary}[corollary]{Corollary}
\aliascntresetthe{corollary}
\crefname{corollary}{Corollary}{Corollaries}

\newaliascnt{construction}{theorem}
\newtheorem{construction}[construction]{Construction}
\aliascntresetthe{construction}
\crefname{construction}{Construction}{Constructions}

\newaliascnt{question}{theorem}
\newtheorem{question}[question]{Question}
\aliascntresetthe{question}
\crefname{question}{Question}{Questions}

\newaliascnt{fact}{theorem}
\newtheorem{fact}[fact]{Fact}
\aliascntresetthe{fact}
\crefname{fact}{Fact}{Facts}

\newaliascnt{proposition}{theorem}
\newtheorem{proposition}[proposition]{Proposition}
\aliascntresetthe{proposition}
\crefname{proposition}{Proposition}{Propositions}

\newaliascnt{conjecture}{theorem}
\newtheorem{conjecture}[conjecture]{Conjecture}
\aliascntresetthe{conjecture}
\crefname{conjecture}{Conjecture}{Conjectures}


\newaliascnt{definition}{theorem}
\newtheorem{definition}[definition]{Definition}
\aliascntresetthe{definition}
\crefname{definition}{Definition}{Definitions}

\newaliascnt{remark}{theorem}
\newtheorem{remark}[remark]{Remark}
\aliascntresetthe{remark}
\crefname{remark}{Remark}{Remarks}

\newaliascnt{observation}{theorem}
\newtheorem{observation}[observation]{Observation}
\aliascntresetthe{observation}
\crefname{observation}{Observation}{Observations}

\crefname{algorithm}{Algorithm}{Algorithms}

\newaliascnt{notation}{theorem}
\newtheorem{notation}[notation]{Notation}
\aliascntresetthe{notation}
\crefname{notation}{Notation}{Notations}

\newcommand{\abs}[1]{\left|#1\right|}
\newcommand{\norm}[1]{\lVert#1\rVert}
\newcommand{\Linf}{\mathcal{L}_\infty }

\newcommand\E{\mathop{\mathbb E}}
\newcommand{\polylog}{\text{polylog}}
\newcommand{\Exp}{\mathop{\mathrm E}\displaylimits} \newcommand{\Var}{{\bf Var}}
\newcommand{\Cov}{{\bf Cov}}


\newcommand{\Query}{\text{Queries}}

\newcommand{\U}{\mathbf U}
\newcommand{\N}{\mathbb N}
\newcommand{\R}{\mathbb R}
\newcommand{\Z}{\mathbb Z}
\newcommand{\C}{\mathbb C}
\renewcommand{\P}{\mathcal{P}}
\newcommand{\F}{\mathbb F}
\newcommand{\GF}{\mathbb{GF}}
\newcommand{\B}{\{ 0,1 \}}
\newcommand{\BM}{\{ -1,1 \}}
\newcommand{\kstar}[1]{\ensuremath{#1}-star}


\newcommand{\NP}{{\mathrm {NP}}}
\newcommand{\PP}{{\mathrm {P}}}
\newcommand{\NC}{{\mathrm {NC}^1}}
\newcommand{\class}[1]{\mathbf{#1}}

\newcommand{\eps}{\varepsilon}
\newcommand{\1}{\mathbf{1}}
\newcommand{\A}{\mathcal A}
\newcommand{\D}{\mathcal D}
\newcommand{\Parity}{\mathrm{Parity}}
\newcommand{\Addr}{\mathrm{Addr}}
\newcommand{\Maj}{\mathrm{Maj}}
\newcommand{\Lex}{\mathrm{LTF}}
\newcommand{\HamW}{wt} \newcommand{\degree}{d}
\newcommand{\ksum}{k\text{-Sum}}
\newcommand{\kclique}{k\text{-Clique}}

\newcommand{\beq}{\begin{equation}}
\newcommand{\eeq}{\end{equation}}
\newcommand{\poly}{\text{poly}}
\newcommand{\xor}{\oplus}
\usepackage{tikz}


\newcommand{\cost}{\mathrm{cost}}
\newcommand{\NT}{\mathrm{NT}}
\newcommand{\RT}{\mathrm{T}}
\newcommand{\RNT}{\mathrm{RNT}}

\newcommand{\DC}{\mathrm{D}} \newcommand{\CC}{\mathrm{C}} \newcommand{\RC}{\mathrm{R}} \newcommand{\RCC}{\mathrm{RC}} \newcommand{\ACC}{\mathrm{AC}} \newcommand{\RACC}{\mathrm{RAC}} 

\newcommand{\sens}{\mathsf{s}}
\newcommand{\bsens}{\mathsf{bs}}

\newcommand{\T}{T} \newcommand{\Ts}{\mathcal T} \newcommand{\dist}{\Delta} \newcommand{\dists}{\boldsymbol\Delta} 


 \newcommand{\emptystring}{\bot}

\newcommand{\remove}[1]{}
\definecolor{green1}{rgb}{0.40, 0.8, 0.2}

\newcommand{\authnote}[4]{{ [{\color{#3} #1's Note:} {{\color{#4} #2}]}}}
\newcommand\omri[1]{{\textcolor{red}{Omri: #1}}}
\newcommand\tomer[1]{{\textcolor{red}{Tomer: #1}}}
\newcommand\moni[1]{{\textcolor{green}{Moni: #1}}}


\def\({\left(}
\def\){\right)}
\makeindex





\title{On the Instance Optimality of Detecting Collisions and Subgraphs
}

\author{
Omri Ben-Eliezer\thanks{Simons Institute for the Theory of Computing, University of California, Berkeley, USA. Part of this research was conducted while the author was at Weizmann Institute and later at Massachusetts Institute of Technology. Email: \texttt{omrib@mit.edu}.}
\and
Tomer Grossman\thanks{Department of Computer Science and Applied Mathematics,
   Weizmann Institute of Science,  Rehovot 76100, Israel. Email: \texttt{tomer.grossman@weizmann.ac.il}.}
\and
 Moni Naor\thanks{Department of Computer Science and Applied Mathematics,
   Weizmann Institute of Science,  Rehovot 76100, Israel. Email:
   \texttt{moni.naor@weizmann.ac.il}. Supported in part by grant  from the Israel
   Science Foundation (no.\ 950/16). Incumbent of the Judith Kleeman Professorial
   Chair.}
}



\date{}
\begin{document}

\maketitle








\begin{abstract}
Suppose you are given a function 
$f\colon [n] \to [n]$
via (black-box) query access to the function. You are looking to find something local, like a collision (a pair $x \neq y$ s.t.\ $f(x)=f(y)$). The question is whether knowing the `shape' of the function helps you or not (by  shape we mean that some permutation of the function is known). 
Formally, we investigate the \emph{unlabeled instance optimality} of substructure detection problems in graphs and functions. A problem is $g(n)$-instance optimal if it admits an algorithm 
$A$ satisfying that for any possible input, the (randomized) query complexity of $A$ is at most $g(n)$ times larger than the query complexity of any algorithm $A'$ which solves the same problem while holding an \emph{unlabeled copy} of the input (i.e., any $A'$ that ``knows the structure of the input''). 
Our results point to a trichotomy of unlabeled instance optimality among substructure detection problems in graphs and functions: 
\begin{itemize}
\item A few very simple properties have an $O(1)$-instance optimal algorithm.
\item Most properties of graphs and functions, with examples such as containing a fixed point or a $3$-collision in functions, or a triangle in graphs, are $n^{\Omega(1)}$-far from instance optimality. 
\item The problems of collision detection in functions and finding a claw in a graph serve as a middle ground between the two regimes. 
We show that these two properties are $\Omega(\log n)$-far from instance optimality, and conjecture that this bound is tight. We provide evidence towards this conjecture, by proving that finding a claw in a graph is $O(\log(n))$-instance optimal among all input graphs for which the query complexity of an algorithm holding an unlabeled certificate is $O\left(\sqrt{\frac{n}{\log n}}\right)$.
\end{itemize}
\end{abstract}


\iffalse
\omri{old abstract}
Suppose you are given a function 
$f\colon [n] \to [n]$
via (black-box) query access to the function. You are looking to find something local, like a collision (a pair $x \neq y$ s.t.\ $f(x)=f(y)$). The question is whether knowing the `shape' of the function helps you or not (by  shape we mean that some permutation of the function is known). Our goal in this work is to characterize all local properties for which knowing the shape may help, compared to an algorithm that does not know the shape. 

Formally, we investigate the instance optimality of fundamental substructure detection problems in graphs and functions.
Here, a problem is considered instance optimal (IO) if there exists an algorithm 
$A$ for solving the problem which satisfies that for any possible input, the (randomized) query complexity of $A$ is at most a multiplicative constant larger than the query complexity of any algorithm $A'$ for solving the same problem which also holds an \emph{unlabeled copy} of the input graph or function.

We provide a complete characterization of those constant-size substructure detection problems that are IO. Interestingly, our results imply that collision detection is not IO, showing that in some cases an algorithm holding an unlabeled certificate requires a factor of $\Theta(\log n)$ fewer queries than any algorithm without a certificate. We conjecture that this separation result is tight, which would make collision detection an ``almost instance optimal'' problem. We prove this conjecture in the regime where finding a collision requires $O(\sqrt{n})$ queries.\tomer{Previous sentence is lie: We only prove it for graphs. Is it ok for abstract?}  \tomer{TODO: Add tilde if nessecary here and in introduction} queries. In contrast, for all other non-trivial substructures, such as finding a fixed point, we show that the separation is polynomial in $n$.
\fi







\section{Introduction}
\label{sec:intro}







Efficient detection of small structures in complex data is a fundamental challenge across computer science. 
In this work, we explore to what extent \emph{prior knowledge} on the input may help. Consider, for instance, the problem of detecting a collision in an unknown function $f\colon [n] \to [n]$ given query access to $f$. (Here, a collision in $f$ is a pair of disjoint elements $x \neq y \in [n]$ so that $f(x) = f(y)$.)
We ask the following question.

\begin{center}
\textit{How does an algorithm that knows nothing about $f$ in advance (aside from the domain size $n$) compare to an algorithm that has some prior knowledge an the structure of $f$? 
}


\end{center}
The prior knowledge we consider in this work takes the form of an \emph{unlabeled copy} of $f$ that the algorithm receives in advance as in Grossman et al.~\cite{GrossmanKN20}. That is, the algorithm receives a permutation of $f$ -- the composed function $f \circ \pi$ for some unknown permutation $\pi$ -- as an ``untrusted hint". We typically call this permutation of $f$ an \emph{unlabeled certificate}; we require the algorithm to be correct with good probability regardless of whether the hint is correct (i.e., even if $f$ is not a permutation of the unlabeled certificate). However, the number of queries made by the algorithm is only measured if the true input is indeed a permutation of the unlabeled certificate.


In the worst case, clearly $\Omega(n)$ queries are necessary, whether we know anything about the structure of $f$ or not. But are there beyond-worst-case instances where holding additional structural information on $f$ may accelerate collision detection?

\begin{definition}[instance optimality; informal] \label{def:inst_opt_inf} A randomized Las Vegas\footnote{For simplicity we consider in this paper Las Vegas randomized algorithms, but all of the results apply also to Monte Carlo type algorithms (that allow some error in the returned value).}
algorithm $A$ deciding if an unknown function $f \colon [n] \to [n]$ satisfies a property $\P$ is \emph{instance optimal} if there exists an absolute constant $\alpha$ satisfying the following. For every function $f$, and any randomized algorithm $A'$ for the same task, the following holds:
\begin{align*}
\text{Queries}_A(f) \le \alpha \cdot \max_\pi \text{Queries}_{A'}(f \circ \pi)
\end{align*}
where the $\text{Queries}_A(\cdot)$ operator refers to the expected number of queries that an algorithm $A$ makes on a certain input.

Finally, we say that $\P$ is instance optimal if there exists an instance optimal algorithm for it.

\end{definition}
Note the order of the quantifiers in the definition: for every $f$, the algorithm $A$ has to compete with an algorithm $A'$ that ``specializes'' to functions of the form $f \circ \pi$. 
In other words, an algorithm $A$ is instance optimal if it performs as well as every algorithm $A'$, that knows the structure of $f$, but not the actual labels. Note that the correctness of algorithm $A'$ is unconditional -- that is $A'$ must be correct even if the structure of $f$ doesn't match the certificate $A'$ receives.

An algorithm being unlabeled instance optimal means it always performs as well (up to a constant) as the algorithm that knows the structure of the input. If there is no instance optimal algorithm, that means there exists some function where knowing the structure of the function is helpful.
Thus, instance optimality is a strong requirement: If a property $\P$ is instance optimal that means that knowing the structure of the input function $f$ \emph{never helps}.
When a property is not instance optimal, it will sometimes be useful to discuss its ``distance'' from instance optimality.
\begin{definition}[distance from instance optimality; informal]
Consider the setting of Definition \ref{def:inst_opt_inf}.
For a function $\omega(n)$ that grows to infinity as $n \to \infty$, we say that $\P$ is $\omega$-far from instance optimality if for every algorithm $n \in \N$ and $A$ there exist a function $f$ and an algorithm $A'$ satisfying 
\begin{align*}
\text{Queries}_A(f) \ge \omega(n) \cdot \max_\pi \text{Queries}_{A'}(f \circ \pi).
\end{align*}
Similarly, $\mathcal{P}$ is $\omega$-close to instance optimality if the above inequality holds with $\leq$ instead of  $\geq$.
\end{definition}










We may now rephrase our initial question about collisions in the language of instance optimality. Is collision detection an instance optimal problem? I.e., is the property of containing a collision instance optimal? Is it far from instance optimality? Suppose that we have query access to a function $f \colon [n] \to [n]$ and are interested in finding a collision. There are two fundamental types of queries to $f$ that one can make: the first option is to query an element $x$ that we have already seen in the past, by which we mean that we have already queried some element $y$ satisfying that $f(y) = x$. This option amounts to extending a ``walk'' on the (oriented) graph of $f$. 
The second option is to query a previously unseen element $x$, which amounts to starting a new walk. The question, then, is the following: is there a universal algorithm $A$ (which initially knows nothing about $f$) for choosing when to start new walks, and which walks to extend at any given time, that is competitive with algorithms $A'$ that know the unlabeled structure of $f$?

\paragraph{Substructure detection problems.}
There are many other types of natural problems in computer science that involve small (i.e., constant-sized) substructure detection. A natural generalization of a collision is a $k$-collision (or multi-collision), where we are interested in finding $k$ different elements $x_1, \ldots, x_k$ satisfying $f(x_1) = \cdots = f(x_k)$. Fixed points, i.e., values $x$ for which $f(x) = x$, are important in local search and optimization problems, in particular for the study of local maxima or minima in an optimization setting. 

Subgraph detection in graphs is also a fundamental problem in the algorithmic literature. Motifs (small subgraphs) in networks play a central role in biology and the social sciences. In particular, detecting and counting motifs 
efficiently is a fundamental challenge in large complex networks, and a substantial part of the data mining literature is devoted to obtaining efficient algorithms for these tasks. It is thus natural to ask: is it essential to rely on specific properties of these networks in order to achieve efficiency? In other words, are subgraph detection and counting instance optimal problems?

Similarly, the problem of finding collisions is a fundamental one in cryptography. Many cryptographic primitives are built around the assumption that finding a collision for some function, $f$ is hard (e.g.\ efficiently signing large documents, commitments with little communication and of course distributed ledgers such as  blockchain). If one wants to break such a cryptographic system, should one spend resources studying the structure of $f$? If finding collisions is instance optimal, that would mean that any attempt to find collisions by studying the structure of a function is destined to be futile.

In this work we focus on the instance optimality of constant-size substructure detection problems in graphs and functions. 
Before stating our results, let us briefly discuss these data models.
















\paragraph{Models.}
We consider two different types of data access in our work. The first type is that of functions. In this case the input is some function $f$, and the goal is to determine whether $f$ satisfies a certain property (e.g., whether it contains a collision or a fixed point). In this case the goal of an instance optimal algorithm is to perform as well as an algorithm that receives, as an untrusted hint, the unlabeled structure of the algorithm without the actual assignment of labels. Here the complexity is measured as the number of queries an algorithm makes, where each query takes an input $x$ and returns $f(x)$.


The second type of data is of graphs. Here the goal is to find a constant-sized subgraph. An instance optimal algorithm should perform as well as an algorithm that is given an isomorphism of the graph as an ``untrusted hint". For simplicity, we focus on the standard adjacency list model (e.g., \cite{Gonen2011}). Here for each vertex the algorithm knows the vertex set $V$ in advance, and can query the identity of the $i$-th neighbor of a vertex $v$ (for $v$ and $i$ of its choice, and according to some arbitrary ordering of the neighbors), or the degree of $v$. We note that all of the results also hold in other popular graph access models, including the adjacency matrix model and the neighborhood query model. 

Interestingly, graphs and functions seem closely related in our context. Specifically, the problem of finding a claw in a graph (a star with three edges) is very similar to that of finding a collision in a function, and the results we obtain for these problems are for the most part analogous.










\subsection{Main Results and Discussion}
\label{subsec:main_results}





Our main result in this paper characterizes which substructure detection problems in functions and graphs are instance optimal. 
Let us start with the setting of functions. 

A structure $H = ([h], E)$ is an oriented graph where each vertex has outdegree at most one, and we say that $f$ contains $H$ as a substructure if there exist values $x_1, \ldots, x_{h}$ such that $f(x_i) = x_j$ if and only if the edge $i\to j$ exists in $H$. (For example, a collision corresponds to the structure $([3], \{1\to 3, 2 \to 3\})$.) Finally, the property $\mathcal{P}_H$ includes all functions $f$ containing the structure $H$. Our first theorem constitutes a partial characterization for instance optimality in functions.

\begin{theorem} [Instance optimality of substructure detection in functions]\label{thm:main}
Let $H$ be a connected, constant-sized oriented graph with maximum outdegree $1$, and consider the function property $\mathcal{P}_H$ of containing $H$ as a substructure. Then $\P_H$ is
\begin{enumerate}
\item Instance optimal if $H=P_k$ is a simple oriented path of length $k$;
\item $n^{\Omega(1)}$-far from instance optimal for any $H$ that contains a fixed point, two edge-disjoint collisions, or a $3$-collision;
\item \label{funcpart3} $\Omega(\log n)$-far from instance optimal for any $H$ that contains a collision.
\end{enumerate}

\end{theorem}

Similarly, in graphs we denote by $\mathcal{P}_H$ the property of containing $H$ as a (non-induced) subgraph. Our next theorem provides a characterization for the instance optimality of subgraph detection. 

\begin{theorem} [Instance optimality of subgraph detection in graphs]\label{thm:graphconstantcharacterization}
Let $H$ be a connected, constant-sized graph with at least one edge. Then $\mathcal{P}_H$ is:
\begin{enumerate}
\item Instance optimal if $H$ is an edge or a wedge (path of length 2);
\item $n^{\Omega(1)}$-far from instance optimal if $H$ is any graph other than an edge, a wedge, or a claw (a star with $3$ edges);
\item \label{graph:part3} $\Omega(\log n)$-far from instance optimal when $H$ is a claw.
  \end{enumerate}
\end{theorem}



\paragraph{Almost instance optimality of claws and collisions?}
While we provide a full characterization of those substructures (or subgraphs) $H$ for which $\P_H$ is instance optimal, there remains a notable open problem: is the problem of containing a collision (in functions) or a claw (in graphs) ``almost instance optimal'', e.g., is it $O(\log n)$-close to instance optimality?


The problems of finding a collision in a function and detecting a claw in a graph are closely related and seem to be similar in nature (see Section \ref{sec:3}).
We conjecture that both of these problems are close to being instance optimal. 

\begin{conjecture} \label{conjcollision}
There exists an algorithm $A$ for collision detection (in functions $f \colon [n] \to [n]$) that is $O(\log n)$-close to instance optimality.
\end{conjecture}
\begin{conjecture}
\label{conjgraph}
Determining if a graph contains a claw is $O(\log n)$-close to instance optimality.
\end{conjecture}

While we are not yet able to prove the conjectures in full generality, we provide initial evidence toward the correctness, at least in the graph case. Specifically, we prove Conjecture~\ref{conjgraph} for graphs in which claw detection is ``easy'' with a certificate, that is, can be done in up to $O\left(\sqrt{\frac{n}{\log n}}\right)$ queries.




\begin{theorem}[informal; see Theorem \ref{thm:near_instance_optimality}]
\label{thm:inst_opt_graphs_intro}
The graph property of containing a claw is $O(\log n)$-instance optimal when restricted to inputs that require $o\left(\sqrt{\frac{n}{\log n}}\right)$ queries in expectation for an algorithm with an unlabeled certificate.
\end{theorem}
While the result was phrased for undirected graphs, it carries on also for collision detection in functions $f \colon [n] \to [n]$, in the case where the algorithm is allowed to go both ``forward'' (i.e., for an $x$ to retrieve $f(x)$) and ``backward'' (i.e., for $x$ to retrieve elements of the inverse set $f^{-1}(x)$). See the paragraph below on model robustness for further discussion.



We conjecture that the same algorithm we use to show near instance optimality in the regime of Theorem~\ref{thm:inst_opt_graphs_intro}
is also near instance optimal in the general regime. The algorithm $A_{\text{all-scales}}$ is roughly defined as follows. $A_{\text{all-scales}}$ maintains $m = O(\log n)$ parallel ``walks'' $W_1, \ldots, W_m$ at different ``scales'', where in each round (consisting of a total of $m$ queries) $A_{\text{all-scales}}$ adds one step to each of the walks. We try to extend each $W_i$ until it reaches length $2^i$ or until it has to end (either because of finding a collision/claw or due to reaching the end of a path or closing a cycle). In the case that $W_i$ reaches length $2^i$, we ``forget'' it and restart $W_i$ at a fresh random starting point. 

\paragraph{The challenge of merging walks.}
The only barrier to proving the above two conjectures in the general case seems to be our current inability to deal with ``merging'' walks in the algorithm. Any algorithm for collision detection in functions, or claw detection in graphs, can be viewed as maintaining a set of walks. In each step we either choose to start a new walk by querying a previously unseen vertex, or extend an existing walk by querying its endpoint (or one of its two endpoints, in the graph case). The event of merging corresponds to the case that two disjoint walks $W$ and $W'$ meet, resulting in the creation of a longer walk $W \cup W'$. Our proof of Theorem~\ref{thm:inst_opt_graphs_intro} shows the instance optimality of claw detection in the regime where merging is unlikely to happen during the algorithm's run.









\label{subsec:discussion}

\paragraph{Model robustness.}
Throughout the paper we chose to focus on specific models for convenience. However, all our results are model robust and apply in many ``natural" models. In particular, in the case of functions we chose to work on the model where an algorithm can only go forward. That is, an algorithm can query $f(x)$ in a black box manner, and doesn't have the capability to make inverse/backward ($f^{-1}(x)$) queries. Similar characterization results to the graph case also apply if an algorithm can walk backwards; in fact, the model where walking backward is allowed seems to serve as a middle ground between our models for graphs and functions, in the sense that we deal with directed graph properties but are allowed to move in the graph as if it were undirected.

For convenience we wrote all our results for Las Vegas randomized algorithms. All the results in this paper also apply if we require the algorithm to be a Monte Carlo randomized algorithm, i.e., one that is allowed to err with constant probability.

In graphs, we use the popular adjacency list model (which allows sampling random vertices, querying a single neighbor, or querying the degree of a vertex) for data access. The same characterization results also apply under other types of data access, such as the adjacency matrix model or the neighborhood query model (where querying a node retrieves all of its neighbors at once).












\subsection{Technical Overview: Collisions and Fixed Points}

In this section we give an overview of our main ideas and techniques. Many of the ideas are shared between functions and graphs; we chose to present the main ideas for a few canonical problems, such as fixed point and collision detection in functions, and claw detection in graphs. 

Showing the polynomial separation for most graph and function properties amounts, roughly speaking, to providing constructions where a certain substructure is hidden, but where certain hints are planted along the way to help the algorithm holding a certificate to navigate within the graph. Given the constructions, which are themselves interesting and non-trivial, it is not hard to prove the separation. As an example of a polynomial separation construction and result, we discuss the case of a fixed point in functions. For more general statements and proofs regarding these separations, please refer to Sections \ref{sec:4} (for functions) and \ref{sec:5} (for graphs).

The $\Omega(\log n)$-separation for claws and collisions is the most technically involved ``lower bound'' type contribution of this paper. Unlike the polynomial separation results, where the core idea revolves around describing the ``right'' way to hide information, here the construction is more complicated (roughly speaking): the trick of planting hints that allow the algorithm to navigate does not work well, and our arguments rely on the observation that it is sometimes essential for an algorithm without a certificate to keep track of multiple different scales in which relevant phenomena may emerge, compared to an algorithm with a certificate that knows in advance which of the scales is relevant.
The proof is also more challenging, requiring us to closely track counts of intermediate substructures of interest. For the sake of the current discussion, we focus on collision detection, but the proof (and construction) for claws is very similar; see Section \ref{sec:3}.

Before diving into the ideas behind fixed point and collision detection, let us briefly mention the simplest component in the characterization: an instance optimal algorithm for finding a path of length $k$. The algorithm chooses a random value and evaluates the function $k$ times on successive values to see if a path of length $k$ emerges (and not a smaller cycle, or a smaller path ending in a fixed point). This is repeated until a path is found or all values have been exhausted.  It is instance optimal, since knowing the structure of the function does not help; stopping after less than $k$ steps is meaningless, since it only saves us a constant fraction of the queries.

\subsubsection{Fixed point detection: Polynomially far from instance optimality}

\begin{figure} 
    \centering
\setlength{\unitlength}{1cm}
\thicklines

\begin{picture}(10,6)
\put(3,2){\vector(-1,0){0}}
\put(3,2){\line(1,0){2}}
\put(2,3){\line(0,1){2}}
\put(2,3){\vector(0,-1){0}}
\put(2,3){\vector(1,0){0}}


\put(2.71,2.71){\line(0.71,0.71){1.41}}

\put(2.71,2.71){\vector(-0.71,-0.71){0}}
\put(2.71,2.71){\vector(0.71,-0.71){0}}

\put(2,2){\circle{2}} 
\put(3,2){\vector(0,-1){0}}

\put(9,2){\line(1,0){2}}
\put(9,2){\vector(0,-1){0}}
\put(9,2){\vector(-1,0){0}}


\put(8,3){\line(0,1){2}}
\put(8,3){\vector(0,-1){0}}
\put(8,3){\vector(1,0){0}}


\put(8.71,2.71){\line(.71,.71){1.41}}
\put(8.71,2.71){\vector(-0.71,-0.71){0}}
\put(8.71,2.71){\vector(0.71,-0.71){0}}


\put(8,2){\circle{2}}
\end{picture}

\caption{There are $n^{1/4}/\log n$ cycles, where each cycle is of length $n^{3/4}$. Each path entering a cycle is of size $n^{1/4}$. The distance between every two paths on the $i$-th cycle is $p_i$.} \label{fig:fixed}
\end{figure}
We give an overview of the proof that finding a fixed point is polynomially far from instance optimality. Small variations of the constructions can be used to show that the same is true for any structure containing a fixed point, a $3$-collision, or two edge-disjoint collision.

In order to obtain such a result 
we provide a distribution of functions that have several fixed 
points  with a secret parameter so that an algorithm with a certificate (knowing the  parameter in this case) can find a fixed point in $n^{\frac{3}{4}}$ queries while any algorithm that does not know the secret parameter (i.e.\ without  a certificate) requires $\tilde\Omega(n)$ queries to find a fixed point.  

The idea is to construct a function $f$ with $\tilde{\theta}(n^{1/4})$ cycles of size roughly $n^{3/4}$, where one random value $x$ in one of the cycles is turned into a fixed point (which effectively turns the said cycle into a path ending at $x$). It is quite clear that for such a distribution finding the fixed point take time $\tilde\Omega(n)$. But we want to add some information or hint that will allow a certificate holder to find out which is the ``correct" cycle.

To give such a hint we add to each cycle  many paths of length $n^{1/4}$ entering it. The distance between two paths entering the $i$th cycle is some (unique)  prime $p_i$ where $p_i$ is of size roughly $n^{1/4}$ (so roughly $n^{1/2}$ paths enter the cycle). See~\Cref{fig:fixed} for a drawing of this construction.

The hint is the value $p_i$ associated with the unique cycle that ends up with a fixed point. The algorithm (with the hint) we propose will check many (about $\sqrt n$) `short' (length $n^{1/4}$) paths and see when they collide with another set of paths that is supposed to be on the cycles (these are $n^{1/4}$ `long' paths of  length $\sqrt n$).  Once our algorithm finds three paths entering the same cycle which are of distances that are all a multiple of $p_i$, the algorithm will conclude that this is the unique path that at its end the fixed point resides and will continue on the path. On the other hand,  for any algorithm that does not know which of the $p_j$'s is the chosen one and hence the  which path ends in a fixed point, each $x$ residing in a cycle is equally likely to be a fixed point, and thus the algorithm requires $\tilde{\Omega}(n)$ queries in expectation. 



\subsubsection{Finding Collisions:  $\Omega(\log n)$ far from instance optimality}

The distribution constructed above will not work for collision detection, since functions generated according to this distribution will inherently have many collisions. Below we describe a substantially different construction demonstrating that collision detection is (at least) logarithmically far from instance optimality.
We note that the same proof outline, and same construction idea can also be used to show that finding a claw in a graph is not instance optimal.

In order to obtain such a result 
we provide a distribution of functions that have several collisions, again,  with a secret parameter, so that an algorithm with a certificate (knowing the  parameter in this case) can find a collision in $n^{c}$ queries for some constant $c<1/2$, while any algorithm that does not know the secret parameter (i.e.\ without  a certificate) requires $\Omega(n^{c} \log n)$ queries to find a collision.

The hard distribution is as follows: there are $\log n$ length scales. For scale $i$ we have $n/{2.2^i}$ cycles, each  of length $2^i$ (note that the total number of nodes in all cycles is $O(n)$). For a uniformly randomly chosen scale $t$ we turn $n^{1-c}/{1.1^t}$ of the cycles to be a path ending in a loop of size $2$ at the end (this is a collision). 

The secret parameter is the value of $t$. The algorithm with a certificate simply picks a value at random and follows the path it defines for $2^t$ steps. The algorithm stops if (i)  a collision is discovered or (ii) the path has reached length $2^t$ without a collision  or (iii) the path created a cycle of length $2^i < 2^t$. 
The probability of picking a node on a good path (one ending in  a collision of length $2^t$)  is 
$$\frac{n^{1-c} \cdot 2^t}{n \cdot 1.1^t} $$
(since there are $n^{1-c}/{1.1^t}$ such cycles, each of size $2^t$).
The cost (in terms of queries)  of picking a value on the wrong size cycle, say of size $2^i$, is  $\min(2^i,2^t)$. It is possible to show that the total expected work when picking the wrong value is $O(2^t/1.1^t)$.\footnote{the constant $1.1$ is a bit arbitrary, and other constants larger than 1 will also work.} Therefore the expected amount of work until a good path is found is    
$$\frac{2^t}{1.1^t} \cdot \frac{1.1^t \cdot n }{n^{1-c} \cdot 2^t} = n^{c}.$$ 
The result is $O(n^{c})$ queries in expectation. 

We next show that any algorithm that does not know $t$ requires $\Omega(n^{c} \log n)$ queries, which results in a logarithmic separation from the algorithm with a certificate. In essence, this means that the algorithm needs to spend a substantial effort at all possible scales (instead of just one scale, $t$) in order to find the collision. 

Consider an algorithm without a certificate, and suppose that we choose the secret parameter $t$ in an online manner as follows. Our initial construction starts with $n/{2.2^i}$ cycles of length $i$. For each such $i$, we pick $n^{1-c}/1.1^i$ of the cycles of length $2^i$, and color one of the nodes in each such cycle by red (call these points the ``$i$-red points''. Note that at this time we have no information whatsoever on $t$. Now, each time that a red point on some cycle of length $2^i$ is encountered, we flip a coin with an appropriate probability (which initially is of order $1/\log n$) to decide whether the current value of $i$ is the secret parameter $t$ or not. If it is, then we turn all $i$-red points (for this specific value of $i$) into collisions as described above, and remove the color from all other red points (in paths of all possible lengths). Otherwise, we remove the color from all $i$-red points (for this specific $i$) and continue.

It turns out that this construction produces the same distribution as we described before (where $t$ was chosen in advance). However, it can also be shown that to find a collision with constant probability, $\Omega(\log n)$ red points need to be encountered along the way. The rest of the analysis provides an amortized argument showing that the expected time to find each red vertex by any algorithm is $\Omega(n^{c})$. The main idea of the amortized analysis (which we will not describe in depth here) is to treat cycles in which we made many queries -- at least a constant fraction of the cycle -- differently from cycles where we made few queries. For cycles of the first type, the probability to find a red point (if one exists) is of order $2^i / n^c$, but the amount of queries that we need to spend is proportional to $2^i$. For cycles of the second type, each additional query only has probability $O(1 / n^c)$ to succeed finding a red point, but the query cost is only $1$. In both cases, the rate between the success probability and the query cost is of order $1/n^{c}$.






\remove{
\subsubsection{Overview of \Cref{thm:graphconstantcharacterization}}
In this section we give an overview of~\Cref{thm:graphconstantcharacterization}. The proof that finding claw is $O(\log n)$-far from being instance optimal uses the same construction as that showing that finding collisions in functions is $O(\log n)$-far from being instance optimal. 

We show that unless $H$ is a claw, a wedge, or a single edge, it is polynomially far from instance optimality. We have two constructions, one if $H$ is a $\kstar{k}$, with $k \geq 4$ ($k=3$ is the case of claws) and one if $H$ is not.

\paragraph{Finding a \kstar{k} is polynomially far from being instance Optimal}

The distribution is one where we have a path, $P$ of length $\sqrt{n}$, and each vertex in the path is the start of an additional path of length $\sqrt{n}$. See~\Cref{fig:pathsIntro}. Finally, a vertex, $u$ is picked uniformly at random, and  connected to $k$ vertices.

It is quite clear that an algorithm without a certificate requires $\Omega(n)$ queries. An algorithm with a certificate, will find the path $P$ in $\sqrt{n}$ queries. The algorithm will then know, due to this unlabeled certificate,which of the paths originating from $P$ to follow in order to find $u$.

\begin{figure}
\begin{center}
\begin{tikzpicture}[scale=0.16]
\tikzstyle{every node}+=[inner sep=0pt]
\draw [black] (12.8,-3.7) circle (3);
\draw (12.8,-3.7) node {$v_0$};
\draw [black] (12.8,-15.1) circle (3);
\draw (12.8,-15.1) node {$v_1$};
\draw [black] (61.1,-15.1) circle (3);
\draw [black] (12.8,-26.5) circle (3);
\draw (12.8,-26.5) node {$v_2$};
\draw [black] (61.1,-26.5) circle (3);
\draw [black] (12.8,-49) circle (3);
\draw (12.8,-49) node {$v_{\sqrt{n}}$};
\draw [black] (61.1,-49) circle (3);
\draw [black] (12.8,-12.1) -- (12.8,-6.7);
\draw [black] (12.8,-18.1) -- (12.8,-23.5);
\draw [black] (15.8,-15.1) -- (58.1,-15.1);
\draw (36.95,-15.6) node [below] {$Path\mbox{ }of\mbox{ }length\mbox{ }\sqrt{n}$};
\draw [black] (12.8,-23.5) -- (12.8,-18.1);
\draw [black] (58.1,-15.1) -- (15.8,-15.1);
\draw [black] (15.8,-26.5) -- (58.1,-26.5);
\draw (36.95,-26) node [above] {$Path\mbox{ }of\mbox{ }length\mbox{ }\sqrt{n}$};
\draw [black] (12.8,-29.5) -- (12.8,-46);
\draw [black] (12.8,-46) -- (12.8,-29.5);


    \draw [black] (15.8,-49) -- (58.1,-49);
\draw (36.95,-49.5) node [below] {$Path\mbox{ }of\mbox{ }length\mbox{ }\sqrt{n}$};
\end{tikzpicture}
 \caption{Construction showing that finding a $\kstar{k}$ is not instance optimal. \label{fig:pathsIntro}}
\end{center}
\end{figure}


\paragraph{TODO} \tomer{TODO}
Next we give an overview of the proof that finding a subgraph, $H$ that isn't a \kstar{k}, is far from being optimal. We will The distribution is one where we $\theta(\sqrt{n})$ stars, each with a unique degree. Each of the stars will have degree roughly $\sqrt{n}$. $|H|$ vertices of degree 1 are then chosen uniformly at random, and are then connected.

It is quite clear that an algorithm without a certificate requires $\Omega(n)$ queries. An algorithm with a certificate, will find the path $P$ in $\sqrt{n}$ queries.

An algorithm with a certificate, on the other hand, will know the degree of the star center for each of the vertices chosen to form $H$. Thus, an algorithm with a certificate will only have to look for $H$ on a subgraph of size $O(\sqrt{n})$, which can trivially be done in $O(\sqrt{n})$.
}

\subsubsection{Finding claws: $O(\log n)$-close to instance optimality in merging-free regime}
The proof that collision detection is $\Omega(\log n)$-far from instance optimality extends very similarly to claw detection in graphs. We next show that this bound is tight in the ``low query'' regime where $G$ admits an algorithm for claw detection (with a certificate) using $q \leq \alpha \sqrt{\frac{n}{\log n}}$ queries, for a small constant $\alpha > 0$.

Every claw-free graph is a union of disjoint cycles and paths. Thus, every algorithm for finding a claw can be viewed as maintaining a set of walks of several types: Some of the walks may have closed a cycle; others may have reached one or two ends of the path $P$ that the walk resides in. All other walks simply consist of consecutive vertices in the interior of some path in $G$. 

Clearly, walks of the first type -- those that have closed a simple cycle -- cannot be extended to find a claw. Our first part of the proof is a reduction eliminating the need to consider walks of the second type (i.e., ones that have reached at least one endpoint). Specifically, we show that for every graph $G$ on $n$ vertices there is a graph $G'$ on $n+2$ vertices satisfying several properties:
\begin{itemize}
\item In all simple paths in $G'$, either both endpoints have degree $1$ or both are degree $3$ or more (i.e., are claw centers).
\item The query complexity of detecting a claw in $G'$ is equal, up to a multiplicative constant, to that of $G$. This is true both with or without an unlabeled certificate.
\item The construction of $G'$ from $G$ is deterministic. Thus, an unlabeled certificate for $G'$ can be constructed if one has an unlabeled certificate for $G$.
\end{itemize}
The construction is very simple: we add two new vertices and connect them to all degree-$1$ vertices (and to each other, if needed). 

\paragraph{Merging without claws requires $\Omega\left(\sqrt{\frac{n}{\log n}}\right)$ queries.}
The second part of our argument shows that one cannot make two walks merge in the first $\alpha \sqrt{{n}/{\log n}}$ queries (with constant probability and for some small constant $\alpha > 0$) without finding a claw beforehand. The proof relies on an advanced birthday paradox type analysis that is suitable for adaptive algorithms. For the purpose of this part, one may consider $G$ as a union of paths and cycles (without any claws). We bucket these paths and cycles into $O(\log n)$ groups, where in each group all paths and cycles are of roughly the same length -- within a multiplicative factor of $1.1$ from each other. Suppose that after each ``new'' (previously unseen) vertex is queried, the algorithm immediately knows to which bucket this vertex (and the corresponding walk emanating from it) belongs. We show that even in this case the lower bound holds.


Focusing on a specific bucket, let $\mathcal{W}$ be the set of all walks in this bucket at some point in the algorithm's run. An assignment of walks to ``locations'' within the bucket is considered valid if no two walks intersect.
We argue that the set of locations of walks is uniformly random among all sets of valid configurations. 
Similarly to our analysis of the $\Omega(\log n)$ separation (see the previous subsection), our next step in this part deals separately with ``short walks'' and ``long walks''. Very roughly speaking, our proof shows that walks have sufficient ``degree of freedom'' so that their probability to merge will be very small, even if provided that they lie in the same bucket.
We omit the precise details of the analysis from this overview, and point the reader to Section \ref{sec:nomerge}.


\paragraph{Asymptotic stochastic dominance of $A_{\text{all-scales}}$}
The third and last part of the argument shows that the algorithm $A_{\text{all-scales}}$ mentioned in Section~\ref{subsec:main_results} stochastically dominates any other algorithm (asymptotically) in the following sense. Conditioned on an algorithm $A$ (making $q = O\left(\sqrt{\frac{n}{\log n}}\right)$ queries) not encountering any merging walks during its operation, the algorithm $A_{\text{all-scales}}$ is at least as likely to find a claw, while using a slightly larger amount of roughly $4q \log n$ queries. 

Recall first how $A_{\text{all-scales}}$ is defined. For $0 \leq i < \log n$ let $A^{(i)}$ be the algorithm which repeatedly does the following: pick a random vertex in $G$; make a bidirectional walk from it for $2^{i+1}$ steps, or until a claw is found (leading to a ``win'') or an endpoint is found (leading to an early termination). 
$A_{\text{all-scales}}$ maintains one copy of each $A^{(i)}$ (for a total of $\log n$ copies), and alternates between them: each round of $A_{\text{all-scales}}$ makes $\log n$ queries, one for each $A^{(i)}$.

Now let $A$ be any algorithm operating on graphs on $n$ vertices.
Consider the event $E_i = E_i(G, q, A)$ that $A$ finds a claw (for the first time) after at most $q$ queries through the following process. $A$ queries a ``new'' vertex $v$ of distance between $2^{i}$ and $2^{i+1}-1$ from the claw center $w$, and finds it by completing a walk from $v$ to $w$. 
We claim that the event that $A$ finds a claw is equal to the union of the events $E_i$ (for $0 \leq i \leq \log n$).


The proof goes through a careful coupling argument between $A$ and any fixed $A^{(i)}$ (separately). Through the coupling, we may assume that $A$ and $A^{(i)}$ have access to the same source of randomness generating ``new'' vertex queries, that is, the $j$-th new vertex starting a walk $W_j$ in $A$ is also the $j$-th new vertex in $A^{(j)}$. We may further assume, by symmetry considerations, that $A$ respects an ``older first'' principle: if two walks $W$ and $W'$ have exactly the same ``shape'' (within $G$) at some point, then $A$ will prefer to extend the walk among them that is older. Now, suppose that $A$ finds the claw in its walk $W_j$, of distance between $2^i$ and $2^{i+1}-1$ from $v_j$. By the ``older first'' principle, this implies that for all vertices $v_{j'}$ with $j' < j$ that are at least $2^i$ away from an endpoint (call these values of $j'$ good), $A$ must have walked for at least $2^i$ from $v_{j'}$ so far. For all other values of $j' < j$ (call these bad), $A$ walked a number of steps that is at least the distance from an endpoint.
In contrast, $A^{(i)}$ makes up to $4 \cdot 2^{i}$ queries around each good vertex, and up two times as many queries as $A$ around each bad one. 

\subsection{Related Work}
The term instance optimality was coined by Fagin, Lotem and Naor \cite{FaginLN03}. If an algorithm always outperforms every other algorithm (up to a constant), particularly the algorithm that is aware of the input distribution, it is defined as being instance optimal. This definition is very strict, and thus there are numerous situations in which it cannot be meaningfully attained. As a result, several works (including ours) address this by necessitating that an instance optimal algorithm be competitive against a certain class of algorithms (for example, those that know an a permutation of the input, rather than the input itself). This notion of instance optimality is sometimes referred to as ``unlabeled instance optimality".

\paragraph{Unlabeled instance optimality.} Afshani, Barbay, and Chan~\cite{AfshaniBC17} considered and presented unlabeled instance-optimal algorithms for locating the convex hull and set maxima of a set of points. Valiant and Valiant \cite{ValiantV16} developed an unlabeled instance optimal algorithm for approximating a distribution using independent samples from it (with the cost function being the number of samples). Later \cite{ValiantV17}, they provided an algorithm for the identity testing problem. Here the problem is  determining, given an explicit description of a distribution, whether a collection of samples was selected from that distribution or from one that is promised to be far away. More recent works on instance optimality in distribution testing include, for example, the work of Hao et al.~\cite{HaoOSW18,HaoOrlitsky2020}.

Grossman, Komargodski, and Naor examined unlabeled instance optimality in the query model \cite{GrossmanKN20}. Their work relaxes the definition of instance optimality by no longer requiring an optimal algorithm to compete against an algorithm that knows the entire input, but rather against an algorithm that knows \emph{something} about the input. Arnon and Grossman \cite{ArnonG21} define the notion of min-entropic optimality, where instead of relaxing the ''for-all" quantifier over algorithms, they relax “for-all” quantifier over inputs. That is, for an
algorithm to be optimal it is still required to perform as well any other algorithm; however it is no longer required to be optimal on every input distribution, but rather only on a certain class of inputs. 

\paragraph{Instance optimality in graphs.}
Subgraph detection and counting has not been thoroughly investigated from the perspective of instance optimality; establishing a unified theory of instance optimality in this context remains an intriguing open problem. However, instance optimality has been investigated for other graph problems of interest. For example, Haeupler, Wajc and Zuzic \cite{HWZ21} investigate instance optimality and a related notion called universal optimality in a family of classical and more ``global'' distributed graph problems such as computing minimum spanning trees and approximate shortest paths. 

\paragraph{Strong instance optimality.} The original, robust definition of instance optimization calls for an algorithm to be superior to every other algorithm.  For getting the top $k$ aggregate score in a database with the guarantee that each column is sorted, \cite{FaginLN03} provided an instance-optimal algorithm. Demaine, L\'{o}pez-Ortiz, and Munro \cite{DemaineLM00} provided instance-optimal algorithms for locating intersections, unions, or differences of a group of sorted sets. Baran and Demaine \cite{BaranD04} showed an instance optimal algorithm for finding the closest and farthest points on a curve. Grossman, Komargodski and Naor \cite{GrossmanKN20} and Hsiang and Liu \cite{HsiangL23} studied instance optimality in the decision tree model. 





 \paragraph{Cryptography and complexity.}
 The problems of finding a constant sized structure in $f$, where $f$ is a total function guaranteed to contain the structure at hand has been studied extensively and is a fundamental problem in computational complexity and there are complexity classes in TFNP defined around it \cite{MegiddoP91,Papadimitriou94}. We note that we can slightly change the functions in our paper to also make them total problems, and all our proofs will still hold. 

As mentioned above, the problem of finding collisions is a fundamental one in cryptography. The standard definition is that of a collision resistant hash (CRH),  where finding a collision is a computationally hard problem. Such functions are very significant for obtaining efficient signature schemes and commitment to large piece of data using little communication. But other related structures are also considered in the literature: for instance, functions where it is hard to find multiple collisions~\cite{KomargodskiNY18}. 




\subsection{Organization}
In Section \ref{sec:prelims} we formally define the computational models we use as well as the notions of an unlabeled certificate (i.e., the ``untrusted hint'') and instance optimality. In Section \ref{sec:3} we prove that finding a collision in functions, and a claw in a graph is not instance optimal. In Section \ref{sec:4} we prove that functions that are not subsets of the collisions of two paths, followed by an additional path is polynomial far from being instance optimal. Such properties include many fundamental structures such as finding a fixed point, or a multi-collision. This gives us a complete characterization in the function model. In~\Cref{sec:5} we complete the full characterization for the model defined on graphs. We do this by proving that finding a path of length one or two is instance optimal, and that determining if a graph contains a subgraph $H$ is polynomially far from being instance optimal, unless $H$ is a path of length 1 or 2, or a claw. Finally, in~\Cref{sec:AlmostIO} we prove that finding a claw in a graph is $O(\log(n))$ close to being instance optimal among graphs for which finding a claw or collision can be done in $O(\sqrt{n} /\log(n))$, by an algorithm with an unlabeled certificate. 
\section{Preliminaries}
\label{sec:prelims}

\subsection{Functions}\label{sec:prelimsfunc}




Let $n \in \N$.
A \emph{property} $\P$ of functions is a collection of functions $f \colon [n] \to [n]$ that is closed under relabeling. That is, if $f \in \P$ then $f \circ \pi \in \P$ for any permutation $\pi \colon [n] \to [n]$. 
We sometimes say that $f$ \emph{satisfies} $\P$ when $f \in \P$.

In this work we will be interested in the property of containing some constant size substructure $H$ (e.g., a collision or a fixed point). Let $H$ be an oriented graph with $h$ vertices. Suppose further that the outdegree of each vertex in $H$ is at most $1$.\footnote{Note that a function can be viewed as an oriented graph where the outdegree is always equal to one, hence $H$ can appear as a substructure in such a function if and only if the outdegrees are at most $1$.} The property $\mathcal{P}_H$ consists of all functions $f \colon [n] \to [n]$ satisfying the following. There exist $h$ disjoint elements $x_1, \ldots x_h \in [n]$ and a mapping between $V(H)$ and $\{x_1, \ldots, x_h\}$, so that $H$ contains an edge between $u$ and $v$ if and only if $f(x_u) = x_v$, where $x_u$, $x_v$ are the mappings of $u$ and $v$, respectively.

A Las Vegas (randomized) algorithm for the property $\mathcal{P}$ in the query model is a randomized decision tree that determines membership in the property $\P$ with probability $1$ (i.e., it is always correct, and the quantity of interest is the number of queries the algorithm requires).
Given a Las Vegas randomized algorithm $A$ (which knows $n$) with random seed $r$ and given $f \colon [n] \to [n]$,
denote by $\Query^\P_A(f,r)$ the amount of queries that $A$ makes when evaluating if $f$ satisfies a property using the random seed $r$. Usually when the property $\mathcal{P}$ is clear from context we omit it from the notation.

We write $\Query^\P_A(f) = \E_{r} \Query^\P_A(f,r)$ (or $\Query_A(f)$) to denote the expected number of queries that the algorithm $A$ makes over input $f$, where the expectation is taken over all possible random seeds.


\begin{definition}[Unlabeled Certificate Complexity]
  The \emph{Unlabeled Certificate} complexity of a property $\P$, and function $f$ is:
\begin{align*}
    \RACC(\P, f) =  \min_{A \in \A_\P} \max_{\pi} 
    \Query_A(f \circ \pi),
\end{align*}
where $\A_{\P}$ is the set of all Las Vegas algorithms for evaluating if $f$ satisfies property $\P$, and $\pi$ ranges over all permutations of $[n]$.
\end{definition}


So far, we have considered only properties of functions of a given size $n$. Our definition of instance optimality is asymptotic in its nature and so we extend the definition of a property by allowing it to have functions of different sizes. Suppose that  $\mathcal{P}$ is a property which contains graphs of all sizes $n \geq N$, for some constant $N$. 
We can then define a corresponding sequence of algorithms $\{A_n\}_{n \geq N}$, where $A_n$ is responsible for graphs of size $n$.
\begin{definition}[instance optimality]
  A sequence of properties $\P=\{\P_n\}_{n\in \N}$ invariant under a relabeling is \emph{instance optimal} if there exist an absolute constants $c > 0$,
  and a sequence
  $\A = \{A_n\}_{n\in \N}$, where each $A_n$ is a Las Vegas algorithm for $\mathcal{P}_n$, such that on every input $f \colon [n] \to [n]$, it holds that
  \begin{align*}
    \Query^{\P_n}_{A_n}(f) \leq c \cdot \RACC(\mathcal{P}_n,f)
  \end{align*}
\end{definition}
\iffalse
\begin{definition}[Instance Optimality]
 We say that a sequence of algorithms $\{A\}_{n \in \N}$ evaluating if a sequence of function $\{f\}_{n \in \N}$ satisfies a property $P$ is \emph{Instance Optimal} if there exists a universal constant, $c$, such that for any function $f$, it holds that:

  \begin{align*}
   \Query^{\P_n}_A(f) \le c \cdot \RACC(\P,f)
  \end{align*}
  We say that the property $\P$ is instance optimal if there exists an instance optimal algorithm for $\P$. Similarly we say that property $\P$ is not instance optimal if there is no instance Optimal algorithm for evaluating $\P$.
\end{definition}
\fi
Next we present the analogous definition for being far from instance optimality.
\begin{definition}[\emph{$\omega$-far} from instance optimality]
Let $\omega\colon \N \to \N$ denote a function that grows to infinity as $n \to \infty$.
We say that a sequence of algorithms $\{A_n\}_{n \in \N}$ evaluating if a sequence of functions $\{f_n\}_{n \in \N}$ (where $f_n \colon [n] \to [n]$) satisfies a sequence of properties $\P = \{\P_n\}_{n \in \N}$ is \emph{$\omega$-far from instance optimal} if there exists a constant $N$ where for all $n \ge \N$ it holds that:
  \begin{align*}
   \Query^{\P_n}_{A_n}(f_n) \ge \omega(n) \cdot \RACC(\P_n,f_n).
  \end{align*}
  We say that the sequence of properties $\{\P_n\}$ is \emph{$\omega$-far from instance optimal} if any sequence of algorithms $\{\A_n\}_{n \in \N}$ evaluating it is $\omega$-far from instance optimal.
\end{definition}
In particular, a property $\P$ (or more precisely a sequence $\{\P_n\}$ of properties of functions $f \colon [n] \to [n]$, for any $n$) is polynomially far from instance optimal, if it is $\omega$-far for some $\omega(n) = n^{\Omega(1)}$ polynomial in $n$. 


\subsection{Graphs}


A graph property $\P$ is a collection of graphs that is closed under isomorphism. That is, if $G = (V,E) \in \P$ and $\pi \colon V \to V$ is a permutation, then the graph $G^\pi = (V,E^\pi)$ where $(u,v) \in E$ if and only if $(\pi(u), \pi(v)) \in E^{\pi}$ satisfies $G^{\pi} \in \P$.


Here we consider the adjacency list query model. We assume that the vertex set $V$ is given to us in advance. Given a single query, an algorithm can either (i) find the degree $d_v$ of $v$, or (ii) find the $i$-th neighbor of $v$ (in some arbitrary ordering). We note that other variants of the adjacency list model in the literature also allow pair queries, that is, given $u,v\in V$ the algorithm can ask whether there is an edge between $u$ and $v$. Our results hold word for word also in this variant.








Definitions of instance optimality are analogous to~\Cref{sec:prelimsfunc}, except here the unlabeled certificate is an isomorphism of the graph.

\begin{definition}[Unlabeled certificate complexity]
  The \textbf{randomized unlabeled certificate} complexity of a graph property $\P$ with respect to a graph $G$ is defined as follows.
  \begin{align*}
    \RACC(\P, G) =  \min_{A \in \A_\P} \max_{\pi \in \Gamma} 
    \Query^\P_A(\pi(G)),
\end{align*}
  where $\Gamma$ is the set of all permutations of the vertex set, and $\A_\P$ is the set of all Las Vegas randomized algorithms that always evaluate membership in $\P$ correctly.
\end{definition}

\begin{definition}[instance optimality]
  A sequence of graph properties $\P=\{\P_n\}_{n\in \N}$ is \emph{instance optimal} if there exist a constant $c > 0$ and a sequence of Las Vegas randomized algorithms
  $\A = \{A_n\}_{n\in \N}$ for $\P$, such that on every input $G$ on $n$ vertices, it holds that
  \begin{align*}
    \Query^{\P_n}_{A_n}(G) \leq c \cdot \RACC(\mathcal{P}_n,G)
  \end{align*}
\end{definition}

\begin{definition}[$\omega$-far from instance optimality]
Let $\omega\colon \N \to \N$ denote a function that grows to infinity as $n \to \infty$.
A sequence of algorithms $\{A_n\}_{n \in \N}$ evaluating if a sequence of graphs $\{G_n\}_{n \in \N}$ (where $G_n$ is a graph of order $n$) satisfies a sequence of properties $\P = \{\P_n\}_{n \in \N}$ is \emph{$\omega$-far from instance optimal} if there exists a constant $N$ where for all $n \ge \N$ it holds that:
  \begin{align*}
   \Query^{\P_n}_{A_n}(G_n) \ge \omega(n) \cdot \RACC(\P_n,G_n).
  \end{align*}
  We say that the sequence of properties $\{\P_n\}$ is \emph{$\omega$-far from instance optimal} if any sequence of algorithms $\{\A_n\}_{n \in \N}$ evaluating it is $\omega$-far from instance optimal.
\end{definition}

We conclude with standard graph theory terminology. A simple path with $k \geq 2$ vertices (in an undirected graph) is a collection of disjoint vertices $v_1, \ldots, v_k$ where $v_i$ is connected to $v_{i+1}$ by an edge for each $i=1,2,\ldots,k-1$. A simple cycle is defined similarly, but with $v_k$ also connected to $v_1$. 
Finally, the claw graph plays a central role in this work.
\begin{definition}[Claw]
The \emph{Claw graph}, $S_3$, is a \kstar{3}. That is, a four vertex graph consisting of a single vertex, of degree three, which is connected to three vertices each with degree one.
\end{definition} 
\section{Collisions and Claws: Logarithmic Separation} \label{sec:3}



In this section we formally present and analyze our construction proving~\Cref{thm:main}~\Cref{funcpart3} and~\Cref{thm:graphconstantcharacterization}~\Cref{graph:part3}: that detecting collisions (in functions $f\colon [n] \to [n]$) and claws (in graphs) is not instance optimal. 

\begin{theorem}[~\Cref{thm:graphconstantcharacterization}~\Cref{graph:part3} Reworded]
The property $\P_{S_3}$ of containing a claw is $\Omega(\log(n))$-far from instance optimality.

\end{theorem}

\begin{theorem}[~\Cref{thm:main}~\Cref{funcpart3} Reworded]\label{thm:3.2}
Fix $a,b,c \in \N$. Let $H = H_{a,b,c}$ denote the oriented graph containing two paths of length $a$ and $b$ which collide in a vertex, followed by a path of length $c$. 
The function property $\P_H$ is $\Omega(\log(n))$-far from instance optimal. 
\end{theorem}


These two cases (i.e., claws in graphs and collisions in functions) are very similar and the proof that they are not instance optimal is almost identical. Thus, for the majority of the section we focus on the case of claws in graphs. At the end of the section we describe the minor adaptations required for the case of collisions in functions.





We start by presenting the construction for claws. In~\Cref{sec:clawstocollision} we adapt the construction for collisions in functions. Here and in the rest of the paper, we do not try to optimize the constant terms. In particular, the constant $c=1/10$ appearing in the exponent of the query complexity is somewhat arbitrary; the same construction essentially works for any $c < 1/2$ (and with some adaptations it can be made to work for larger values of the constant $c$).



\begin{construction}
\label{const:first_const}
Consider the following process for generating a graph over the vertex set $[n]$, which starts with an empty graph and gradually adds edges to it.

\begin{itemize}
    \item For each integer $\frac{1}{1000}  \log n \leq i \leq \frac{1}{100} \log n$, pick $a_i = n / 2.2^i$ uniformly random disjoint simple paths of length $2^i$ in the graph. 
    
    \item Pick a uniformly random integer $\frac{1}{1000} \log n \leq t \leq \frac{1}{100} \log n$, which we consider as the ``good'' index. Pick a random collection $\P_t$ of $b_t = n^{9/10} / 1.1^t$ of the paths of length $2^t$. Apply to each path $P \in \P_t$ the following transformation:
    let $u_P$ and $v_P$ denote the two ends of the path. Now connect $u_P$ to two isolated vertices, and $v_{P}$ to two other isolated vertices. This turns $P$ into a tree of size $2^t + 4$ built from a long path and two claws, one at each end of the path.
    
    \item All vertices that do not participate in any of the above structures remain isolated. 
\end{itemize}

\end{construction}
We claim that an algorithm holding a certificate requires only $O(n^{1/10})$ queries to find a claw. Since $t$ is known from the certificate, the strategy is simply to only try walks of length $2^t$.

\begin{lemma}
$\E_{G \leftarrow \Delta} \RACC(\P, G) = O(n^{1/10})$.
\end{lemma}


\begin{proof}\textcolor{red}{TOPROVE 0}\end{proof}






The main result of this section, given below, is a lower bound showing that algorithms without a certificate require a number of queries that is larger by a multiplicative logarithmic factor compared to the best algorithm with a certificate.

\begin{theorem}
\label{thm:lower_bound_claw_no_cert}
For any algorithm $\A$ (without a certificate), $ \E_{G \leftarrow \Delta}\Query_A(G) = \Omega(n^{1/10} \log(n))$. \end{theorem}

To prove Theorem \ref{thm:lower_bound_claw_no_cert}, we first revisit Construction \ref{const:first_const}, discussing an equivalent way to generate the same distribution that is more suitable for our analysis. This alternative construction has some offline components, that take place before the algorithm starts to run, and an online component, that reveals some of the randomness during the operation of the algorithm.

For what follows, let $B(n)$ denote the maximum possible number of (initially isolated) vertices that are added as neighbors of claws in the second part of Construction \ref{const:first_const}. Note that this number is maximized when $t$ takes its minimal possible value, and satisfies $B(n) \leq n^{9/10} / 1.1^{\log (n) / 1000} = n^{9/10 - \Omega(1)}$.

Unlike the original construction, here we think of the vertices of the constructed graph as having one of three colors: black, red, or blue. These colors shall help us keep track of the analysis. In essence (and roughly speaking), blue vertices lead to an immediate victory but they are very rare and unlikely to be found in less than $n^{1/10 + \Omega(1)}$ queries; red vertices are not as rare: finding one of these takes roughly $n^{1/10}$ queries, but $\Omega(\log n)$ red vertices are required to find a claw with constant probability; finally, all other vertices are black, and encountering a black vertex contributes very little to the probability of finding a claw.

\begin{construction}
\label{const:red_blue_black}
We start with an empty graph on $n$ vertices colored black,
and color $B(n)$ of the vertices in blue.

We then construct, as in the first bullet of Construction \ref{const:first_const}, $n / 2.2^{i}$ disjoint paths of length $2^i$ out of black vertices only, for every $\frac{1}{1000}\log n \leq i \leq \frac{1}{100} \log n$.

Next, for every $i$ we pick a subset of $b_i = n^{9/10} / 1.1^i$ paths out of those of length $2^i$. We color the ends of these paths in red.

The last part of the construction happens online, while the algorithm runs. In each time step where the algorithm visits a black vertex, the construction remains unchanged. If the algorithm encounters a red vertex, then we reveal the randomness in the construction in the following way.
\begin{itemize}
    \item Let $I = \{ \frac{1}{1000}\log n \leq i \leq \frac{1}{100} \log n : \text{there exists a path of length $2^i$ with a red end}\}$. Note that initially, $I$ simply contains all values of $i$ in the relevant range; however in the construction $I$ will become smaller with time.
    \item Let $i \in I$ denote the unique integer satisfying that the currently visited red vertex lies on a path of length $2^i$.
    We flip a coin with probability $1/|I|$. If the result is `heads', we consider $i$ as the ``good'' index and do the following: all red ends of paths of length $2^i$ are connected to (isolated) blue vertices, all vertices in the graph are recolored by black, and the construction of the graph is complete.
    \item If the result of the above flip is `tails', we turn all red ends of paths of length $2^i$ to black, and remove $i$ from $I$.
\end{itemize}
Finally, if the algorithm encounters a blue vertex, we pick $i \in I$ uniformly at random to be the ``good'' length, and connect the red ends of paths of length $2^i$ to blue isolated vertices randomly. We then recolor all vertices in the graph to black and consider the construction complete.
\end{construction}

It is straightforward to check that Construction \ref{const:red_blue_black} produces the exact same distribution over graphs as Construction \ref{const:first_const}, and furthermore it does not reuse randomness revealed by the algorithm in previous parts.
Of particular interest is the following observation.
\begin{observation}
Consider any point of time during the online phase of Construction \ref{const:red_blue_black}, and let $I$ be as the defined in the first bullet. Then for any $t \in I$, the probability that $t$ will be the eventual ``good'' index, conditioning on all previous choices made during the construction, is $1/|I|$.
\end{observation}


We say that $\A$ \emph{wins} if it either finds a claw or encounters a blue vertex. The following lemma is a strengthening of Theorem \ref{thm:lower_bound_claw_no_cert}, and its proof immediately yields a proof for the theorem. 
\begin{lemma}
\label{lem:winning}
There exists $C > 0$ such that for any $n \in \N$, any algorithm without a certificate requires at least $C n^{0.1} \log n$ queries to win with success probability $9/10$.
\end{lemma}
From this lemma, it immediately follows that the expected winning time for the algorithm is $\Omega(n^{0.1} \log n)$, which in turn implies the theorem (by definition of winning). Indeed, any algorithm that finds a claw can immediately find a blue vertex and win, since each claw center has two blue neighbors.
Thus, we devote the rest of this section to the proof of Lemma \ref{lem:winning}.

The next (easy) lemma states that encountering a blue vertex is a rare event which will not substantially impact our analysis.
\begin{lemma}
\label{lem:no_blue_vertices}
There exists an absolute constant $\eps > 0$ satisfying the following. For any algorithm $\A$ without a certificate, with high probability $\A$ does not query a blue vertex within $n^{\frac{1}{10} + \eps}$ steps.
\end{lemma}
\begin{proof}\textcolor{red}{TOPROVE 1}\end{proof}


\iffalse
\vspace{1cm}
\omri{Previous text below.}
For the analysis, we will consider the construction where for each $t$ we turn $b_t$ random paths into paths with a fixed point in the end (and for one randomly chosen $t$ into a path with a collision in the end). Since the lower bound holds for this construction it also holds for the original construction \tomer{Prove} \omri{I think we can just change the original construction, the above prove doesn't really change as you indeed point out}, and thus we are good even if we consider fixed points to be a collision.


\begin{lemma}
$\A$ must find $\Omega(\log(n))$ red points / ``false'' stars.
\end{lemma}
\begin{proof}[Proof sketch]
Suppose that we have seen $r$ red points so far, and none of them were actual stars. The probability that a certain length that we haven't seen so far is good is bounded by $\frac{1}{\log(n) - r}$. Thus the probability that a collision is found while less than $\frac{\log(n)}{100}$ false points are found is  bounded by \beq \prod_{r=0}^{\frac{\log(n)}{100}}  1 - \frac{1}{\log(n) - r} \ge \text{Finish algebra}\eeq


\end{proof}

\tomer{formalize below}
\begin{lemma} \label{Lem:distDoesntChange}
With high probability for any algorithm that makes $O(n^c \log n)$ and has found at most $\log(n)$ collisions of each length, the distribution of paths will be the same up to a constant. \tomer{Modify next sentence}That is, for all $i$, there are at least $\frac{3(a_i - b_i)}{4}$ partial cycles of length between $\frac{2^i}{4}$ and $2^i$, and $\frac{3b_i}{4}$ paths ending in a fixed point of length between $\frac{2^i}{4}$ and $2^i$
\end{lemma}

\begin{proof}
Suppose an algorithm is given for free all the path length, and is only not told for each path if it is a cycle or a collision. Further, suppose that the moment the algorithm makes a single query on a path he is given for free the entire path.

We will show that for every $i$, that for  $k \ge 4$ if $\frac{a_i}{4}$ cycles have been found, then with (exponentially) high probability the number of path of length $i$ 
ending in a collision that have been found is between $\frac{2b_i}{k}$ and $\frac{b_i}{2k}$

We prove this for a given $i$ using Chernoff bounds. 

Define $X_l$ to be the random variable that the $l$th path is a cycle. We bound the values of. $\E[X_l]$.

$\E[X_l]$ is maximal if $\log(n)-1$ queries have been made all of which resulted in a fixed point. In this the probability that the next path is a cycle is $\frac{a_i - b_i}{a_i - \log(n) - 1} \le 2\frac{a_i-b_i}{a_i}$.

$\E[X_l]$ is minimal if $n^c \log(n)$ queries have been made all of which resulted in a cycle. In this case the probability that the next query is a cycle is $\frac{a_i - b_i - n^c \log(n)}{a_i} \ge \frac{a_i-b_i}{2a_i} $

\tomer{Todo: chernoff x2}



Taking the union bound over all $i$'s we have our desired result












\end{proof}
\fi

The following result shows that finding $\Omega(\log n)$ red vertices with constant probability requires $\Omega(n^{0.1} \log n)$ queries. To complete the proof, we shall see later that either finding a blue vertex or collecting at least logarithmically many red vertices is essential to win with constant probability.

\begin{lemma}
\label{lem:collision_red_vtxs}
There exists a constant $c > 0$ so that for all $n \in \N$, any algorithm $\A$ which makes $c n^{0.1}\log n$ queries will find less than $\frac{1}{1000}\log n$ red vertices in expectation.
\end{lemma}

\begin{proof}\textcolor{red}{TOPROVE 2}\end{proof}









\iffalse
\omri{The following is an old proof. It is pretty good already but rewriting from scratch would be easier for me.}
\begin{proof}  Suppose the algorithm has made at most $\frac{n^c \log n}{1000}$ queries. Thus due to~\Cref{Lem:distDoesntChange} the probability of that a given path of length $i$ ends in fixed point/collision rather than a cycle is at most $\frac{2b_i}{a_i}$ throughout the execution of the algorithm. \tomer{Fix sentence}


Suppose that once an algorithm has made $\frac{l}{2}$ queries on one path (note that this doesn't have to be continuous, e.g. if two paths the algorithm follows are on the same path, but a combined total of $\frac{l}{2}$ queries are made) we give the algorithm for free the rest of the path (or cycle). Furthermore, suppose that once an algorithm makes a query he is given for free the length of the path he is one. The only thing the algorithm doesn't know is for each path if it ends in a (psuedo) collision, or is a cycle.

We analyze this algorithm by considering two algorithms one that never makes more than $\frac{l}{2}$ queries, and one that always makes $\frac{l}{2}$ queries. \tomer{Explain that this can be analyzed as two independent algorithms}



\newcommand{\lreal}{l_{\text{real}}}


The probability that the algorithm finds a collision is at most 
\beq  \label{eq:sucprob}
\frac{1}{l-k} \frac{2b_{\log(l)}}{a_{\log(l)}}
\eeq

which equals to:

\beq
\frac{1}{\lreal-k} \frac{2l}{n^{1-c}}
\eeq





If $k < l/2$, the above equation is at most  
\beq \frac{2}{l} \frac{2}{n^{1-c}} < \frac{4}{n^{1-c}} \eeq 

And thus the algorithm that makes at most $\frac{l}{2}$ queries in each path has at most $\frac{4}{n^{1-c}}$ probability of finding a collision per each query, and thus $\Omega(n^{1-c})$ queries are required.



Next consider the algorithm that always makes $\frac{l}{2}$ queries per path, and then gets the rest of the path for free. Plugging into \Cref{eq:sucprob} the probability that the algorithm finds a collision is at most $\frac{2l}{n^{1-c}}$ (due to \ref{Lem:distDoesntChange}). Thus we can think of this algorithm mas one where each ``query" costs $\frac{l}{2}$ (for different $l$ values) and has a success probability of $\frac{2l}{n^{1-c}}$. Define $x_i$ to the a random variable indicating if the $i$th path resulted in a collision. Due to \ref{Lem:distDoesntChange} we know that (up to constants) $\E[X_i] = \E[X_i | X_1,...,X_{i-1}]$. By \ref{eq:sucprob} $\E[X_i] = \frac{2l_i}{n^{1-c}}$. Due to linearity of expectation \beq \E[\sum_i X_i] = \sum_i \frac{2l_i}{n^{1-c}} \eeq

Setting the above equal to $\log(n)/200$ and calculating $\sum \frac{l_i}{2}$ (the number of queries the algorithm makes) we have that $\frac{1}{50}n^{1-c} \log(n)$ queries are necessary to find $\log(n)/100$ collisions in expectation. Due to Markov's inequality, the probability of finding $\log(n)$ collisions in less than $\frac{1}{50}n^{1-c} \log(n)$ is at most $\frac{1}{50}$.
















\end{proof}
\fi
We now have all the ingredients to complete the proof of Lemma \ref{lem:winning}.

\begin{proof}\textcolor{red}{TOPROVE 3}\end{proof}


\subsection{From Claws to Collisions \label{sec:clawstocollision}}
We now briefly define a similar construction aimed at showing the $\Theta(\log n)$ separation for collisions in functions. The same construction and proof also apply if instead of a collision we consider  an ``extended collision'' $H_{a,b,c}$ as defined in Theorem \ref{thm:3.2}.

\begin{construction}
\label{const:collision_construction}
Consider the following process for generating a function $f \colon [n] \to [n]$:
\begin{itemize}
    \item For each integer $\frac{1}{1000}  \log n \leq i \leq \frac{1}{100} \log n$, add $a_i = n / 2.2^i$ disjoint paths of length $2^i$ in the function. 
    
    \item Pick a uniformly random integer $\frac{1}{1000} \log n \leq t \leq \frac{1}{100} \log n$. Pick a collection $\P_t$ of $b_t = n^{9/10} / 1.1^t$ of the paths of length $2^t$. For each such path $P$, let $u$ and $v$ denote the first and last element in the path; set $f(v)$ to be an arbitrary value in $P \setminus \{u,v\}$, which creates a collision. Close all other paths (of all lengths) into cycles: specifically, using the same notation, set $f(v)=u$.
    
    \item For all values $x \in [n]$ that do not participate in any of the above paths, set $f(x) = x$ to be a fixed point.\footnote{We note that the construction can be easily modified to not include fixed points: simply use the remaining values to close cycles of length $2$ or $3$ instead of fixed points, which are essentially cycles of length $1$.}
\end{itemize}
\end{construction}

As is the case with claws in graphs, an algorithm holding a certificate would know the value of $t$, and make walks of length $2^t$ until finding a collision, with a query complexity of $O(n^{1/10})$. Meanwhile, to show the lower bound for an algorithm without a certificate, we use a coloring scheme with only two colors -- red and black -- where elements that are ends of paths which serve as ``candidates'' for a collision are marked red, and all other elements are marked black. Similarly to the above, in order to find a collision with constant probability, the algorithm needs to find $\Omega(\log n)$ red elements with constant probability, which requires $\Omega(n^{1/10} \log n)$ queries.











\remove{
\begin{lemma}
Assuming the total number of queries made thus far is $o(n^c \log(n))$ (or similar), an algorithm whp won't land twice on the same path. 
\end{lemma}

\begin{lemma}
Assuming the total number of queries made thus far is $o(n^c \log(n))$ (or similar), then learning with (exponentially) high probability whether the correct length is $t$, requires $\omega(n^c)$ queries, all spent on walking path of length ``close" to $t$.  

This is independent of $t$, 
\end{lemma}

To be able to use the above lemma I think we need something stronger. I.e claim that an algorithm that spends $O(\frac{n^c}{\log(n)})$ queries walking paths of length $t$ doesn't learn much (i.e the probability that a path is good is very close to $\frac{1}{\log n}$. I think here is where the $\loglog(n)$ might come in -- an algorithm can learn something meaningful in $O(\frac{n^c}{\log\log(n)})$
}



\section{Function Properties Far from Instance Optimal} \label{sec:4}
In this section we study the instance optimality of properties of functions $f \colon [n] \to [n]$. As shown in the previous section, the property of containing a collision is $\Theta(\log n)$-far from instance optimal. We show that any pattern with either at least two collisions or at least one fixed point is \emph{polynomially} far from instance optimality. This is summarized in the theorem below.

\begin{theorem}
\label{thm:function_charac}
Let $H$ be any constant-size oriented graph (possibly with self-edges) where each node has out-degree at most one. Suppose further that $H$ either contains (i) a fixed point (i.e., an edge from a node to itself) or (ii) at least two nodes with in-degree at least two, or (iii) at least one node with in-degree at least three. 

The function property $\P_H$ of containing $H$ as a substructure is $\tilde\Omega(n^{1/4})$-far from being instance optimal.
\end{theorem}








The rest of this section is devoted to the proof of the theorem. 
We start with the construction used to prove the theorem.
\begin{construction}
Let $H$ be an oriented graph satisfying the conditions of Theorem \ref{thm:function_charac}.
 Define the \emph{entry vertices} of $H$ to be those vertices with in-degree 0 in $H$ (in the special case where $H$ is a single fixed point, define its single vertex as the entry point). Let $T$ be the total number of entry vertices in $H$. 
 
 Define an input distribution $\Delta$ as follows: first, pick $\alpha \frac{n}{\log n}$ vertices uniformly at random and split them into $N = \alpha n^{1/4} / \log(n)$ disjoint cycles of length $n^{3/4}$, for a small absolute constant $\alpha > 0$. Denote these cycles by $C_1,...,C_{N}$. For each cycle $C_i$ we associate a unique prime number, $p_i$, where all $p_i$'s are in the range $(n^{1/4}/4, {n^{1/4}}/2)$ for an appropriate value of $c$. Note that this is possible due to well-known results on the density of prime numbers. We say that all points contained in the union $\bigcup_{i=1}^{N} C_i$ are of \emph{type 1}.

For each cycle $C_i$, we add paths of length $\alpha n^{1/4}$ entering it, where the distance (in $C_i$) between the entry points of every two adjacent paths entering the cycles is exactly $p_i$. Since each cycle $C_i$ has a length of $n^{3/4}$, it has $\Theta(\sqrt{n})$ paths entering it, each of length $n^{1/4}$. We say that all points participating in these paths are of \emph{type 2}.

Lastly, a collection $x_1, \ldots, x_T$ of exactly $T$ points from $\bigcup_{i=1}^{N} C_i$ is picked uniformly at random conditioned on the event that no two of these points come from the same cycle. Denote the latter event by $E$ and note that $\Pr(E) = 1-o(1)$. For each $x_k$, let $C_{i_k}$ denote the cycle containing it, and turn this cycle into a path ending at $x_k$ by removing the outgoing edge from $x_k$. Finally, insert a copy of $H$ using all $x_1, \ldots, x_T$ as entry points.

To complete the function into one that has size $n$, partition all remaining (unused) points into disjoint cycles of length $n^{3/4}$ each.
\end{construction}













Crucially, an algorithm with a certificate knows the indices $i_1, \ldots, i_T$ (and the corresponding primes $p_{i_1}, \ldots, p_{i_T}$). Given these primes, it turns out that the algorithm is able to find the relevant cycles, walk on them until finding the entry points, and building the full $H$-copy, all using $O(n^{3/4})$ queries. In contrast, for an algorithm without the certificate, the entry points are distributed uniformly over the union of all cycles, and thus a lower bound of $\tilde\Omega(n)$ can be shown.

\begin{lemma} \label{fixedpointupper}
$\E_{f \leftarrow \Delta} RAC(\P_H, f) = O(n^{3/4})$.
\end{lemma}


\begin{lemma} \label{fixedpointlower}
For any algorithm $\A$ (without a certificate), $\E_{f \leftarrow \Delta} \Query_A(f) = \Omega(n / \log n)$.
\end{lemma}

\begin{proof}\textcolor{red}{TOPROVE 4}\end{proof}

\begin{proof}\textcolor{red}{TOPROVE 5}\end{proof}






 \section{Instance Optimality in Graphs} \label{sec:5}



We consider whether questions of the type: ``Does $G$ contains a subgraph $H$" are instance optimal.
 We begin by defining a \kstar{k}.
 
 \begin{definition} [\kstar{k}]
The \emph{\kstar{k}} $S_k$ is a graph with $k+1$ vertices: $k$ vertices of degree 1, all connected to a vertex of degree $k$.
 \end{definition}
 
  For example, a \kstar{1} is a graph consisting of 2 vertices, connected by an edge. A \kstar{2} (or a wedge) contains 2 vertices of degree 1 connected to a third vertex of degree 2. A \kstar{3} is a claw. 





 In Section \ref{sec:3}, we have seen that claws are not instance optimal, by showing a $\Omega(\log n)$-separation between an algorithm with a certificate and one without a certificate in some case. What about other choices of $H$? It turns out that if $H$ is a \kstar{1} or \kstar{2} then finding $H$ is instance optimal. If $H$ is any other graph, then finding $H$ is polynomially far from instance optimal. 







\begin{theorem}[\Cref{thm:graphconstantcharacterization} repeated]
Let $H$ be any fixed graph and consider the property $\mathcal{P}_H$ of containing a copy of $H$ as a subgraph. Then $\mathcal{P}_H$ is:
\begin{itemize}
\item instance optimal if $H$ is an edge or a wedge (path with two edges);
\item $n^{\Omega(1)}$-far from instance optimal if $H$ is any graph other than an edge, a wedge, or a claw; and
\item $\Omega(\log(n))$-far from instance optimal when $H$ is a claw.
  \end{itemize}
\end{theorem}



The theorem follows from the lemmas below, together with the results of Section \ref{sec:3}. \Cref{lemma:deg1} proves the first item of the theorem. 
\Cref{lemma:star} proves that for every $H$ that is not a \kstar{k}, finding $H$ is not instance optimal. \Cref{lemma:paths} proves the separation for $k$-stars when $k \geq 4$. 



 \begin{lemma} \label{lemma:deg1}
$\mathcal{P}_H$ is instance optimal when $H$ is a \kstar{1} (edge) or a \kstar{2} (wedge).
 \end{lemma}
 


 \begin{lemma} \label{lemma:star}
 For all graphs $H$ that are not a \kstar{k}, there exists a distribution, $\Delta$, such that $\E_{G \leftarrow \Delta} \Query_A(G) = \Omega(n)$ for any algorithm $A$ (without a certificate) determining membership in $\P_H$, whereas $\RACC(\P_H,\Delta) = O(n^{1/2} \log n)$. 
 \end{lemma}
 
 \begin{lemma} \label{lemma:paths}
Let $H = S_k$ for $k \geq 4$. There exists a distribution $\Delta$ such that for any algorithm $A$ (without a certificate) determining membership in $\P_H$, $\E_{G \leftarrow \Delta} \Query_A(G) = \Omega(n)$, while $\RACC(\P_H,\Delta) = O(n^{1/2})$. 
 \end{lemma}



\begin{proof}\textcolor{red}{TOPROVE 6}\end{proof}

\iffalse
\omri{Perhaps we should drop this. Trivial in the neighborhood query model.}
\begin{proof}[Proof Of \Cref{lemma:deg2}]
We will show that the following algorithm is unlabeled instance optimal: randomly make queries until an edge is found. Once an edge is found, make queries to the two vertices of this edge (in alternating order) until another edge is found. If no other edge is found, make random queries until a new edge is found. 

We analyze this by considering 2 cases when $m < 10n$ and when $m \ge 10n$.

First Suppose $ m <10n$. Consider any algorithm. In order to find a vertex of degree 2, it must in particular find a single edge. This is because if an algorithm outputs 0 without finding a single edge, then it will err with high probability if the graph is one chosen from the distribution where 3 vertices are chosen at random, and are connected. Similarly if the algorithm outputs 1 without finding a single edge then it errs with probability 1 if the graph is chosen to be the empty graph. Once an edge is found the algorithm makes $2n$ additional queries. Due to a similar argument as in~\Cref{lemma:anon_isedge_graph} finding an edge is unlabeled instance optimal, and requires at least $\frac{n^2}{10n} = \frac{n}{10} = \Omega(n)$ queries in expectation. Thus, for every edge any algorithm finds, our algorithm makes, in expectation, an additional $O(n)$ queries.  Since a total of $\Omega(n)$ expected queries have been made to find this edge, the total number of queries increases in expectation, by at most a constant fraction. Once a singleton edge is found, repeat the above process without the already found edge and thus this algorithm is unlabeled instance optimal.

Next suppose $m \ge 10n$. As shown in the proof of~\Cref{lemma:anon_isedge_graph}, finding an edge requires at least $\frac{n^2}{10n} = \frac{n}{10} = \Omega(n)$ queries in expectation. Thus any algorithm, with an unlabeled certificate must make $\Omega(n^2/m)$ queries. To show unlabeled instance optimality it suffices to show an algorithm, without a certificate, that makes $O(n^2/m)$ queries. Since $n$ is a trivial upper bound on the possible number of singleton edges, each edge the algorithm queries will have an adjacent vertex of degree at least two with probability at least $p = \frac{m-n}{m}$. The expected number of edges the algorithm has to look at until it finds an edge that has an adjacent vertex of degree at least two is $\frac{m}{m-n}$. This is because we can think of each edge being found as a single trial in a geometric distribution, where a success is defined to be when the algorithm finds an edge that has an adjacent vertex of degree at least two, and thus the expected number of trials is $\frac{m}{m-n}$. Once the algorithm finds an edge where one of its vertices has degree 2 it will have to make, in expectation $\theta(n^2/m)$ more queries to find another edge (adjacent to one of its vertices) since there are a total of $m$ edges, which are distributed among $n$ vertices. Thus the expected degree of the vertex the algorithm has found is $\theta(m/n)$, and in expectation $\theta(\frac{n}{m/n}) = \theta(n^2/m)$ queries are required (due to linearity of expectation). If the algorithm queried an edge whose both neighbors have degree 1 then it makes $2n$ extra queries (finding the degree of each of the vertices around this edge) and then it must repeat the process and again by making $n^2/m$ queries (in expectation) to find a new edge.


 Thus in total the number of queries the algorithm makes is:\begin{align*}& \text{The expected queries to find a single edge } \\& + [ \left(\text{The expected number of singleton edges the algorithm finds}\right) \\& \times \left(\text{The expected number of queries the algorithm makes for each singleton edge it finds}\right)  ]\\&+ \text{expected number of queries made given that the algorithm finds an edge which is not a singelton edge} \end{align*} 
 
Where a singleton edge is defined to be an edge $(u,v)$ such that both $u$ and $v$ are vertices of degree 1.

 
 That is: 
  \beq \frac{n^2}{m} + \left[\left(\frac{m}{m-n} - 1\right) \left(2n + \frac{n^2}{m}\right) \right]+ \frac{n^2}{m} = O\left(\frac{n^2}{m}\right) \eeq

$\left(\frac{m}{m-n} - 1\right) \left(2n + \frac{n^2}{m}\right)  = O(n^2/m)$ since $n + \frac{n^2}{m} < 4n$ and since $m > 10n$ and $\left(\frac{m}{m-n} - 1\right) \left(4n\right) = \frac{4n^2}{m - n} >  \frac{4n^2}{1/2m} = O( \frac{n^2}{m})$. 



\end{proof}
\fi

\begin{proof}\textcolor{red}{TOPROVE 7}\end{proof}



\begin{proof}\textcolor{red}{TOPROVE 8}\end{proof}

\remove{
\section{The case of Claws}
For now we consider the case of permutations. 
\begin{theorem}

\end{theorem}

\newcommand{\lshort}{l_{\text{short}}}
\newcommand{\lmid}{l_{\text{mid}}}
\newcommand{\llong}{l_{\text{long}}}
\newcommand{\tmid}{t_{\text{mid}}}
\newcommand{\wlow}{w_{\text{low}}}
\newcommand{\whigh}{w_{\text{high}}}

\begin{proof}\textcolor{red}{TOPROVE 9}\end{proof}

\begin{proof}\textcolor{red}{TOPROVE 10}\end{proof}
} 
\section{Finding Claws Is almost Instance Optimal when the input has low Complexity} \label{sec:AlmostIO}


In this section we prove Theorem \ref{thm:inst_opt_graphs_intro}. That is, we prove Conjecture \ref{conjgraph}, that finding a claw in a graph is $O(\log(n))$ close to being instance optimal in the regime where finding a claw can be done in $O(\sqrt{n/\log n})$ queries by an algorithm with an unlabeled certificate. For what follows, let $\P_{S_3} = \P_{S_3}(n)$ denote the property of containing a claw (a $3$-star) in $n$-vertex graphs. 

\begin{theorem}[Near-instance optimality in the low-complexity regime] 
\label{thm:near_instance_optimality}
There exist universal constants $C, \alpha > 0$ and a Las Vegas algorithm $A_{\text{all-scales}}$ satisfying the following.
For every graph $G$ on $n$ vertices in which the unlabeled certificate complexity satisfies
$$\RACC(\P_{S_3}, G) \leq \alpha \sqrt{\frac{n}{\log n}},$$
the algorithm $A_{\text{all-scales}}$ detects a claw with expected query complexity at most $C \log n \cdot \RACC(\P_{S_3}, G)$.
\end{theorem}






\iffalse
\begin{theorem}[Near-instance optimality in the sub-random regime] 

Let $\mathcal{P_C}$ be the property of finding a collision in a function $f$. Let $\mathcal{C}$ be the set of all functions $f \colon [n] \to [n]$ for which $\RACC(\mathcal{P_C}, f) \in \Omega(\sqrt{n} / \log n)$. That is, its unlableled certificate complexity is $\Omega(\sqrt{n} / \log n)$. There exists a universal Las Vegas algorithm (without access to an unlabeled certificate) that finds a collision in $(O(\RACC(\mathcal{P_C}, f)) \cdot \log(n)$ for any $f \in \mathcal{C}$.

The same statement is true for equivalence classes of graphs on $n$ vertices, with respect to the property $\P_{S_3}$.
\end{theorem}
\fi


The proof consists of three main parts, each contained in a separate subsection. \Cref{cleaning} modifies the input graph to be more symmetrical to make all further analysis easier. \Cref{sec:nomerge} shows that it is not possible to find certain type of intersections (merges) between walks in time less than $\sqrt{n/\log(n)}$ unless one finds a claw. Lastly, in \Cref{sec:final} we prove that conditioned on no two paths merging, a ``memoryless'' strategy that follows paths at logarithmically many different paths in parallel, where each path $i$ is followed for length $2^i$,
is $O(\log n)$-instance optimal. 

\subsection{Cleaning up the graph} \label{cleaning}
We start by reducing the problem of finding a claw in an $n$-vertex graph $G$ with $n$ vertices to a closely related problem on a graph $G'$ on the same vertex set, where $G'$ additionally satisfies the following symmetry property, which will be useful for our analysis.

\begin{definition}[path-symmetric graph]
A graph $G$ is \emph{path-symmetric} if for every vertex $v$ of degree $1$ in $G$, the other endpoint $u$ of the simple path $P$ containing $v$ in $G$ is also of degree $1$.
\end{definition}

\paragraph{The construction.} Given an undirected graph $G = (V,E)$ with $|V| = n$, $V$ into disjoint sets $V_0 \cup V_1 \cup V_2 \cup V_{3+}$, where for $i=0,1,2$, $V_i$ contains all degree-$i$ vertices in the graph, and $V_{3+}$ includes all other vertices. 

To each $v \in V_1 \cup V_2$ we can uniquely assign a path $P = P(v)$ that contains $v$.
For any $v \in V_1$ (i.e., endpoint of the path $P(v)$), we say that $v$ is a ``yes'' endpoint if the other endpoint $u$ of $P(v)$ is in $V_{3+}$ (i.e., $u$ is a claw center); otherwise, $v$ is a 
``no'' endpoint (and in this case, $u \in V_1$ as well). Let $V_1^{\text{yes}}$ and $V_1^{\text{no}}$ denote the set of ``yes'' and ``no'' endpoints, respectively.

The transformation $G \mapsto G'$ is defined as follows. We add two new vertices $w,w'$ and connect them to all vertices in $V_1^{\text{yes}}$ within $G$. If $|V_1^{\text{yes}}| = 1$, we also connect $w$ to $w'$.
Thus, all vertices from $V_1^{\text{yes}}$ are claw centers in $G'$.
All other vertices in $G$ remain unchanged. The degree of $w,w'$ in $G'$ depends on $n_{\text{yes}} = |V_1^{\text{yes}}|$; it is zero if $n_{\text{yes}} = 0$, two if $n_{\text{yes}} \in \{1,2\}$, and three or more otherwise.
Note that this transformation is deterministic. Thus, an algorithm holding an unlabeled certificate of $G$ can easily construct from it an unlabeled certificate of $G'$.

It immediately follows from the construction that $G'$ is path-symmetric. Our main lemma in this subsection states that finding claws in $G'$ has roughly the same query complexity as in $G$. 
\begin{lemma}
\label{lem:cleaning_up_graph_stars}
$
\RACC(\P_{S_3}, G) = \Theta(\RACC(\P_{S_3}, G')),
$
that is, the expected query complexity of finding a $S_3$ in $G$ with an unlabeled instance is equal, up to a multiplicative constant, to the same query complexity for $G'$.

The same equivalence is true also without access to an unlabeled certificate.
\end{lemma}

The importance of Lemma~\ref{lem:cleaning_up_graph_stars} is that it allows us to eliminate from a graph all vertices of type $V_1^{\text{yes}}$. These vertices are problematic for our analysis; our proof in subsequent subsections relies heavily on symmetry arguments that break down when visiting a vertex of this type. The lemma implies that we can consider graphs in which each path $P$ is of one of two types: either both endpoints of $P$ are claw centers, or both are degree-$1$ vertices.

We note that Lemma~\ref{lem:cleaning_up_graph_stars} holds in any regime, not just the ``low-complexity'' one. In particular, the lemma may be useful toward proving Conjecture~\ref{conjgraph} in the general case.

\begin{proof}\textcolor{red}{TOPROVE 11}\end{proof}

\iffalse
Our starting point is to show that claw finding in graphs with more than, say $2n$ edges can be done trivially in a constant number of rounds.

\begin{lemma}
Let $G$ be a graph with $t$ non-isolated vertices, and at least $2t$ edges among these vertices. The query complexity of finding a claw is $\Theta(n/t)$, both for algorithms that hold an unlabeled certificate and for algorithms without such a certificate.
\end{lemma}
\begin{proof}
The lower bound is trivial: querying a single non-isolated vertex requires $\Omega(n/k)$ queries, even for an algorithm holding a certificate. The rest of the proof is dedicated to the upper bound. 

Consider the (certificate-free) following basic strategy to find a claw. 
\begin{enumerate}
\item Query a vertex uniformly at random; repeat until a non-isolated vertex, $v$, is queried.
\item If $v$ is the center of a claw, end. Otherwise, query the neighbors of $v$.
\item If one of $v$'s neighbors is the center of a claw, end. Otherwise, return to the first bullet.
\end{enumerate}
We claim that a single iteration of this loop (i) requires $O(n/t)$ queries in expectation, and (ii) finds a claw with probability $\Omega(1)$. The statement of the lemma follows immediately from these two claim.

For the first claim, finding a non-isolated $v$ takes $O(n/t)$ random node queries in expectation, since each query finds such a vertex with probability $t/n$. The second and third steps of a single iteration require $O(1)$ neighbor queries each.

For the second claim, 

\end{proof}
\fi

\subsection{Hardness of merging walks} \label{sec:nomerge}



In this subsection we prove a hardness result concerning \emph{merging walks} in graphs. We shall care about a property of \emph{walks}, as \emph{oriented subgraphs} of the input (undirected) graph. Although the main focus of this subsection is on the graph case, the definition below of a walk graph is suitable in both graphs and functions.

\begin{definition}[Algorithm's walk graph]
\label{def:walk-graph}
The walk graph of a query-based algorithm $A$ in $n$-vertex graphs (in the query model) and functions $f \colon [n] \to [n]$ (treated as out directed graphs with out-degree $1$) is initialized as an empty graph on $[n]$. 

Each time that an algorithm $A$ queries a vertex $v$ to obtain a neighbor $u$ of it (or an out-neighbor, in the functions case), we add the oriented edge $v \to u$ to $G_A$. 
\end{definition}
A walk is any connected component (in the undirected sense) of the walk graph, preserving the orientations of the walk graph. That is, it is an object of the following structure:
$$
v_{-k} \leftarrow v_{-k+1} \leftarrow \ldots \leftarrow v_0 \rightarrow v_1 \rightarrow \ldots \rightarrow v_l,
$$
where $k,l \geq 0$ (and can equal zero), and $v_0$ is the first vertex to be queried among $\{v_i : -k \leq i \leq l\}$.

Note that the walk graph is kept oriented (each edge only has a single orientation); if $v \to u$ has already been added to $G_A$, then $u \to v$ cannot be added to it in the future.

\begin{definition}[Merging walks]
\label{def:merging_walks}
Consider the walk graph $G_A$ for an algorithm $A$ in a graph $G$. We say that two walks $$v_{-k} \leftarrow \ldots \leftarrow v_{-1} \leftarrow v_0 \to v_1 \to \ldots \to v_l \ \ \text{ and }\ \  u_{-k'} \leftarrow \ldots \leftarrow u_{-1} \leftarrow u_0 \to u_1 \to \ldots \to u_{l'}$$ 
in $G_A$ with distinct starting points $v_0 \neq u_0$ \emph{merge} if their endpoints intersect non-trivially: $$\{v_{-k}, v_l\} \cap \{u_{-k'}, u_{l'}\} \neq \emptyset.$$ 
\end{definition}


Our main result is a universal (not graph-specific) lower bound on the query complexity of finding merging paths, as long as a the walk-graph of the algorithm does not contain a claw center.
\begin{lemma}[Hardness of merging for graphs]\label{lem:hardness_merge_graphs}
There exists $\alpha > 0$ such that the following holds. For
every $n$-vertex graph $G$ and algorithm $A$ making up to $q = \alpha \sqrt{n / \log n}$ queries to $G$, the probability that walks merge in $G_A$ at least once during the run of $A$ and that further $G_A$ does not contain a claw center at the time the merging takes place, is at most $1/5$. 
\end{lemma}
The constant $1/5$ here is arbitrary (and can be replaced with any other small positive constant). 
\iffalse
\begin{lemma}[Hardness of merging for functions]\label{lem:hardness_merge_functions}
Let $f \colon [n] \to [n]$ be a function and let $A$ be any algorithm making up to $q = o(\sqrt{n} / \log n)$ queries to $f$. The probability that the walk graph of $A$, $G_A$, contains merged walks and does not contain a collision (in $f$) is $o(1)$. 
\end{lemma}
\fi
The above lemma implies that the ``strategy'' of first finding merging walks and then using them to find a claw would require at least $\Omega(\sqrt{n /\log{n}})$ queries. In the next subsection, we show the complementary result: that in a graph where no merges have been observed, a simple strategy that does not require an unlabeled certificate is almost instance-optimal.





\paragraph{Toward the proof of Lemma~\ref{lem:hardness_merge_graphs}.} We start by introducing objects and ideas to be used in the proof of the lemma.
We may assume that the input graph $G$ does not contain claw centers, i.e., has maximum degree at most $2$. (Indeed, querying any edge that contains a claw center makes the algorithm fail, and it is easy to show that removing all these edges cannot decrease the probability to win.)
Therefore, the graph $G$ is a disjoint union of simple paths, simple cycles, and isolated vertices. For what follows, it will be convenient to measure the length of a path/cycle as the number of \emph{vertices} (not edges) it contains, that is, an isolated vertex is a path of length $1$.

We bucket the paths in $G$ into sets of similarly-sized paths and cycles. Let $t = \log_{1.1} n = \Theta(\log n)$, 
and for every $0 \leq i < t$ let $\mathcal{P}_i$ denote the collection of all paths and cycles in $G$ containing at least $1.1^i$ and less than $1.1^{i+1}$ vertices.
Further let $N_i$ denote the sum of lengths of paths and cycles in $\mathcal{P}_i$. Note that these collections are disjoint and thus $\sum_{i=0}^{\log n} N_i = n.$ Define
$$
S = \left\{ 0 \leq i \leq t : N_i \geq \frac{\sqrt{n \log n}}{t}  \right\},
$$
where we note that $\sum_{i \notin S} N_i \leq \sqrt{n \log n}$. In particular, if an algorithm makes $q \leq \alpha \sqrt{n / \log n}$ queries then with probability $1-2\alpha$ none of the vertices in $\bigcup_{i \notin S} \mathcal{P}_i$ will be queried. That is, we may assume that the walk-graph of the algorithm does not intersect $\bigcup_{i \notin S} \P_i$, and contains only walks strictly in paths outside this union.

We prove the statement of Lemma~\ref{lem:hardness_merge_graphs} under an even stronger algorithmic model. Each time that a ``new'' vertex $v$ is being queried by the algorithm $A$, we immediately notify the algorithm about which bucket $\P_i$ the vertex $v$ belongs to. Note that this implies, in particular, that at all times during the algorithm's run, each walk $W$ in the walk-graph of $A$ is contained in a bucket known to the algorithm. We show that even under this stronger assumption, it is hard to merge walks.

Our first main claim is a concentration of measure argument regarding the number of walks within each bucket. It is used in the proof of Lemma~\ref{lem:hardness_merge_graphs} to show that walks in the same bucket have, very roughly speaking, a high ``degree of freedom'', resulting in good bounds on the merging probability.


\begin{claim}[Properties of walks]
\label{claim:walk_properties}
There exists a constant $\alpha > 0$ satisfying the following. Let $A$ be a query-based algorithm in $G$ making $q \leq \alpha \sqrt{n / \log{n}}$ queries. The following statements hold with probability at least $9/10$.
\begin{enumerate}
\item For all $0 \leq i < t$, the number of walks in $G_A$ contained in $\bigcup_{P \in \P_i} P$ is bounded by $\frac{10 N_i q t}{n}$.
\item For all $i \in S$ for which $1.1^i \leq \sqrt{\frac{n}{\log n}}$, the number of such walks is smaller than $\frac{1}{2} |\P_i|$.\footnote{Note that the quantity $|\P_i|$ counts the \emph{number} of paths in $\P_i$, not their total length.} 
\end{enumerate}
\end{claim}
\begin{proof}\textcolor{red}{TOPROVE 12}\end{proof}

\begin{proof}\textcolor{red}{TOPROVE 13}\end{proof}

\subsection{On algorithms for finding claws without merging}\label{sec:final}
We next establish the near-instance-optimality of claw detection in the absence of merging between walks. Our main result is as follows. 

\begin{lemma}[Near instance-optimality in graphs without merges]
\label{lem:instance_optimality_graphs_no_merges}
There exists a constant $\alpha > 0$ for which the following holds.
Let $G$ be a path-symmetric graph admitting an algorithm $A$ for $\P_{S_3}$ with expected query complexity $q_G \leq \frac{1}{20}\sqrt{n}$. Suppose that the probability of $A$ to cause merging in $G_A$ within its first $2q_G$ queries, provided that it has not found a claw before merging, is bounded by $1/5$. 
Then there exists an algorithm $A_{\text{all-scales}}$ without an unlabeled certificate, that finds a claw with expected query complexity $O(q_G \log n)$.
\end{lemma}

\paragraph{The algorithm $A_{\text{all-scales}}$.}
Before proving the lemma, we present the  algorithm $A_{\text{all-scales}}$ operating on a graph $G$ on $n$ vertices $v_1, \ldots, v_n$. The algorithm $A_{\text{all-scales}}$ first picks a random permutation $(v_{\pi(1)}, v_{\pi(2)}, \ldots, v_{\pi(n)})$ of the vertices. Now, for each $0 \leq i < \log n$, define $A^{(i)}$ as follows. 
\textit{Iterate} the following for $j=1,2,\ldots,n$: 
\begin{enumerate}
\item Set $v = v_{\pi(j)}$.
\item Walk from $v$ in both directions (alternately) for $2^{i+1}$ steps, or until one of the following is found: a claw, a path endpoint, or a closing of a simple cycle. Terminate algorithm and report success if a claw was found. In the other two cases, stop the iteration and continue to the next one.
\end{enumerate}

The algorithm $A_{\text{all-scales}}$ simulates one copy of each algorithm $A^{(i)}$, $0 \leq i < \log n$, in an interleaved manner as follows. $A_{\text{all-scales}}$ proceeds in rounds, where in each round it simulates exactly one step for each algorithm $A^{(i)}$. (Note that a single round of $A_{\text{all-scales}}$ uses a total of $\log n$ queries, one for each copy.)



\begin{proof}\textcolor{red}{TOPROVE 14}\end{proof}

\iffalse
\paragraph{Events of interest.}
We now wish to define several types of events that will be useful in the proof of the lemma. 
Let $A$ be any algorithm (for claw detection) making up to $q$ queries in a graph $G$. 

To prove this lemma, we break walks that find claws into $\log n$ many types, where ``type $i$ walks'' are ones where the claw was found following a walk of length roughly $2^i$ to $2^{i+1}$. Our main technical argument is that in the ``sub-merging'' regime, an optimal algorithm for finding such walks involves repeatedly attempting to walk for $2^{i+1}$ steps until success. 

Formally, let 
Let $E_i(G,q,A)$ denote the event that an algorithm $A$ making up to $q$ queries in a graph $G$ contains, within its walk-graph $G_A$, a walk of length between $2^{i}$ and $2^{i+1}-1$ which ends in a claw (where the walk length is measured precisely at the time the claw was first queried). Formally, our main claim regarding the optimality of $A^{(i)}$ is as follows.

Before proving this result, we show how to use it to complete the proof of Lemma \ref{lem:instance_optimality_graphs_no_merges}.

\begin{proof}[Proof of Lemma~\ref{lem:instance_optimality_graphs_no_merges}]

\end{proof}
\fi


It remains to combine all pieces for the proof of Theorem \ref{thm:near_instance_optimality}.
\begin{proof}\textcolor{red}{TOPROVE 15}\end{proof}

%
 
\addcontentsline{toc}{section}{References}
\bibliographystyle{alpha}
\bibliography{main}

\appendix


\end{document}
