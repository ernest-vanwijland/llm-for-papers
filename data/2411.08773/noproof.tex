\documentclass[11pt]{amsart}
\usepackage[top=1.0in, bottom=1.0in, left=1.0in, right=1.0in]{geometry}





\usepackage{amsfonts, amssymb, amsmath, enumerate}
\usepackage{hyperref}
\usepackage[foot]{amsaddr}
\usepackage{bbm}
\usepackage{xcolor}
\usepackage[style=nature]{biblatex}
\usepackage{wasysym}
\addbibresource{ref.bib}
\hypersetup{
	colorlinks,
	linkcolor={red!40!gray},
	citecolor={blue!40!gray},
	urlcolor={blue!70!gray}
}

\newcommand{\michal}[1]{\textcolor{orange}{Michal:\ #1}}
\newcommand{\shabarish}[1]{\textcolor{blue}{Shabarish:\ #1}}

\numberwithin{equation}{section}
\usepackage{amsfonts, amssymb, amsmath, enumerate}


\usepackage{mathrsfs}

\usepackage{mathtools}
\usepackage[normalem]{ulem}
\usepackage{comment}

\usepackage{bbm}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{tikz}



\numberwithin{equation}{section}

\DeclareUnicodeCharacter{2113}{l}

\definecolor{darkgreen}{cmyk}{0.6,0,0.8,0}
\newcommand{\Green}[1]{{\color{darkgreen}  {#1}}}
\def\cmark{\Green{\checkmark}}
\DeclareMathOperator{\ind}{\mathbbm{1}}

\DeclareMathOperator{\E}{\mathbb{E}}
\DeclareMathOperator{\R}{\mathbb{R}}
\DeclareMathOperator{\N}{\mathbb{N}}
\DeclareMathOperator{\Z}{\mathbb{Z}}
\DeclareMathOperator{\bS}{\mathbb{S}}
\DeclareMathOperator{\Pb}{\mathbb{P}}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator*{\diag}{diag}
\DeclareMathOperator*{\Span}{span}
\DeclareMathOperator{\supp}{supp}
\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\degr}{deg}
\DeclareMathOperator*{\re}{Re}
\DeclareMathOperator*{\tr}{tr}
\DeclareMathOperator*{\Tr}{Tr}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\card}{card}
\DeclareMathOperator{\conc}{Conc}
\DeclareMathOperator{\sparse}{Sparse}
\DeclareMathOperator{\polylog}{polylog}
\DeclareMathOperator{\poly}{poly}
\DeclareMathOperator{\argmin}{argmin}
\DeclareMathOperator{\nnz}{nnz}
\DeclareMathOperator{\sym}{sym}
\DeclareMathOperator{\cov}{Cov}
\DeclareMathOperator{\var}{Var}
\DeclareMathOperator{\cdom}{CDom}
\DeclareMathOperator{\dom}{Dom}
\DeclareMathOperator{\spec}{spec}

\def \s {\sigma}
\def \etc {,\ldots,}
\def \P {\mathbb{P}}

\newcommand{\vh}{\widehat{v}}
\newcommand{\uh}{\widehat{u}}
\newcommand{\rh}{\widehat{r}}
\newcommand{\alh}{\widehat{\alpha}}
\newcommand{\qt}{\widetilde{q}}


\newcommand{\xt}{\widetilde{X}}
\newcommand{\xtt}{\widetilde{X}(t)}
\newcommand{\yt}{\widetilde{Y}}
\newcommand{\bh}{\widehat{B}}
\newcommand{\zh}{\widehat{Z}}
\newcommand{\xh}{\widehat{X}}
\newcommand{\yh}{\widehat{Y}}
\newcommand{\bb}{\mathbf{B}}
\newcommand{\mt}{\widetilde{M}}
\newcommand{\ut}{\widetilde{U}}
\newcommand{\vt}{\widetilde{V}}
\newcommand{\deltat}{\widetilde{\delta}}
\newcommand{\gh}{\widehat{G}}
\newcommand{\cS}{\mathcal{S}}
\newcommand{\barcS}{\widebar{\mathcal{S}}}
\newcommand{\cM}{\mathcal{M}}
\newcommand{\cN}{\mathcal{N}}
\newcommand{\cP}{\mathcal{P}}
\newcommand{\supth}{\textsuperscript{th}}
\newcommand{\bu}{\mathbf{u}}
\newcommand{\indep}{\perp \!\!\! \perp}

\DeclareMathOperator{\incomp}{Incomp}
\DeclareMathOperator{\lb}{LB}
\DeclareMathOperator{\comp}{Comp}
\DeclareMathOperator{\TV}{TV}
\DeclareMathOperator{\TM}{TM}
\DeclareMathOperator{\Dom}{Dom}
\DeclareMathOperator{\Le}{Le}
\DeclareMathOperator{\Th}{Th}
\DeclareMathOperator{\e}{\epsilon}
\DeclareMathOperator{\LC}{LC}
\DeclareMathOperator{\im}{Im}
\DeclareMathOperator{\Ber}{Ber}



\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}
\DeclarePairedDelimiter{\abs}{\lvert}{\rvert}
\DeclarePairedDelimiter{\ip}{\langle}{\rangle}
\DeclarePairedDelimiter{\paren}{(}{)}
\DeclarePairedDelimiter{\sqbr}{[}{]}
\DeclarePairedDelimiter{\angbr}{\langle}{\rangle}


\usepackage{amsthm}
\usepackage{thmtools, thm-restate}
\declaretheorem[numberwithin=section]{theorem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem*{lemmanon}{Lemma}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem*{osnaptracemom}{Lemma ~\ref{prop:momestdecoupmatrix}}
\newtheorem*{diffineqlemma}{Lemma ~\ref{lem:diffineq}}

\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\newtheorem{case}{Case}
\newtheorem{step}{Step}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem*{def:OSE}{Definition \ref{def:OSE}}


\begin{document}

\title[Optimal Oblivious Subspace Embeddings with Near-optimal Sparsity]{
Optimal Oblivious Subspace Embeddings \\with Near-optimal Sparsity
}
\author{Shabarish Chenakkod}
\author{Micha{\l} Derezi\'nski}
\author{Xiaoyu Dong}
\thanks{Partially supported by DMS 2054408 and CCF 2338655. The authors are very grateful for the generous help and support of Mark Rudelson throughout the duration of this work.}
\address{University of Michigan, Ann Arbor, MI, USA}
\email{shabari@umich.edu, derezin@umich.edu}
\address{National University of Singapore, Singapore}
\email{xdong@nus.edu.sg}



\begin{abstract}
An oblivious subspace embedding is a random $m\times n$ matrix $\Pi$ such that, for any $d$-dimensional subspace, with high probability $\Pi$ preserves the norms of all vectors in that subspace within a $1\pm\epsilon$ factor. In this work, we give an oblivious subspace embedding with the optimal dimension $m=\Theta(d/\epsilon^2)$ that has a near-optimal sparsity of $\tilde O(1/\epsilon)$ non-zero entries per column~of~$\Pi$. This is the first result to nearly match the conjecture of Nelson and Nguyen [FOCS 2013] in terms of the best sparsity attainable by an optimal oblivious subspace embedding, improving on a prior bound of $\tilde O(1/\epsilon^6)$ non-zeros per column [Chenakkod et al., STOC 2024]. We further extend our approach to the non-oblivious setting, proposing a new family of Leverage Score Sparsified embeddings with Independent Columns, which yield faster runtimes for matrix approximation and regression tasks.

In our analysis, we develop a new method which uses a decoupling argument together with the cumulant method for bounding the edge universality error of isotropic random matrices. To achieve near-optimal sparsity, we combine this general-purpose approach with new traces inequalities that leverage the specific structure of our subspace embedding construction.
\end{abstract}

\maketitle
\thispagestyle{empty}
\newpage
\setcounter{page}{1}


\section{Introduction}
Subspace embeddings are one of the most fundamental techniques in dimensionality reduction, with applications in linear regression \cite{sarlos2006improved}, low-rank approximation \cite{clarkson2013low}, clustering \cite{cohen2015dimensionality}, and many more (see \cite{woodruff2014sketching} for an overview). The key idea is to construct a random linear transformation $\Pi\in\R^{m\times n}$ which maps from a large dimension $n$ to a small dimension $m$, while approximately preserving the geometry of all vectors in a low-dimensional subspace. In many applications, such embeddings must be constructed without the knowledge of the subspace they are supposed to preserve, in which case they are called \emph{oblivious subspace embeddings}.
\begin{definition} \label{def:OSE}
    Random matrix $\Pi\in\R^{m\times n}$ is an $(\varepsilon,\delta,d)$-oblivious subspace embedding (OSE) if for any $d$-dimensional subspace $T\subseteq \R^n$, it holds that
    \begin{align*}
        \Pb\Big(\forall x\in T,\quad (1-\varepsilon)\|x\|\leq \|\Pi x\|\leq (1+\varepsilon)\|x\|\Big)\geq 1-\delta.
    \end{align*}
\end{definition}
The two central concerns in constructing OSEs are: 1) how small can we make the embedding dimension $m$, and 2) how quickly can we apply $\Pi$ to a vector or a matrix. A popular way to address the latter is to use a sparse embedding matrix: If $\Pi$ has at most $s\ll m$ non-zero entries per column, then the cost of computing $\Pi x$ equals $O(s\cdot \nnz(x))$, where $\nnz(x)$ denotes the number of non-zero coordinates in $x$. Designing oblivious subspace embeddings that simultaneously optimize the embedding dimension $m$ and the sparsity $s$ has been the subject of a long line of works \cite{clarkson2013low,meng2013low,nelson2013osnap,bourgain2015toward,cohen2016nearly,chenakkod2024optimal}, aimed towards resolving the following conjecture of Nelson and Nguyen~\cite{nelson2013osnap}, which is supported by nearly-matching lower bounds \cite{nelson2014lower,li2022lower}.
\begin{conjecture}[Nelson and Nguyen, FOCS 2013 \cite{nelson2013osnap}]\label{c:nn}
    For any $n\geq d$ and $\varepsilon,\delta\in(0,1)$, there is an $(\varepsilon,\delta,d)$-oblivious subspace embedding $\Pi\in\R^{m\times n}$ with dimension $m=O((d+\log1/\delta)/\varepsilon^2)$ having $s=O(\log(d/\delta)/\varepsilon)$ non-zeros per column.
\end{conjecture}
Nelson and Nguyen gave a simple construction that they conjectured would achieve these guarantees: For each column of $\Pi$, place scaled random signs $\pm1/\sqrt s$ in $s$ random locations. They showed that this construction achieves dimension $m=O(d\polylog(d)/\varepsilon^2)$ and sparsity $s=O(\polylog(d)/\varepsilon)$. A number of follow-up works \cite{bourgain2015toward,cohen2016nearly} improved on this; most notably, Cohen \cite{cohen2016nearly} showed that a sparse OSE can achieve $m=O(d\log(d)/\varepsilon^2)$ with $s=O(\log(d)/\varepsilon)$. However, none of these guarantees recover the optimal embedding dimension $m=\Theta(d/\varepsilon^2)$, with the extraneous $\log(d)$ factor arising due to a long-standing limitation in existing matrix concentration techniques \cite{troppmatrixconc}. 

This sub-optimality in dimension $m$ was finally addressed in a recent work of Chenakkod, Derezi\'nski, Dong and Rudelson~\cite{chenakkod2024optimal}, relying on a breakthrough in random matrix universality theory by Brailovskaya and van Handel \cite{brailovskaya2022universality}. They achieved $m=\Theta(d/\varepsilon^2)$, but only with a significantly sub-optimal sparsity $s = \tilde O(1/\varepsilon^6)$, which is a consequence of how the universality error is measured and analyzed in \cite{brailovskaya2022universality} (here, $\tilde O$ hides polylogarithmic factors in $d/\varepsilon\delta$). This raises the following natural question: 

\smallskip

\textit{Can the optimal dimension $m=\Theta(d/\varepsilon^2)$ be achieved with the conjectured $\tilde O(1/\varepsilon)$ sparsity?}

\smallskip

We give a positive answer to this question, thus matching Conjecture \ref{c:nn} in dimension $m$ and nearly-matching it in sparsity $s$. To achieve this, we must substantially depart from the approach of Brailovskaya and van Handel, and as a by-product, develop a new set of tools for matrix universality
which are likely of independent interest (see Section \ref{s:overview} for an overview). Remarkably, our result is attained by one of the simple constructions that were originally suggested by Nelson and Nguyen in their conjecture.
\begin{theorem}[Oblivious Subspace Embedding]\label{t:ose}
    For any $n\geq d$ and $\varepsilon,\delta\in(0,1)$ such that $1/\epsilon\delta \leq\poly(d)$, there is an $(\varepsilon,\delta,d)$-oblivious subspace embedding $\Pi\!\in\!\R^{m\times n}$ with $m=O(d/\varepsilon^2)$ having $s=\tilde O(1/\varepsilon)$ non-zeros per column.
\end{theorem}

Many applications of subspace embeddings arise in matrix approximation \cite{woodruff2014sketching} where, given a large tall matrix $A\in\R^{n\times d}$, we seek a smaller $\tilde A\in\R^{m\times d}$ such that $\|\tilde A x\|=(1\pm\varepsilon)\|Ax\|$ for all $x\in\R^d$. 
Naturally, this can be accomplished with an $(\varepsilon,\delta,d)$-OSE matrix $\Pi\in\R^{m\times n}$, by computing $\tilde A=\Pi A$ in time $\tilde O(\nnz(A)/\varepsilon)$ and considering the column subspace of $A$. However, given direct access to $A$, one may hope to get true input sparsity time $O(\nnz(A))$ by leveraging the fact that the embedding need not be oblivious.

To that end, we adapt our subspace embedding construction, so that it can be made even sparser given additional information about the leverage scores of matrix $A$. The $i$th leverage score of $A$ is defined as the squared norm of the $i$th row of the matrix obtained by orthonormalizing the columns of $A$~\cite{drineas2006sampling}. We show that if the $i$th leverage score of $A$ is bounded by $l_i\in[0,1]$, then the $i$th column of $\Pi$ needs only $\max\{1,\tilde O(l_i/\varepsilon)\}$ non-zero entries. Since the leverage scores of $A$ can be approximated quickly \cite{drineas2012fast}, this leads to our new algorithm, Leverage Score Sparsified embedding with Independent Columns (LESS-IC), which is inspired by related constructions that use LESS with independent rows ~\cite{less-embeddings,newton-less,gaussianization}.

Just like recent prior works \cite{chepurko2022near,cherapanamjeri2023optimal,chenakkod2024optimal}, our algorithm for constructing a subspace embedding from a matrix~$A$ incurs a preprocessing cost of $O(\nnz(A)+d^\omega)$ required for approximating the leverage scores (here, $\omega$ is the matrix multiplication exponent). However, our approach significantly improves on these prior works in the $\poly(d/\varepsilon)$ embedding cost, leading to matching speedups in downstream applications such as constrained/regularized least squares~\cite{chenakkod2024optimal}.
\begin{theorem}[Fast Subspace Embedding]\label{t:fast}
    Given $A\!\in\!\R^{n\times d}$, $\varepsilon,\gamma\!\in\!(0,1)$ and $1/\varepsilon\!\leq\!\poly(d)$,~in
    \begin{align*}
        O\big(\gamma^{-1}\nnz(A) + d^\omega + \varepsilon^{-1} d^{2+\gamma}\polylog(d)\big)\quad\text{time}
    \end{align*}
    we can compute
    $\tilde A\in\R^{m\times d}$ such that $m=O(d/\varepsilon^2)$ and with probability $\geq 0.99$
  \begin{align*}
    (1-\varepsilon)\|Ax\|\leq \|\tilde A x\| \leq
    (1+\varepsilon)\|Ax\|\qquad\forall x\in\R^d.
  \end{align*}
\end{theorem}
This is a direct improvement over the previous best known runtime for constructing an optimal subspace embedding \cite{chenakkod2024optimal}, which suffers an additional $\tilde O(d^{2+\gamma}/\varepsilon^6)$ cost due to their sub-optimal sparsity. Remarkably, our result is also the first to achieve $\tilde O(d^{2+\gamma}/\varepsilon)$ dependence even if we allow a sub-optimal dimension, i.e., $m=O(d\log(d)/\varepsilon^2)$. Here, the previous best time \cite{chepurko2022near,cherapanamjeri2023optimal} has an additional $\tilde O(d^{2+\gamma}/\varepsilon^2)$ cost, due to using a two-stage leverage score sampling scheme in place of a sparse embedding matrix. Our new LESS-IC embedding is crucial in achieving the right dependence on $\varepsilon$, as neither of the previous constructions appear capable of overcoming the $\Omega(d^{2+\gamma}/\varepsilon^2)$ barrier.

As an example application of our results, we show how our fast subspace embedding construction can be used to speed up reductions for a wide class of optimization problems based on constrained or regularized least squares regression, including Lasso regression \cite{bourgain2015toward}. The following corollary follows immediately from Theorem \ref{t:fast}, and is a direct improvement over Theorem 1.8 of \cite{chenakkod2024optimal} in terms of the runtime dependence on $\epsilon$ from $\tilde O(d^{2+\gamma}/\epsilon^6)$ to $\tilde O(d^{2+\gamma}/\epsilon)$, while achieving a matching $O(d/\epsilon^2)\times d$ reduction.
\begin{corollary}[Fast reduction for constrained least squares]\label{t:reduction}
   Given $A\in\R^{n\times d}$, $b\in\R^n$, $\epsilon>0$, function $g:\R^d\rightarrow\R_{\geq 0}$ and set
    $\mathcal C\subseteq\R^d$ consider an $n\times d$  problem $\mathrm{LS}_{\mathcal C,g}(A,b,\epsilon)$:
    \begin{align*}
    \text{Find $\tilde x$ such that}\quad f(\tilde x)\leq (1+\epsilon)\min_{x\in\mathcal C}
      f(x),\quad\text{where}\quad
      f(x) = \|Ax-b\|_2^2+ g(x).
    \end{align*}
    There is an algorithm that reduces this problem to an
    $O(d/\epsilon^2)\times d$ instance $\mathrm{LS}_{\mathcal C,g}(\tilde A,\tilde
    b,0.1\epsilon)$ in $O(\gamma^{-1}\nnz(A) + d^\omega +
    \epsilon^{-1}d^{2+\gamma}\polylog(d))$  time.
  \end{corollary}



\section{Related Work}

Subspace embeddings have played a central role in the area of randomized linear algebra ever since the work of Sarlos \cite{sarlos2006improved} (for an overview, see the following surveys and monographs \cite{woodruff2014sketching,drineas2016randnla,martinsson2020randomized,derezinski2024recent}). Initially, these approaches focused on leveraging fast Hadamard transforms \cite{ailon2009fast,tropp2011improved} to achieve improved time complexity for linear algebraic tasks such as linear regression and low-rank approximation. Clarkson and Woodruff \cite{clarkson2013low} were the first to propose a sparse subspace embedding matrix, the CountSketch, which has exactly one non-zero entry per column but does not recover the optimal embedding dimension guarantee. Before this, the idea of using a sparse random matrix for dimensionality reduction was successfully employed in the context of Johnson-Lindenstrauss embeddings \cite{dasgupta2010sparse,kane2014sparser}, which seek to preserve the geometry of a finite set, as opposed to an entire subspace. 

In addition to the aforementioned efforts in improving sparse subspace embeddings \cite{clarkson2013low,meng2013low,nelson2013osnap,bourgain2015toward,cohen2016nearly,chenakkod2024optimal}, some works have aimed to develop fast subspace embeddings that achieve optimal embedding dimension either without sparsity \cite{chepurko2022near,cherapanamjeri2023optimal}, under additional assumptions \cite{cartis2021hashing}, or with one-sided embedding bounds \cite{tropp2025comparison}. Our time complexity result, Theorem \ref{t:fast}, improves on all of these in terms of the dependence on $\varepsilon$, thanks to a combination of our new analysis techniques and the new LESS-IC construction.


\section{Main Results}


In this section, we define the subspace embedding constructions used in our results, and provide detailed statements of our theorems.


As is customary in the literature, we shall work with an equivalent form of the subspace embedding guarantee from Definition \ref{def:OSE}, which frames this problem as a characterization of the extreme singular values of a class of random matrices. Namely, consider a deterministic $n\times d$ matrix $U$ with orthonormal columns that form the basis of a $d$-dimensional subspace~$T$. Then, a random matrix $\Pi\in\R^{m\times n}$ is an $(\varepsilon,\delta,d)$-subspace embedding for $T$ if and only if all of the singular values of the matrix $\Pi U$ lie in $[1-\varepsilon,1+\varepsilon]$ with probability $1-\delta$, i.e., 
\begin{align}
\Pr(1-\varepsilon\leq s_{\min}(\Pi U)\leq s_{\max}(\Pi U)\leq 1+\varepsilon)\geq 1-\delta,\label{eq:ose-equiv}
\end{align}
where $s_{\min}$ and $s_{\max}$ denote the smallest and largest singular values. To ensure that $\Pi$ is an oblivious subspace embedding, we must therefore ensure \eqref{eq:ose-equiv} for the family of all random matrices of the form $\Pi U$, where $U$ is any $n\times d$ matrix with orthonormal columns.

\subsection{Oblivious Subspace Embeddings}

Our subspace embedding guarantees are achieved by a family of OSEs which have a fixed number of non-zero entries in each column, a key property that was also required of sparse OSE distributions called OSNAP described by Nelson and Nguyen \cite{nelson2013osnap}. As we explain later, our analysis techniques apply to other natural families of sparse embedding distributions, including those with i.i.d.~entries \cite{achlioptas2003database}, however the OSNAP-style construction is crucial for achieving the near-optimal sparsity $s=\tilde O(1/\varepsilon)$.

In our construction of the $m\times n$ OSE matrix $\Pi$, we start by defining an unscaled version of the matrix, called $S$, which has entries in $\{-1,0,1\}$. We then scale $S$ to appropriately normalize the entry-wise variances, obtaining $\Pi$.
Concretely, we wish to obtain an $m \times n$ sparse random matrix $S$ which has exactly $s$ non-zero $\pm1$ entries in each column. Assume $s$ exactly divides $m$. Then we can divide each column of $S$ into $s$ subcolumns and randomly populate one entry in each subcolumn by a Rademacher random variable (see Figure \ref{fig:osnap}). We call this family of distributions (unscaled) OSNAP, carrying over Nelson and Nguyen's terminology (technically, their definition is somewhat broader than ours).

\begin{figure}[h!]
    \centering
    \begin{tikzpicture}[scale=1]
      \pgfmathsetmacro{\a}{1.4};
      \pgfmathsetmacro{\b}{2.8};
      \pgfmathsetmacro{\c}{3.2};
      \pgfmathsetmacro{\d}{4};
      \pgfmathsetmacro{\aa}{0.2};
      \pgfmathsetmacro{\aaa}{0};
      \pgfmathsetmacro{\bbb}{0.8};
      \pgfmathsetmacro{\ccc}{1.6};
      \pgfmathsetmacro{\ddd}{2.6};
      \pgfmathsetmacro{\eee}{3.8};
      \pgfmathsetmacro{\fff}{4.8};
      \draw (0, -1) rectangle (5,2);
      \draw (2.5,2.25) node {\mbox{\scriptsize Embedding matrix $\Pi$}};

    \draw (\ccc,-1) rectangle (\ccc + 0.2,0); 
    \draw (\ccc+0.1,-0.3) node {$*$};
    \draw (\ccc,0) rectangle (\ccc + 0.2,1);
    \draw (\ccc+0.1,0.5) node {$*$};
    \draw (\ccc,1) rectangle (\ccc + 0.2,2);
    \draw (\ccc+0.1,1.9) node {$*$};

  
\end{tikzpicture}
    \caption{An example of a column divided into $s=3$ subcolumns with each subcolumn having exactly one non-zero entry in a random position.}
    \label{fig:osnap}
\end{figure}


Each non-zero entry in the matrix $S$ can be identified by a tuple $(l, \gamma) \in [n]\times [s]$ where $l$ identifies the column of the non-zero entry and $\gamma$ is the index of the entry in that column. Thus the $(l,\gamma)\textsuperscript{th}$ non-zero entry in $S$ is located in column $l$ and row $\mu_{(l,\gamma)}$, where $\mu_{(l,\gamma)}$ is a uniformly chosen integer from the interval $[(m/s)(\gamma-1)+1:(m/s)\gamma]$. For example, the $(1,1)\textsuperscript{th}$ non-zero entry in $S$ is located in column $1$ and some row in the interval $[1:m/s]$. An $m \times n$ matrix with a non-zero entry in column $l$ and row $\mu_{(l,\gamma)}$ is given by $e_{\mu_{(l,\gamma)}}e_l^T$, where $e_{\mu_{(l, \gamma)}}$ and $e_l$ represent standard basis vectors in $\R^m$ and $\R^n$ respectively, and for $S$ we wish to place a random sign $\xi_{(l,\gamma)}$ at this position. This motivates our formal definition for OSNAP,


\begin{definition}[OSNAP]\label{def:osnap} 
An $m \times n$ random matrix $S$ is called an unscaled oblivious sparse norm-approximating projection with $K$-wise independent subcolumns ($K$-wise independent unscaled OSNAP) with parameters $p, \varepsilon, \delta \in (0,1]$ such that $s=pm$ divides $m$ if,
\[ S = \sum_{l=1}^n \sum_{\gamma=1}^s \xi_{(l,\gamma)} e_{\mu_{(l, \gamma)}} e_l ^\top \]
where,
\begin{itemize}
    \item $\{ \xi_{(l,\gamma)} \}_{l \in [n], \gamma \in [s]}$ is a collection of $K$-wise independent Rademacher random variables.
    \item $\{ \mu_{(l,\gamma)} \}_{l \in [n], \gamma \in [s]}$ is a collection of $K$-wise independent random variables such that each $\mu_{(l,\gamma)}$ is uniformly distributed in $[(m/s)(\gamma-1)+1:(m/s)\gamma]$.
    \item The collection $\{ \xi_{(l,\gamma)} \}_{l \in [n], \gamma \in [s]}$ is independent from the collection $\{ \mu_{(l,\gamma)} \}_{l \in [n], \gamma \in [s]}$.
\end{itemize}

In this case, $\Pi = (1/\sqrt{pm})S$ is called a $K$-wise independent OSNAP with parameters $p, \varepsilon, \delta$. In addition, if all the random variables in the collections $\{ \xi_{(l,\gamma)} \}_{l \in [n], \gamma \in [s]}$ and $\{ \mu_{(l,\gamma)} \}_{l \in [n], \gamma \in [s]}$ are fully independent, then $S$ is called a fully independent unscaled OSNAP and $\Pi$ is called a fully independent OSNAP.
\end{definition}



Thus, each column of the OSNAP matrix $\Pi$ has $s = pm$ many non-zero entries, and the sparsity level can be varied by setting the parameter $p \in [0,1]$ appropriately. With the distribution formally defined, we now provide the full statement of our subspace embedding guarantee for OSNAP,

\begin{restatable}[Subspace Embedding Guarantee for OSNAP]{theorem}{osnapmainthm}
\label{t:ose-full}
Let $\Pi = (1/\sqrt{pm})S$ be an $m \times n$ matrix distributed according to the $8 \lceil\log (\frac{d}{\varepsilon \delta})\rceil$-wise independent OSNAP distribution with parameter $p$. Let $U$ be an arbitrary $n \times d$ deterministic matrix such that $U^\top U=I$. Then, there exist positive constants $c_{\ref*{t:ose-full}.1}$ and $c_{\ref*{t:ose-full}.2}$ such that for any $0 < \delta, \varepsilon < 1$ and $d>10$,  we have 
\begin{align*}
\Pb \left( 1 - \varepsilon  \leq s_{\min}(\Pi U)   \leq s_{\max}(\Pi U) \leq 1 + \varepsilon \right) \geq 1-\delta
\end{align*}
if the embedding dimension satisfies $m \ge c_{\ref*{t:ose-full}.1}  (d + \log(1/\delta\varepsilon))/\varepsilon^2$ and the sparsity $s=pm$ satisfies $s \ge \min\{c_{\ref*{t:ose-full}.2} (\log ^2(\frac{d}{\varepsilon\delta})/\varepsilon+\log^3(\frac{d}{\varepsilon\delta})),m\}$ non-zeros per column.
\end{restatable}

\begin{remark}
We note that if $1/\varepsilon$ is polynomial in $d/\delta$, i.e., $\varepsilon \ge \frac{1}{(d/\delta)^K}$ for some absolute constant $K\geq 1$, then the $\log(1/\varepsilon)$ term in $\log(d/\varepsilon\delta) = \log(d/\delta) + \log(1/\varepsilon)$ is dominated by $\log(d/\delta)$. In this case, our requirement will become
\begin{align*}pm \ge \min \left\{C(K) \paren*{\frac{(\log (d/\delta))^2}{\varepsilon}+(\log (d/\delta))^3},m \right\}
\end{align*}
for some constant $C(K)$ depending only on $K$.
A weaker lower bound on $\varepsilon$, $\varepsilon > 1/e^d$ is sufficient to reduce the requirement on $m$ to:
\[ m \ge 2c_{\ref*{t:ose-full}.1}  \frac{d + \log(1/\delta)}{\varepsilon^2}.   \]

\end{remark}


This is a direct improvement over Theorem 1.2 of  \cite{chenakkod2024optimal}, which requires sparsity $s\geq c\log^4(d/\delta)/\varepsilon^6$ where $c$ is an absolute constant, with the same condition on $m$. The primary gain lies in the polynomial dependence on $1/\varepsilon$, but we note that our result also achieves a better logarithmic dependence on $d$, which means that an improvement is obtained even for $\varepsilon=\Theta(1)$.

Our techniques can be used to obtain a similar result for a simple OSE model with i.i.d.~sparse Rademacher entries \cite{achlioptas2003database}, which was also considered by \cite{chenakkod2024optimal}. However, in this case, we need an additional requirement of $s=pm \ge c\log (\frac{d}{\varepsilon\delta})/\varepsilon^2$ for the sparsity (see Section \ref{sec:oseieproof} for details; this is again a direct improvement over a result of \cite{chenakkod2024optimal}).
\begin{remark}
The $1/\varepsilon^{2}$ factor in the column–sparsity of an OSE model with i.i.d. entries is unavoidable.  
To see why, let
\[
  U=\begin{bmatrix}I_{d}\\[2pt] 0\end{bmatrix},
  \qquad
  \Pi=\frac{1}{\sqrt{pm}}\;S ,
\]
and note that
$\sigma_{\min}(\Pi U),\sigma_{\max}(\Pi U)\in[1-\varepsilon,\,1+\varepsilon]$
forces, for every $j\le d$,
\begin{equation}\label{oseielower}
  \bigl|\|\Pi e_j\|_{2}^{2}-1\bigr|
  =\Bigl|\tfrac{N_j}{pm}-1\Bigr|
  \le\varepsilon,
  \qquad
  N_j:=\operatorname{nnz}(S e_j)\sim\operatorname{Binomial}(m,p).
\end{equation}
 
Set
\[
  Z:=\frac{N_j-pm}{\sqrt{mp(1-p)}} ,
  \qquad
  a:=\varepsilon\,\sqrt{\frac{mp}{1-p}}\le\sqrt2\,\varepsilon\sqrt{mp}\quad( \text{for } p\le\tfrac12).
\]
Condition \ref{oseielower} is equivalent to $|Z|\le a$.  
With $F_Z(x)=\Pr[Z\le x]$ and $\Phi$ the standard normal cumulative distribution function,
the Berry Esseen theorem gives
\[
  \sup_{x\in\mathbb R}|F_Z(x)-\Phi(x)|\le\frac{6}{\sqrt{mp}}.
\]
Hence
\[
  \Pr\bigl[|Z|\le a\bigr]
  \;=\;F_Z(a)-F_Z(-a)
  \;\le\;\bigl(\Phi(a)-\Phi(-a)\bigr)+\frac{12}{\sqrt{mp}}.
\]
Using $\Phi(a)-\Phi(-a)=2\int_{0}^{a}\!\phi(t)\,dt\le a/\sqrt{\pi}$ and the bound on $a$, we have
\[
  \Pr\left( \bigl|\|\Pi e_j\|_{2}^{2}-1\bigr| \le \varepsilon \right)
  \;\le\;
  \frac{a}{\sqrt{\pi}}+\frac{12}{\sqrt{mp}}
  \;\le\;
  \frac{\sqrt2}{\sqrt{\pi}}\varepsilon\sqrt{mp}+
  \frac{12}{\sqrt{mp}}
\]

By general lower bounds for OSE, we know that, when $\varepsilon \to 0$, we need $pm \to \infty$ and therefore so $\frac{12}{\sqrt{mp}} \to 0$.

Therefore, for small enough $\varepsilon$, if $pm<c/\varepsilon^{2}$ with $c:=\frac{1}{81}$, the right–hand side is $<\tfrac13$.
Thus any OSE-IE that succeeds with constant probability must satisfy $pm=\Omega\!\bigl(\varepsilon^{-2}\bigr)$.
\end{remark}


\subsection{Characterization via a Moment Property}
\label{s:osemoments}

Our proof techniques for Theorem \ref{t:ose-full} are based on the moment method, and thus, they naturally imply the following slightly stronger moment-based characterization of an oblivious subspace embedding, which was proposed by \cite{cohen2016optimal} as an extension of the corresponding moment-based characterization of a Johnson-Lindenstrauss embedding \cite{kane2014sparser}.
\begin{definition}\label{d:osemoments}
    A distribution $\mathcal{D}$ over $\mathbb{R}^{m \times n}$ has
$(\varepsilon,\delta,d,\ell)$-OSE moments if, for all matrices
$U \in \mathbb{R}^{n \times d}$ with orthonormal columns,
\begin{align*}
  \E_{\Pi \sim \mathcal{D}}
  \bigl\|(\Pi U)^{T}(\Pi U) - I\bigr\|^{\ell}
  < \varepsilon^{\ell}\delta.
\end{align*}
\end{definition}
Note that a simple application of Markov's inequality recovers the guarantee in Definition~\ref{def:OSE} from the $(\varepsilon,\delta,d,\ell)$-OSE moments property with any $\ell\geq 1$. Moreover, \cite{cohen2016optimal} showed that this moment-based OSE characterization implies several other desirable guarantees of embedding matrices in the context of approximate matrix multiplication, generalized regression and low-rank approximation.

As an immediate consequence of our analysis, we obtain the following OSE moment guarantee for the OSNAP distribution.

\begin{corollary}\label{cor:osemoments}
    Let $\Pi$ be an $m \times n$ matrix with an OSNAP distribution having sparsity~$s$. Let $0 < \delta, \varepsilon < 1$ and $d>10$. Then $\Pi$ has $(\varepsilon, \delta, d, \ell)$-OSE moments with $\ell = 16\log(\frac{d}{\varepsilon \delta})$ when $m \ge c_{\ref*{cor:osemoments}.1}  (d + \log(1/\delta\varepsilon))/\varepsilon^2$ and  $s \geq \min\{c_{\ref*{cor:osemoments}.2} (\log ^2(\frac{d}{\varepsilon\delta})/\varepsilon+\log^3(\frac{d}{\varepsilon\delta})),m\}$.
\end{corollary}
\begin{remark}
$\Pi$ can be applied to a matrix $A$ in time $O(\nnz(A)(\log^2(\frac{d}{\varepsilon\delta})/\varepsilon + \log^3(\frac{d}{\varepsilon\delta})))$.  
As noted by \cite[Remark 3]{cohen2016optimal}, such runtimes can be further refined by chaining together several embeddings with an OSE moment property. For example, \cite{cohen2016nearly} showed that  OSNAP with $m=O(d\log(d/\delta)/\varepsilon^2)$ and $s=O(\log(d/\delta)/\varepsilon)$ has $(\varepsilon,\delta,d,\log(d/\delta))$-OSE moments. Thus, letting $\varepsilon=\Theta(1)$ for simplicity, we can combine a $O(d\log (d/\delta))\times n$ OSNAP matrix $\Pi_1$ having sparsity $s=O(\log(d/\delta))$ together with a $O(d+\log(1/\delta))\times O(d\log(d/\delta))$ OSNAP matrix having sparsity $s=O(\log^3(d/\delta))$ to obtain $\Pi=\Pi_2\Pi_1$ with $(\Theta(1),\delta,d,\log(d/\delta))$-OSE moments which can be applied to a matrix $A\in\R^{n\times d}$ in time $O(\nnz(A)\log(d/\delta) + d^2\log^4(d/\delta))$.
\end{remark}

\subsection{Leverage Score Sparsified Embedding with Independent Columns}
In a related problem, we seek to embed a subspace given by a fixed $U \in \R^{n \times d}$, with information about the squared row norms of $U$ being used to define the distribution of non-zero entries in $\Pi$. Such distributions for $\Pi$ are called non-oblivious (a.k.a.~data-aware) subspace embeddings. Previous work \cite{chenakkod2024optimal} has dealt with one such family of distributions termed LESS embeddings \cite{less-embeddings,newton-less,gaussianization}, showing that they require $\tilde O(1/\varepsilon^4)$ non-zero entries \textit{per row} of $\Pi$ to obtain an $\varepsilon$-embedding guarantee. Since the embedding matrix is very wide, this leads to a much sparser embedding (sparser than any OSE) that can be applied in time sublinear in the input size, leading to fast subspace embedding algorithms. 

In this work, we show that our new techniques  also extend to LESS embeddings and enable us to prove sharper sparsity estimates than~\cite{chenakkod2024optimal}. To fully leverage our approach, we define a new type of sparse embedding (LESS-IC), which can be viewed as a cross between CountSketch and LESS. Here, IC stands for independent columns. At a high level, the CountSketch part ensures that we can use our decoupling method to achieve optimal dependence on $1/\varepsilon$, while the LESS part enables adaptivity to a fixed subspace.

Specifically, a LESS-IC embedding matrix $\Pi$ has a fixed number of non-zero entries in each column, chosen so that it is proportional to the leverage score (i.e. the squared row norm) of the corresponding row of $U$. This is achieved by modifying the OSNAP distribution such that the number of subcolumns is no longer the same in each column. For columns corresponding to very small leverage scores, we only have one ``subcolumn''. Thus, each column has at least one non-zero~entry. This means that the cost of applying LESS-IC to an $n\times d$ matrix $A$ can no longer be sublinear (like it can in the existing LESS embedding constructions), but rather has a fixed linear term of $O(\nnz(A))$, plus an additional sublinear term. Given that the preprocessing step of approximating the leverage scores has to take at least $\nnz(A)$ time, the linear term in the cost of applying LESS-IC is negligible.

To generate an embedding matrix with the LESS-IC distribution, it suffices to have a good enough approximation for the leverage scores of the matrix $U$, in the following sense.

\begin{definition}[Approximate Leverage Scores]\label{def:apprls}
    Given a matrix $U\in\R^{n\times d}$ with orthonormal columns and $\beta_1 \ge 1, \beta_2 \ge 1$, a tuple $(l_1, \ldots, l_n) \in [0,1]^n$ of numbers are $(\beta_1,\beta_2)$-approximate leverage scores for $U$ if, for $1\leq i\leq n$,
    \begin{align*}
        \frac{\norm{e_i^\top U}^2}{\beta_1} \leq l_i \qquad\text{and}\qquad
    \sum_{i=1}^n l_i \leq \beta_2 \sum_{i=1}^n \norm{e_i^\top U}^2 = \beta_2 d.
    \end{align*}
    We say that the numbers $(l_1, \ldots, l_n) \in [0,1]^n$ are $\beta$-approximations of the leverage scores (i.e. squared row norms) of $U$ with $\beta=\beta_1\beta_2$.
\end{definition}

To see how approximate leverage scores determine the distribution of entries in the LESS-IC distribution, let us first consider a simpler distribution, LESS-IE from \cite{chenakkod2024optimal}, based on a similar construction first proposed by \cite{less-embeddings}. Here, we once again start by defining an unscaled matrix $S$, which is then normalized to obtain the subspace embedding matrix $\Pi$.

\begin{definition}[LESS-IE]\label{def:lessindent}
An $m \times n$ random matrix $S$ is called an unscaled leverage score sparsified embedding with independent entries (unscaled LESS-IE), and also $\Pi = (1/\sqrt{pm})S$ is called a LESS-IE, corresponding to $(\beta_1, \beta_2)$-approximate leverage scores $(z_1,...,z_n)$ with parameter $p$, if $S$ has entries $s_{i,j}=\frac{1}{\sqrt{\beta_1  z_j}} \delta_{i,j} \xi_{i,j}$ where $\delta_{i,j}$ are independent Bernoulli random variables taking value 1 with probability $p_{ij}= \beta_1 z_j p$, whereas $\xi_{i,j}$ are i.i.d.~Rademacher random~variables. 
\end{definition}

In the LESS-IE model, we have $\beta_1 pm z_j $ many non-zero entries in  column $j$ in expectation. However, to achieve $1/\varepsilon$ dependency of the sparsity, we need to have \emph{exactly} $\beta_1 pm z_j $ many non-zero entries in the column in the LESS-IC model to fully take advantage of the error cancellation that occurs in our decoupling argument (See Section \ref{subsec:decouposnap} and Section \ref{subsec:oseiediag}). Though these sections deal with oblivious subspace embeddings, the same arguments still apply in the LESS case). This is done by modifying the OSNAP construction so that the size (and consequently, the number) of subcolumns is different across~columns.

Notice that to have $\beta_1 pm z_j$ many non-zero entries in column $j$, we would need $\beta_1 pm z_j$ many subcolumns in column $j$ each with one non-zero entry in a random position. This means that the size of each subcolumn needs to be $m/(\beta_1 pm z_j) = 1/(\beta_1pz_j)$. However, since $1/(\beta_1pz_j)$ may not be an integer, we consider subcolumns of size $ b_j := \max \{ \lfloor 1/(\beta_1pz_j)\rfloor, 1 \}$. 

In column $j$, we stack subcolumns of size $b_j$ until we fill up all the rows up to $m$. Let $s_j$ be the smallest number of subcolumns to do this. Then, it may happen that the row indices of the bottom-most subcolumn exceed $m$. For example, consider the distribution on the first column of $\Pi$ when $m=70$, and $b_1 = 15$. In this case $s_1 = 5$, so we can stack four subcolumns of size 15 and the $5\textsuperscript{th}$ subcolumn only spans row indices $[61:70]$. In each subcolumn, we randomly choose a row to place a non-zero entry, which would be a Rademacher random variable. (See Figure \ref{fig:lessic}). The non-zero entries are appropriately scaled so that all entries of the matrix have the same variance (See Section \ref{sec:lessproofs} for the full definition).

\begin{figure}[h!]
    \centering
    \begin{tikzpicture}[scale=1]
      \pgfmathsetmacro{\a}{1.4};
      \pgfmathsetmacro{\b}{2.8};
      \pgfmathsetmacro{\c}{3.2};
      \pgfmathsetmacro{\d}{4};
      \pgfmathsetmacro{\aa}{0.2};
      \pgfmathsetmacro{\aaa}{0};
      \pgfmathsetmacro{\bbb}{0.8};
      \pgfmathsetmacro{\ccc}{1.6};
      \pgfmathsetmacro{\ddd}{2.6};
      \pgfmathsetmacro{\eee}{3.8};
      \pgfmathsetmacro{\fff}{4.8};
      \draw (0,-0.8) rectangle (5,2);
      \draw (2.5,2.25) node {\mbox{\scriptsize Embedding matrix $\Pi$}};

  \draw (5.5,1) node {\mbox{$\times$}};
  \draw (7,2.25) node {\mbox{\scriptsize Matrix $U$}};  
  \draw[fill=red!20] (6,-3) rectangle (8,2);

    \draw (\aaa,-0.8) rectangle (\aaa + 0.2,-0.4); 
    \draw (\aaa+0.1,-0.5) node {$*$};
  \draw (\aaa,-0.4) rectangle (\aaa + 0.2,0.2); 
  \draw (\aaa+0.1,-0.3) node {$*$};
  \draw (\aaa,0.2) rectangle (\aaa + 0.2,0.8);
  \draw (\aaa+0.1,0.7) node {$*$};
  \draw (\aaa,0.8) rectangle (\aaa + 0.2,1.4);
  \draw (\aaa+0.1,1.3) node {$*$};
  \draw (\aaa,1.4) rectangle (\aaa + 0.2,2);
  \draw (\aaa+0.1,1.7) node {$*$};
    \draw[fill=blue!30] (6, 1.8 - \aaa) rectangle (8, 2 - \aaa);

    \draw (\bbb,-0.8) rectangle (\bbb + 0.2,-0.4); 
    \draw (\bbb+0.1,-0.7) node {$*$};
    \draw (\bbb,-0.4) rectangle (\bbb + 0.2,0.4);
    \draw (\bbb+0.1,0.3) node {$*$};
    \draw (\bbb,0.4) rectangle (\bbb + 0.2,1.2);
    \draw (\bbb+0.1,0.7) node {$*$};
    \draw (\bbb,1.2) rectangle (\bbb + 0.2,2);
    \draw (\bbb+0.1,1.9) node {$*$};
    \draw[fill=blue!30] (6, 1.8 - \bbb) rectangle (8, 2 - \bbb);
       
  
    \draw (\ccc,-0.8) rectangle (\ccc + 0.2,0); 
    \draw (\ccc+0.1,-0.3) node {$*$};
    \draw (\ccc,0) rectangle (\ccc + 0.2,1);
    \draw (\ccc+0.1,0.5) node {$*$};
    \draw (\ccc,1) rectangle (\ccc + 0.2,2);
    \draw (\ccc+0.1,1.9) node {$*$};
    \draw[fill=blue!30] (6, 1.8 - \ccc) rectangle (8, 2 - \ccc);

    \draw (\ddd,-0.8) rectangle (\ddd + 0.2,0.6);
    \draw (\ddd+0.1,-0.1) node {$*$};
    \draw (\ddd,0.6) rectangle (\ddd + 0.2,2);
    \draw(\ddd + 0.1 ,1.7) node {$*$};
    \draw[fill=blue!30] (6, 1.8 - \ddd) rectangle (8, 2 - \ddd);

    \draw (\eee,-0.8) rectangle (\eee + 0.2,-0.2); 
    \draw (\eee+0.1,-0.3) node {$*$};
    \draw (\eee,-0.2) rectangle (\eee + 0.2,2);
    \draw (\eee+ 0.1,-0.1) node {$*$};
    \draw[fill=blue!30] (6, 1.8 - \eee) rectangle (8, 2 - \eee);

    \draw (\fff,-0.8) rectangle (\fff + 0.2,2); 
\draw (\fff + 0.1, 1.1) node {$*$};
    \draw[fill=blue!30] (6, 1.8 - \fff) rectangle (8, 2 - \fff);
    
  \draw (8.95,2.25) node {\mbox{\fontsize{7}{7}\selectfont leverage scores}};
  \foreach \i in {0,...,24}
  {
    \pgfmathtruncatemacro{\x}{\i^2};
    \draw[fill=darkgreen!30] (8.1, 0.2 * \i - 3)
    rectangle (8.1 + 0.0025 * \x, 0.2 * \i - 2.8);
  }
\end{tikzpicture}
    \caption{In the LESS-IC distribution, column $j$ is filled with $s_j$ many subcolumns, with the bottom-most subcolumn truncated to fit the size of $\Pi$. Each subcolumn has one non-zero entry. Notice that as the leverage scores decrease, the number of subcolumns decreases and the matrix becomes sparser. However, each column always has at least one non-zero entry.}
    \label{fig:lessic}
\end{figure}


For the LESS-IC distribution, we show the following subspace embedding guarantee. The structure of the proof is similar to the case of OSNAP, and only the specific expressions change due to the different distribution.

\begin{restatable}[Subspace Embedding Guarantee for LESS-IC]{theorem}{lessicmainthm}
\label{t:less-ic}
    Let $\Pi = (1/\sqrt{pm}) S$ be an $m \times n$ matrix distributed according to the $8 \lceil\log (\frac{d}{\varepsilon \delta})\rceil$-wise independent  LESS-IC distribution with parameter $p$ for some fixed $n\times d$ matrix $U$ satisfying $U^\top U=I$ with given $(\beta_1, \beta_2)$-approximate leverage scores.
Then, there exist positive constants $c_{\ref*{t:less-ic}.1}$ and $c_{\ref*{t:less-ic}.2}$ such that for any $0 < \varepsilon, \delta < 1$, and $d>10$, we have 
\begin{align*}
\Pb \left( 1 - \varepsilon  \leq s_{\min}(\Pi U)   \leq s_{\max}(\Pi U) \leq 1 + \varepsilon \right) \geq 1-\delta
\end{align*}
when $m \ge c_{\ref*{t:less-ic}.1}  \left( \frac{d +  \log^2(d/\delta)+\log(1/\varepsilon)}{\varepsilon^2} + \log^3(d/\delta)/\varepsilon \right) $ and 
$$ c_{\ref*{t:less-ic}.2} \max \left \{  \frac{(\log (d/{\varepsilon\delta}))^{2.5}}{\varepsilon}, (\log (d/{\varepsilon\delta}))^{3} \right \} \le pm \le m.$$ The matrix $\Pi$ has $O(n + \beta pmd)$ many non-zero entries and can be applied to an $n\times d$ matrix~$A$ in $O(\nnz(A) + \beta pmd^2)$ time, where $\beta=\beta_1\beta_2$ is the leverage score approximation factor.
\end{restatable}


\begin{remark}
    When $\delta = d^{-O(1)}$, we recover the optimal dimension $m=\Theta(d/\varepsilon^2)$ while showing that one can apply the LESS-IC embedding in time $O(\nnz(A)) + \tilde O(\beta d^2/\varepsilon)$. In comparison, \cite{chenakkod2024optimal} showed that a corresponding LESS-IE embedding can be applied in $\tilde O(\beta d^2/\varepsilon^6)$ time. Using our techniques, one could improve the runtime of LESS-IE to $\tilde O(\beta d^2/\varepsilon^2)$, but our new LESS-IC construction appears necessary to recover the best dependence on $1/\varepsilon$.
\end{remark}

\subsection{Fast Subspace Embedding (Proof of Theorem \ref{t:fast})}

Here, we briefly outline how our LESS-IC embedding yields a fast subspace embedding construction to recover the time complexity claimed in Theorem \ref{t:fast}. This follows analogously to the construction from Theorem~1.6 of \cite{chenakkod2024optimal}, and our improvement in the dependence on $1/\varepsilon$ compared to their result (from $1/\varepsilon^6$ to $1/\varepsilon$) stems from the improved sparsity of our LESS-IC embedding. 

The key preprocessing step for applying the LESS-IC embedding is approximating the leverage scores of the matrix $A$. Using Lemma 5.1 in \cite{chenakkod2024optimal} (adapted from Lemma 7.2 in~\cite{chepurko2022near}), we can construct coarse approximations of all leverage scores so that $\beta_1=O(n^\gamma)$ and $\beta_2=O(1)$ in time $O(\gamma^{-1}(\nnz(A)+d^2) + d^\omega)$. Applying LESS-IC (Theorem \ref{t:less-ic}) with these leverage scores and parameters $\beta_1,\beta_2$, computing $\Pi A$ takes $O(\nnz(A) + n^{\gamma} d^2\log^{3}(d/\varepsilon\delta)/\varepsilon)$, where $\nnz(A)$ comes from the fact that every column  of $\Pi$ has at least one non-zero, while the second term accounts for the additional $O(\beta d\log^{3}(d/\varepsilon\delta)/\varepsilon)$ non-zeros. 

Thus, if $d\geq n^c$ for, say, $c=0.1$, then we conclude the claim by appropriately scaling $\gamma$ by a constant factor. Now, suppose otherwise. First, note that without loss of generality we can assume that $\gamma<0.1$ (through scaling the time complexity by a constant factor), $\nnz(A)\geq n$ (by removing empty rows) and $\varepsilon\geq \sqrt{d/n}$ (because otherwise $m\geq n$ and we could use $\tilde A= A$). Thus, under our assumption that $d<n^c$, we have $n^{\gamma} d^2/\varepsilon \leq n^{0.5+\gamma+2c}\leq n^{0.8} \ll \nnz(A)$, and the time complexity is dominated by the $O(\gamma^{-1}\nnz(A))$ term.

Finally, we note that Corollary \ref{t:reduction} follows simply by constructing a subspace embedding $\Pi$ via Theorem \ref{t:fast} with respect to matrix $[A\mid b]$, and computing $\tilde A=\Pi A$, $\tilde b = \Pi b$. The proof of the claim is identical to the proof of Theorem 1.8 in \cite{chenakkod2024optimal}. Our improvement comes directly from the faster runtime of our subspace embedding construction.



\subsection{Outline of the Paper}

Section \ref{s:overview} provides a high level overview of the ideas used in the proofs of our main results, Theorem \ref{t:ose-full} and Theorem \ref{t:less-ic}. Section \ref{sec:proofsketch} provides a sketch of the proof of Theorem \ref{t:ose-full}, listing the main technical steps, leaving the full proof with all technical details to Section \ref{sec:osnap-proof}. The proof of Theorem \ref{t:less-ic} follows similarly and is covered in Section \ref{sec:lessproofs}. The subspace embedding guarantee for a sparse matrix with independent entries is proved in Section \ref{sec:oseieproof}. Section \ref{sec:prelim} contains some  basic facts from the existing literature that are used throughout the paper.

\subsection{Notation}
The following notation and terminology will be used in the paper. The notation $[n]$ is used for the set $\{1,2,...,n\}$ and the notation $\operatorname{P}([n])$ denotes the set of all partitions of $[n]$. Also, for two integers $a$ and $b$ with $a \le b$, we use the notation $[a:b]$ for the set $\{k \in \Z:a \le k \le b\}$. For $x \in \R$, we use the notation $\lfloor x \rfloor$ to denote the greatest integer less than or equal to $x$ and $\lceil x \rceil$ to denote the least integer greater than or equal to  $x$. In $\R^n$ (or $\R^m$ or $\R^d$), the $l$th coordinate vector is denoted by $e_l$. All matrices considered in this paper are real valued and the space of $m \times n$ matrices with real valued entries is denoted by $M_{m \times n}(\mathbb{R})$. Also, for a matrix $X \in M_{d \times d}(\mathbb{R})$, the notation $\Tr (X)$ denotes the trace of the matrix $X$, and $\tr (X) = \frac{1}{d} \Tr (X)$ denotes the normalized trace. We write the operator norm of a matrix $X$ as $\norm{X}$, and it is also denoted by $\norm{X}_{op}$ in some places where other norms appear for clarity. The spectrum of a matrix $X$ is denoted by $\spec(X)$.  The standard probability measure is denoted by $\mathbb{P}$, and the symbol $\mathbb{E}$ means taking the expectation with respect to this standard probability measure. To simplify the notation, we follow the convention from \cite{brailovskaya2022universality} and use the notation $\E [X]^{\alpha}$ for $(\E(X))^{\alpha}$, i.e., when a functional is followed by square brackets, it is applied before any other operations. The covariance of two random variables $X$ and $Y$ is denoted by $\cov(X,Y)$. The standard $L_q$ norm of a random variable $\xi$ is denoted by $\norm{\xi}_q$, for $1 \le q \le \infty$. Throughout the paper, the symbols $c_1, c_2, ...$, and $Const, Const', ...$ denote absolute constants. 



 \section{Main Ideas}
\label{s:overview}


We next outline our new techniques which are needed to establish the main results, Theorems \ref{t:ose-full} and \ref{t:less-ic}. Here, for notational convenience, we will refer to the unscaled random matrix $S$, as opposed to the subspace embedding matrix $\Pi=(1/\sqrt{pm})S$ (see Definition \ref{def:osnap}).

Note that due to the equivalent characterization of the OSE property in \eqref{eq:ose-equiv}, all we need to show is that singular values of $SU$ are clustered around $\sqrt{pm}$ at distance $O(\sqrt{pm} \varepsilon)$. In other words, we need to show that the difference between the spectrum of $SU$ and the spectrum of $\sqrt{pm}I_{d}$ is small, of the order $O(\sqrt{pm} \varepsilon)$. 

In all our models, the entries of $S$ are uncorrelated with mean $0$ and variance $p$, and therefore the entries of $SU$ are uncorrelated with uniform variance. If we consider a random matrix $G$ with Gaussian entries which keeps the covariance profile of the entries of $SU$, then this Gaussian random matrix $G$ has independent Gaussian entries with variance $p$. Using classical results about singular values of Gaussian random matrices, it can be shown that the singular values of $G$ are sufficiently clustered around $\sqrt{pm}$ with high probability for $m = \Omega(d/\varepsilon^2)$.  Thus, it suffices to find conditions under which the singular values of $SU$ are sufficiently close to the singular values of $G$. This is the phenomenon of universality whereby random systems show predictable (in this case Gaussian) behavior under certain limits.

\textbf{Failure of black-box universality.}
Recent work by Brailovskaya-van Handel \cite{brailovskaya2022universality} on universality for certain random matrix models developed tools to bound the distance between the spectrum of a random matrix model obtained as a sum of independent random matrices and the spectrum of a Gaussian random matrix with the same covariance profile. Using these tools, \cite{chenakkod2024optimal} achieved optimal embedding dimension $m=O(d/\varepsilon^2)$ for OSEs by using the bound in \cite[Theorem 2.6]{brailovskaya2022universality} to estimate the Hausdorff distance (a concept of distance between two subsets of $\R$; $A,B \subset \R$ are said to be $\varepsilon$-close in Hausdorff distance if $A$ is in the $\varepsilon$-neighborhood of $B$ and $B$ is in the $\varepsilon$ neighborhood of $A$) between the spectra of \begin{align*}
    \sym(SU)=\left[ {\begin{array}{*{20}{c}}
  {}&{{(SU)^T}} \\ 
  SU&{} 
\end{array}} \right] \quad \text{ and } \quad \sym(G)=\left[ {\begin{array}{*{20}{c}}
  {}&{{G^T}} \\ 
  G&{} 
\end{array}} \right].
\end{align*}


This distance is shown to be $(O(\sqrt{pm}))^{2/3}$, which is of order $\sqrt{pm}\varepsilon$ only when $pm$ has $1/\varepsilon^6$ dependence. Thus, \cite{chenakkod2024optimal} did not obtain the conjectured dependency of the sparsity on $\varepsilon$, which requires $pm$ to only have $1/\varepsilon$ dependency. To get better $\varepsilon$ dependency, we would either need a sharper bound on the Hausdorff distance, or have the distance decrease with $\varepsilon$. For example, if the $(O(\sqrt{pm}))^{2/3}$ bound was improved to $(O(\sqrt{pm}))^{1/2}$, we would only need $(\sqrt{pm})^{1/2} \le \sqrt{pm} \varepsilon$ which can be achieved when $pm$ has $1/\varepsilon^4 $ dependence. On the other hand, if the $(O(\sqrt{pm}))^{2/3}$ bound was improved to $(O(\sqrt{pm}))^{2/3} \varepsilon^{1/2}$, we would only need $pm$ to have $1/\varepsilon^3$ dependence. 


\textbf{Key idea: Universality of centered moments.}
One can instead look at a different approach to characterize the clustering of singular values. To show that the singular values of $\Pi U$ are between $1 \pm \varepsilon$, it is enough to show that $\norm{(\Pi U)^T\Pi U - I_d} \le \varepsilon$ or $\norm{(S U)^TSU - pm\cdot I_d} \le pm \varepsilon$ (Note that $S=\sqrt{pm} \Pi$). One way to achieve this bound with high probability is to use the moment method, i.e., to show that (see proof of Theorem \ref{t:ose-full} in Section \ref{sec:proofsketch}):
$$\E\Big[\tr \big((SU)^T(SU) - pm \cdot I_{d}\big)^{2q}\Big]^{\frac{1}{2q}}=O(pm \varepsilon).$$  

In this case, standard calculations on Gaussian random matrices(see Lemma \ref{cor:gaussianmom}) show that $(\E[\tr (G^TG - pmI_{d \times d})^{2q}])^{\frac{1}{2q}} \le cpm\sqrt{\frac{d}{m}}=O(pm \varepsilon)$ when $m = \Omega(d/\varepsilon^2)$ and $G$ has the covariance profile of $SU$. So it is enough to show that 
$$\E\Big[\tr \big((SU)^T(SU) - pmI_{d}\big)^{2q}\Big]^{\frac{1}{2q}}-\E\Big[\tr \big(G^TG - pmI_{d}\big)^{2q}\Big]^{\frac{1}{2q}}=O(pm\varepsilon).$$
where we recall the notation $\E\Big[\tr \big(X\big)^{2q}\Big]^{\frac{1}{2q}}=\left (\E\tr\left ( X \right )^{2q}\right )^{1/(2q)}
$.

Now, \cite[Proposition 9.12]{brailovskaya2022universality} does take a similar approach of comparing $(SU)^T(SU) - pmI_{d}$ and $G^TG - pmI_{d}$, by relying on an interpolation argument, where one defines a mixture $S(t) = \sqrt{t} S+\sqrt{1-t}G$ and controls the change in the moments along the trajectory specified by $t\in[0,1]$.  Unfortunately, using that result gives a larger power of $pm$ in the bound than desired, resulting again in a worse $\varepsilon$ dependence.

One can also, by viewing $(SU)^T(SU) - pmI_{d} =\sum_{i=1}^m (U^Ts_is_i^TU-pI_{d})$, get a random matrix model which is a sum of independent random matrices (this is not true for OSNAP, but some other models of OSEs), and then compare $\E[\tr ((SU)^T(SU) - pm \cdot I_{d})^{2q}]^{\frac{1}{2q}}$ with $\E[\tr (H)^{2q}]^{\frac{1}{2q}}$ where $H$ is the Gaussian model for $(SU)^T(SU) - pmI_{d}$. This is the approach of  \cite[Proposition 9.15]{brailovskaya2022universality}, but it fails in obtaining the optimal embedding dimension $m = d/\varepsilon^2$.

\textbf{Key technique: Decoupling.} To overcome these obstacles, we develop a fresh analysis while still using the ideas of \cite{brailovskaya2022universality}. Our first step is to observe that due to the property of $S$ having a fixed number of non-zero entries in a column for the OSNAP distribution, all quadratic terms in $(SU)^T(SU) - pm\cdot I_d$ are square-free, and this allows us to use the decoupling technique to reduce the problem of controlling the moments of $(SU)^T(SU) - pm\cdot I_d$ to controlling the moments of $(S_1U)^T(S_2U) + (S_2U)^T(S_1U)$ where $S_1$ and $S_2$ are independent copies of $S$ (See proof of Lemma \ref{prop:momestdecoupmatrix} in Section \ref{sec:proofsketch}).

We still have to separate bounding $\E[\tr((S_1U)^T(S_2U)+ (S_2U)^T(S_1U) )^{2q}]^\frac{1}{2q}$ into two parts, bounding $\E[\tr(G_1^TG_2+G_2^TG_1 )^{2q}]^\frac{1}{2q}$ for the Gaussian model, and the difference 
\begin{align*}\E[\tr((S_1U)^T(S_2U)+ (S_2U)^T(S_1U))^{2q}]^\frac{1}{2q}-\E[\tr(G_1^TG_2+G_2^TG_1 )^{2q}]^\frac{1}{2q},\end{align*}
which is called the universality error.

By standard calculations, we have $\E[\tr(G_1^TG_2+G_2^TG_1)^{2q}]^\frac{1}{2q} \le c \sqrt{pm}\sqrt{pd} = O(pm \varepsilon)$ and the main task is still to bound the universality error. The advantage of the decoupling idea is that, informally speaking, since $S_1$ and $S_2$ are independent, we can condition on one of them, e.g., $S_1$. For fixed $S_1$, the random matrix $(S_1U)^T(S_2U)$ (where all randomness comes from $S_2$) can be viewed as a sum of independent random matrices, with the individual summands having moments of smaller order than the previous approach. We can then use an interpolation argument to bound the trace universality error for $q=\log(\frac{d}{\varepsilon \delta})$ as follows:
\begin{equation}\label{eq:decouplepolylogunivererror}
     \begin{aligned}
         \Big|\E\!\big[\tr((S_1U)^T(S_2U)+ (S_2U)^T(S_1U))^{2q}\big]^\frac{1}{2q}-\E\!\big[\tr(G_1^TG_2+G_2^TG_1 )^{2q}\big]^\frac{1}{2q}\Big|\le \polylog(\tfrac{d}{\varepsilon \delta}).\hspace{-3mm}
     \end{aligned}
 \end{equation}
 Notice that there is no $pm$ dependence on the right hand side. So our requirement that this quantity be bounded by $pm\varepsilon$ is satisfied when $pm \ge \polylog(\frac{d}{\varepsilon \delta})/\varepsilon$, achieving the conjectured $1/\varepsilon$ dependence. 


Nevertheless, the conditioning argument cannot be done directly because
\begin{align*}
\hspace{-1mm}\E\!\Big[\tr\big((S_1U)^T(S_2U)+ (S_2U)^T(S_1U)\big)^{2q}\Big]^\frac{1}{2q}
\ne \E_{S_1}\!\!\bigg[\E_{S_2}\!\!\Big[\tr\big((S_1U)^T(S_2U)+ (S_2U)^T(S_1U)\big)^{2q}\Big]^\frac{1}{2q}\bigg].
\end{align*}

\textbf{Key technique: 2D interpolation via chain rule.}
So, instead we develop a new approach which incorporates the conditioning step directly into a two-dimensional interpolation argument, through the use of the chain rule (see Figure \ref{fig:two-parameter-interpolation}). Define
\[
S_1(t_1) = \sqrt{t_1}\,S_1 + \sqrt{1-t_1}\,G_1, 
\quad
S_2(t_2) = \sqrt{t_2}\,S_2 + \sqrt{1-t_2}\,G_2.
\]
We start from  $(G_1, G_2)$ at $(t_1,t_2) = (0,0)$ and move to $(S_1, S_2)$ at $(1,1)$, interpolating between the easier-to-analyze Gaussian matrices $(G_1,G_2)$ and the true random matrices $(S_1,S_2)$ of interest and controlling the changes in their moments (or the error terms) step by step.

\begin{figure}[]\label{fig:2dinterp}
		\centering
		\begin{tikzpicture}[>=stealth, scale=3.6, font=\small]  

\draw[->] (-0.1,0) -- (1.2,0) node[right] {$t_1$};
			\draw[->] (0,-0.1) -- (0,1.2) node[above] {$t_2$};
			
\fill[green!10] (0,0) rectangle (1,1);
			
\draw (0,0) rectangle (1,1);
			
\node[below left]  at (0,0)  {$(G_1,\; G_2)$};
\node[below right] at (1,0)  {$(S_1,\; G_2)$};
\node[above left]  at (0,1)  {$(G_1,\; S_2)$};
\node[above right] at (1,1)  {$(S_1,\; S_2)$};
			
\draw[dashed, thick] (0,0) -- (1,1);
			\node[above left, xshift=6pt, yshift=-6pt] 
			at (0.7,0.8) {\scriptsize diagonal $t_1 = t_2$};
			
\coordinate (smallPoint) at (0.3,0.3);
			\fill (smallPoint) circle (0.02);
			\draw[->] (smallPoint) -- ++(0.09,0) node[right, xshift=-4pt] {\scriptsize $\frac{\partial}{\partial t_1}$};
			\draw[->] (smallPoint) -- ++(0,0.09) node[above, yshift=-4pt] {\scriptsize $\frac{\partial}{\partial t_2}$};
			
		\end{tikzpicture}
		\caption{
			Two-dimensional interpolation in $(t_1,t_2)\in[0,1]^2$, decomposed using the chain rule.}
		\label{fig:two-parameter-interpolation}
	\end{figure}


Defining $f(M_1,M_2)=\tr((M_1U)^T(M_2U)+(M_2U)^T(M_1U))^{2q}$, and applying the chain rule on the diagonal $t_1=t_2=t$, we obtain:
\begin{align*}
\frac{d}{dt} \mathbb{E}\bigl[f\bigl(S_1(t),S_2(t)\bigr)\bigr]
&=\frac{\partial}{\partial t_1}\E\bigl[ f\bigl(S_1(t_1),S_2(t_2)\bigr)\bigr]\Big|_{t_1=t,\,t_2=t}
+\frac{\partial}{\partial t_2}\E\bigl[ f\bigl(S_1(t_1),S_2(t_2)\bigr)\bigr]\Big|_{t_1=t,\,t_2=t}.
\end{align*}
By independence of $S_1$ and $S_2$, we can condition on $S_2$ when we bound the partial derivative $\frac{\partial}{\partial t_1}\E\bigl[f\bigl(S_1(t_1),S_2(t_2)\bigr)\bigr]\big|_{t_1, t_2=t}$, and do similar calculations for the other term. The benefit of doing this is that we can now fine tune the techniques of \cite{brailovskaya2022universality} to get a differential inequality (Lemma \ref{lem:diffineq}) that leads to inequality (\ref{eq:decouplepolylogunivererror}). In doing so, we are able to find the optimal bounds and exponents in the differential inequality.  

\section{Proof Sketch for the Oblivious Subspace Embedding} \label{sec:proofsketch}

We now sketch the proof of our main subspace embedding guarantee, Theorem \ref{t:ose-full} for OSNAP. The full proof can be found in Section \ref{sec:osnap-proof}. The proof of the subspace embedding guarantee for LESS-IC, Theorem \ref{t:less-ic} is similar and can be found in Section \ref{sec:lessproofs}.

\begin{proof}\textcolor{red}{TOPROVE 0}\end{proof}
Finally, we show how the above arguments also imply the OSE moment property (Definition~\ref{d:osemoments}).
\begin{proof}\textcolor{red}{TOPROVE 1}\end{proof}


\subsection{Controlling Trace Moments of the Embedding Error}

We now sketch the proof of Lemma \ref{prop:momestdecoupmatrix}, which obtains the moment bound for $X^TX-I$ used in the previous proof. The full proof can be found in Section~\ref{subsec:osnaptracemom}. 

\begin{proof}\textcolor{red}{TOPROVE 2}\end{proof}

\subsection{Obtaining the differential inequality in Lemma \ref{lem:diffineq}}

We now discuss the proof of the technical part of our argument in the previous proof, which is to control the derivative of the interpolant. The full proof can be found in Section \ref{subsec:diffineq}.

\begin{proof}\textcolor{red}{TOPROVE 3}\end{proof}

 
\section{Preliminaries} \label{sec:prelim}

\subsection{Oblivious Subspace Embeddings}

Here, we prove some important properties of the OSNAP distribution that we shall use later.

\begin{lemma}[Variance and Uncorrelatedness] \label{lem:osnapvaruncor}
Let $p = p_{m,n} \in (0,1]$ and $S=\{s_{ij}\}_{i \in [m], j \in [n]}$ be a $m \times n$ random matrix as in the unscaled OSNAP distribution. Then, $\E(s_{ij})=0$ and $\operatorname{Var}(s_{ij})=p$ for all $i \in [m], j \in [n]$, and $\cov(s_{i_1 j_1},s_{i_2 j_2})=0$ for any $\{i_1,i_2\} \subset [m], \{j_1,j_2\} \subset [n]$ and $ (i_1,j_1)\neq (i_2,j_2) $
\end{lemma}
\begin{proof}\textcolor{red}{TOPROVE 4}\end{proof}





\begin{lemma}[Norm of a Random Row in Interpolated OSNAP]\label{lem:rownormbound}
    Let $S(t):= \sqrt{t}S + \sqrt{1-t}G$, where $S$ is as in the fully independent unscaled OSNAP distribution and $G$ is an $m \times n$ matrix with i.i.d. Gausian entries with variance $p$. Let $U$ be an $n \times d$ matrix such that $U^TU=I$. Let $\mu$ be a random variable uniformly distributed in $J \subset [m]$ and independent of $S$ and $G$. Then, there exists $c_{\ref*{lem:rownormbound}}> 0$ such that for any positive integer $q>0$, we have
    \begin{align*}
        \E_{\mu, S(t)} [ \norm{e_{\mu}^TS(t)U}^{q} ]^\frac{1}{q} \le c_{\ref*{lem:rownormbound}}\sqrt{\max\{pd,q \}} 
    \end{align*}

\end{lemma}
\begin{proof}\textcolor{red}{TOPROVE 5}\end{proof}

\subsection{Basic Facts of the $L_q(S_q^d)$ Space}

To derive the trace inequalities that will be used in the interpolation argument, we need the following tools from \cite{brailovskaya2022universality}. For a $d \times d$ matrix $M$, following \cite{brailovskaya2022universality}, we define the absolute value $|M|=\sqrt{M^*M}$ and normalized trace $\tr(M)=\frac {1}{d}\Tr(M)$. Let $L_q(S_q^d)$ be the normed vector space of $d\times d$ random matrices $M$ with norm
\begin{align*}
	\norm{M}_q = \begin{cases}
	\big(\E[\tr |M|^q]\big)^{\frac{1}{q}} & \text{if }1\le q<\infty\\
	\| \norm{M}_{op} \|_\infty & \text{if }q=\infty
	\end{cases}
\end{align*}
More precisely, the space $L_q(S_q^d)$ consists of random matrices $M$ with $\norm{M}_q$ well defined (which means $\big(\E[\tr |M|^q]\big)<\infty$ for $1\le q<\infty$ and $\| \norm{M}_{op} \|_\infty$ for $q=\infty$).

Next, we state a H\"older inequality for Schatten classes proved in \cite{brailovskaya2022universality}.

\begin{lemma}[Lemma 5.3. in \cite{brailovskaya2022universality}]
\label{lem:holdervanhandel}
Let $1\le 
\beta_1,\ldots,\beta_k\le\infty$ satisfy $\sum_{i=1}^k\frac{1}{\beta_i}=1$. Then
$$
	|\E[\tr Y_1\cdots Y_k]| \le
	\|Y_1\|_{\beta_1}\cdots \|Y_k\|_{\beta_k}
$$
for any $d\times d$ random matrices $Y_1,\ldots,Y_k$.
\end{lemma}

The proof of Lemma \ref{lem:holdervanhandel} (which we do not include here) relies on convexity and interpolation in $L_q(S_q^d)$. More precisely, one can first prove the result for the case when $\beta_1,...,\beta_k$ are extreme exponents and then extend the result to general $\beta_1,...,\beta_k$ by the following convexity result. Later we will also use this method to prove our trace inequalities (Lemma \ref{lem:decouptraceineq}).

\begin{lemma}[Lemma 5.2. in \cite{brailovskaya2022universality}]
\label{lem:calderon}
Let $F:(L_\infty(S_\infty^d))^k\to\mathbb{C}$ be a multilinear functional.
Then the map
$$
	\bigg(\frac{1}{\beta_1},\ldots,\frac{1}{\beta_k}\bigg)
	\mapsto
	\log \sup_{M_1,\ldots,M_k}
	\frac{|F(M_1,\ldots,M_k)|}{\|M_1\|_{\beta_1}\cdots\|M_k\|_{\beta_k}}
$$
is convex on $[0,1]^k$.
\end{lemma}

More generally, we have the following complex interpolation result from \cite{bergh2012interpolation}.
\begin{theorem}[4.4.1 Theorem in \cite{bergh2012interpolation}]\label{thm:compinterp}
    Let $\{(A_{(\nu,0)},A_{(\nu,0)})\}_{\nu=1}^{n}$ and $(B_0,B_1)$ be compatible Banach spaces. Assume that $T: (A_{(1,0)} \cap A_{(1,0)}) \oplus \cdots \oplus (A_{(n,0)} \cap A_{(n,0)}) \to (B_0 \cap B_1)$ is multilinear.
    Also, assume that for any $(a_1,...,a_n) \in (A_{(1,0)} \cap A_{(1,0)}) \oplus \cdots \oplus (A_{(n,0)} \cap A_{(n,0)})$, we have
    \begin{align*}
        \norm{T(a_1,...,a_n)}_{B_0} \le M_0 \prod \limits_{\nu=1}^{n} \norm{a_{\nu}}_{A_{\nu,0}}
    \end{align*}
    and
    \begin{align*}
        \norm{T(a_1,...,a_n)}_{B_1} \le M_1 \prod \limits_{\nu=1}^{n} \norm{a_{\nu}}_{A_{\nu,1}}
    \end{align*}
    Then for any $0 \le \theta \le 1$, the function $T$ may be uniquely extended to a multilinear mapping from $(A_{(1,0)}, A_{(1,0)})_{[\theta]} \oplus \cdots \oplus (A_{(n,0)}, A_{(n,0)})_{[\theta]}$ to $(B_0,B_1)_{[\theta]}$ with norm at most $M_0^{1-\theta}M_1^{\theta}$, where $(A_{(\nu,0)}, A_{(\nu,0)})_{[\theta]}$ is the complex interpolation space of the pair $(A_{(\nu,0)}, A_{(\nu,0)})$ with exponent $\theta$ defined in \cite[p.88]{bergh2012interpolation}.
\end{theorem}

Applying Theorem \ref{thm:compinterp} to $L_q(S_q^d)$ spaces, we have a more general version of Lemma \ref{lem:calderon}.
\begin{corollary}[Interpolation in $L_q(S_q^d)$ Space]\label{cor:spinterpo}
    Let\begin{align*}(\frac{1}{\beta_{1}(0)},...,\frac{1}{\beta_{k}(0)}) \in [0,1]^k \text{ and }(\frac{1}{\beta_{1}(1)},...,\frac{1}{\beta_{k}(1)}) \in [0,1]^k\end{align*} Assume that
    \begin{align*}
        F: (L_{\beta_{1}(0)}(S_{\beta_{1}(0)}) \cap L_{\beta_{1}(1)}(S_{\beta_{1}(1)})) \oplus \cdots \oplus (L_{\beta_{k}(0)}(S_{\beta_{k}(0)}) \cap L_{\beta_{k}(1)}(S_{\beta_{k}(1)})) \to \R
    \end{align*}
    is multilinear with
    \begin{align*}
        F(M_1,...,M_k) \le K \prod \limits_{\nu=1}^{n} \norm{M_{\nu}}_{\beta_{\nu}(0)}
    \end{align*}
    and
    \begin{align*}
        F(M_1,...,M_k) \le K \prod \limits_{\nu=1}^{n} \norm{M_{\nu}}_{\beta_{\nu}(1)}
    \end{align*}
    for all $(M_1,...,M_k) \in (L_{\beta_{1}(0)}(S_{\beta_{1}(0)}) \cap L_{\beta_{1}(1)}(S_{\beta_{1}(1)})) \oplus \cdots \oplus (L_{\beta_{k}(0)}(S_{\beta_{1}(0)}) \cap L_{\beta_{k}(1)}(S_{\beta_{1}(1)}))$.
Define $\beta_{\nu}(\theta)$ for $\nu=1,2,...,k$  and $0 \le \theta \le 1$ such that
\begin{align*}
    \frac{1}{\beta_{1}(\theta)}=(1-\theta)\frac{1}{\beta_{1}(0)}+\theta \frac{1}{\beta_{1}(1)}
\end{align*}
    Then for any $0 \le \theta \le 1$, the multilinear functional $F$ can be uniquely extended to \begin{align*}L_{\beta_{1}(\theta)}(S_{\beta_{1}(\theta)})  \oplus \cdots \oplus L_{\beta_{k}(\theta)}(S_{\beta_{1}(\theta)}) \end{align*} with
    \begin{align*}
        F(M_1,...,M_k) \le K \prod \limits_{\nu=1}^{n} \norm{M_{\nu}}_{\beta_{\nu}(\theta)}
    \end{align*}
    for all $(M_1,...,M_k) \in L_{\beta_{1}(\theta)}(S_{\beta_{1}(\theta)})  \oplus \cdots \oplus L_{\beta_{k}(\theta)}(S_{\beta_{1}(\theta)}) $.
\end{corollary}
\begin{proof}\textcolor{red}{TOPROVE 6}\end{proof}


\subsection{Spectrum of Gaussian Matrices}
Here we collect results about the spectrum of various Gaussian models that will be used later in conjunction with universality results.



   \begin{lemma}[(2.3), \cite{rudelson2010non}]\label{lem:Gaussianspectrum}
    For $m>d$, let $G$ be an $m \times d$ matrix whose entries are independent standard normal variables. Then,
    \begin{align*}
        \Pb(\sqrt{m}-\sqrt{d}-t \leq s_{\min}(G) \leq s_{\max}(G) \leq \sqrt{m}+\sqrt{d}+t) \geq 1 - 2e^{-t^2/2}
    \end{align*}
\end{lemma}

\begin{corollary}[Trace Moment of Embedding Error for Gaussian Model] \label{cor:gaussianmom}
    Let $G$ be an $m \times d$ matrix whose entries are independent normal random variables with variance $\frac{1}{m}$. Let $\varepsilon<\frac{1}{6}$ and $q \in \mathbb{N} \le m \varepsilon^2 $. Then, there exists $c_{\ref*{cor:gaussianmom}} > 1$ such that for $m \geq \frac{c_{\ref*{cor:gaussianmom}}d}{\varepsilon^2}$,
    \[ \E[\tr(G^TG - I_d)^{2q}]^\frac{1}{2q} \leq  \varepsilon \]
    
\end{corollary}

\begin{proof}\textcolor{red}{TOPROVE 7}\end{proof} 



\begin{lemma}[Trace Moment of Embedding Error for Decoupled Gaussian Model]\label{cor:indgaussianmom}
    Let $G_1$ and $G_2$ be independent $m \times d$ random matrices with i.i.d. Gaussian entries. Then for any positive integer $q$, there exists $c_{\ref*{cor:indgaussianmom}}>0$ such that 
    \[ \E \left[ \tr \left( G_1^TG_2 + G_2^TG_1 \right)^{2q} \right]^\frac{1}{2q} \le c_{\ref*{cor:indgaussianmom}}\sqrt{\max\{d, q\}}\sqrt{\max\{m, q\}}\]
    
\end{lemma}

\begin{proof}\textcolor{red}{TOPROVE 8}\end{proof}


 \section{Full proof of Theorem \ref{t:ose-full}}\label{sec:osnap-proof}

This section contains the full proof of Theorem  \ref{t:ose-full} along with the various lemmas that are used along the way.

\begin{itemize}
    \item Section \ref{subsec:osnapfinal} proves the final subspace embedding guarantee using a bound on the trace moments of the embedding error.
    \item Section \ref{subsec:decouposnap} has details about the decoupling step that reduces the problem of controlling moments of $(SU)^TSU - pm\cdot I_d$ to controlling moments of $(S_1U)^TS_2U+(S_2U)^TS_1U$, for independent $S_1$ and $S_2$.
    \item Section \ref{subsec:osnaptracemom} proves the trace moment bound using 2D interpolation of moments of the form $(S_1(t)U)^TS_2(t)U+(S_2(t)U)^TS_1(t)U$.
    \item Section \ref{subsec:diffineq} obtains the differential inequality for the derivative of the interpolant.
    \item Section \ref{subsec:osnaptraceineq} proves the trace inequality required for obtaining the differential inequality.
\end{itemize}

\subsection{Proving the subspace embedding guarantee for OSNAP} \label{subsec:osnapfinal}
In this section we give the full proof of Theorem \ref{t:ose-full}.
\osnapmainthm*
\begin{comment}
\begin{theorem}[High Probability Bounds for the Embedding Error for OSNAP]\label{thm:osedecoup}
Let $S$ be an $m \times n$ matrix distributed according to the $8\lceil\log (d/\varepsilon\delta)\rceil$-wise independent unscaled OSNAP distribution with parameter $p$. Let $U$ be an arbitrary $n \times d$ deterministic matrix such that $U^TU=I$. Then, there exist positive constants $c_{\ref*{thm:osedecoup}.1}>0$ and
$c_{\ref*{thm:osedecoup}.2}>0$ such that for any $0 < \delta, \varepsilon < 1$ and $d>10$,
we have
\begin{equation}\label{osepro}
    \begin{aligned}
\Pb \left( 1 - \varepsilon  \leq s_{\min}((1/\sqrt{pm})SU)   \leq s_{\max}((1/\sqrt{pm})SU) \leq 1 + \varepsilon \right) \geq 1-\delta
\end{aligned}
\end{equation}
when $m \ge c_{\ref*{thm:osedecoup}.1}  \frac{d + \log(1/\delta\varepsilon)}{\varepsilon^2}  $ and
\begin{align*}pm \ge \min \left\{c_{\ref*{thm:osedecoup}.2} \paren*{\frac{(\log (d/\delta\varepsilon))^2}{\varepsilon}+(\log (d/\delta\varepsilon))^3},m \right\}
\end{align*}



\end{theorem}
\end{comment}


    \begin{proof}\textcolor{red}{TOPROVE 9}\end{proof}
\subsection{Proving the decoupling lemma for OSNAP} \label{subsec:decouposnap}
Let $S$ have the fully independent unscaled OSNAP distribution as described in Definition \ref{def:osnap}. Then, recall that,
\begin{align*}
    S &= \sum_{l=1}^n \sum_{\gamma=1}^{pm} \xi_{l,\gamma} e_{\mu_{(l, \gamma)}} e_l ^T \\
    &=: \sum_{l=1}^n \sum_{\gamma=1}^{pm} Z_{l,\gamma} 
\end{align*}
where $\{ \xi_{l,\gamma} \}_{l \in [n], \gamma \in [s]}$ is a collection of  independent Rademacher random variables, $\{ \mu_{l,\gamma} \}_{l \in [n], \gamma \in [s]}$ is a collection of independent random variables such that each $\mu_{l,\gamma}$ is uniformly distributed in $[(m/s)(\gamma-1)+1:(m/s)\gamma]$ and $e_{\mu_{(l, \gamma)}}$ and $e_l$ represent basis vectors in $\R^m$ and $\R^n$ respectively. 

Recalling that our goal is to look at the moments of $(SU)^T(SU) - pm I_d$, we observe that,
\begin{align*}
    U^TS^TSU - pm\cdot I_d &= \left( \sum_{i=1}^m U^Ts_is_i^TU \right) - pm\cdot I_d \\
\end{align*}
where $s_i^T$ denotes the $i\textsuperscript{th}$ row of $S$. Then, letting $s_{ij}$ denote the entries of $S$, 
\begin{align*}
    U^TS^TSU - pm\cdot I_d &= \left( \sum_{i=1}^m U^T\left( \sum_{j=1}^n s_{ij}e_{j} \right) \left( \sum_{j'=1}^n s_{ij'}e_{j'}^T \right)U \right) - pm\cdot I_d \\
    &=  \sum_{i=1}^m \left( \sum_{j=1}^n s_{ij}u_{j} \right) \left( \sum_{j'=1}^n s_{ij'}u_{j'}^T \right)  - pm\cdot I_d \\
\end{align*}
where $u_j^T$ denotes the $j\textsuperscript{th}$ row of $U$. Separating the cases where $j=j'$ and $j \neq j'$, 
\begin{equation}\label{eq:diagoffdiag}
\begin{aligned}
    U^TS^TSU - pm\cdot I_d &=  \sum_{i=1}^m \sum_{j=1}^n s_{ij}^2 u_ju_j^T  - pm\cdot I_d + \sum_{i=1}^m \sum_{\substack{j,j' =1 \\ j \neq j'}}^n s_{ij}s_{ij'} u_ju_{j'}^T \\
    &=  \sum_{j=1}^n \left( \sum_{i=1}^m s_{ij}^2 \right) u_ju_j^T  - pm\cdot I_d + \sum_{i=1}^m \sum_{\substack{j,j' =1 \\ j \neq j'}}^n s_{ij}s_{ij'} u_ju_{j'}^T
\end{aligned}
\end{equation}
\begin{remark}
    In Equation \eqref{eq:diagoffdiag}, we decomposed the the embedding error $U^TS^TSU - pm\cdot I_d$ into two parts, the diagonal term
    \begin{align*}
        \sum_{j=1}^n \left( \sum_{i=1}^m s_{ij}^2 \right) u_ju_j^T  - pm\cdot I_d
    \end{align*}
    and the off-diagonal term
    \begin{align*}
        \sum_{i=1}^m \sum_{\substack{j,j' =1 \\ j \neq j'}}^n s_{ij}s_{ij'} u_ju_{j'}^T
    \end{align*}
    This decomposition is the key to understand why OSNAP model only needs $\tilde O(\frac{1}{\varepsilon})$ nonzero entries per column but the i.i.d. entries model might require $\tilde O(\frac{1}{\varepsilon^2})$ nonzero entries per column. In fact, as we will see very soon, the key difference between the OSNAP model and the i.i.d. entries model is that the diagonal term vanishes in OSNAP model but does not vanish in the i.i.d. entries model.
\end{remark}
By construction, $\sum_{i=1}^m s_{ij}^2 = pm$, so the diagonal term becomes
\begin{align*}
        \sum_{j=1}^n \left( \sum_{i=1}^m s_{ij}^2 \right) u_ju_j^T  - pm\cdot I_d=pm \left( \sum_{j=1}^n u_ju_j^T  - I_d \right)=0
    \end{align*}
and therefore we have
\begin{align*}
    U^TS^TSU - pm\cdot I_d &=  pm \left( \sum_{j=1}^n u_ju_j^T  - I_d \right) + \sum_{i=1}^m \sum_{\substack{j,j' =1 \\ j \neq j'}}^n s_{ij}s_{ij'} u_ju_{j'}^T \\
    &= \sum_{i=1}^m \sum_{\substack{j,j' =1 \\ j \neq j'}}^n s_{ij}s_{ij'} u_ju_{j'}^T
\end{align*}

To analyze the off-diagonal term, we use the standard technique of decoupling, 



\begin{lemma}[Decoupling] \label{lem:decoup}
When $S$ has the fully independent unscaled OSNAP distribution, we have
\begin{align*}
    \E [ \tr (U^TS^TSU - pm\cdot I_d)^{2q} ] &= \E \left[ \tr \left( \sum_{i=1}^m \sum_{j,j' =1, j \neq j'}^n s_{ij}s_{ij'} u_ju_{j'}^T \right)^{2q} \right] \\
\end{align*}
Consequently, we have
\begin{align*}
    \E [ \tr (U^TS^TSU - pm\cdot I_d)^{2q} ] &\le \E_{S,S'} \left[ \tr \left(  2\paren*{(SU)^TS'U + (SU)^TS'U} \right)^{2q} \right]
\end{align*}
where $S'$ is an independent copy of $S$.
\end{lemma}



\begin{proof}\textcolor{red}{TOPROVE 10}\end{proof}

\subsection{Controlling the trace moments of the embedding error for OSNAP} \label{subsec:osnaptracemom}
In this section we give the full proof of Lemma \ref{prop:momestdecoupmatrix}.

\begin{lemma}[Trace Moments of Embedding Error for OSNAP] \label{prop:momestdecoupmatrix}
Let $S$ be an $m \times n$ matrix distributed according to the fully independent unscaled OSNAP distribution with parameter $p \le 1$. Let $U$ be an arbitrary $n \times d$ deterministic matrix such that $U^TU=I$. Define $X = \frac{1}{\sqrt{pm}}SU$.  Then, there exist constants $c_{\ref*{prop:momestdecoupmatrix}.1}, c_{\ref*{prop:momestdecoupmatrix}.2}, c_{\ref*{prop:momestdecoupmatrix}.3} > 0$ such that for $q \in \N$ satisfying $2 \le q \le m$, when $m \geq c_{\ref*{prop:momestdecoupmatrix}.1} \frac{d+q}{\varepsilon^2}$ and $ pm \ge (\max\{\frac{c_{\ref*{prop:momestdecoupmatrix}.2} q^{2}}{\varepsilon}, c_{\ref*{prop:momestdecoupmatrix}.3} q^3\})^{1+\frac{2}{q-2}} $, we have
\begin{align*}
    \E[\tr(X^TX - I_d)^{2q}]^\frac{1}{2q} \leq  \varepsilon
\end{align*} 

\end{lemma}

\begin{proof}\textcolor{red}{TOPROVE 11}\end{proof}

\subsection{Differential inequality for the derivative of the interpolant}\label{subsec:diffineq}
In this section we give the full proof of Lemma \ref{lem:diffineq}.


\begin{lemma}[Differential Inequality]\label{lem:diffineq}
 Let $S_1$ and $S_2$ be independent random matrices such that either both $S_1$ and $S_2$ have the fully independent unscaled OSNAP distribution with parameter $p$ or both $S_1$ and $S_2$ have the unscaled OSE-IE distribution with parameter $p$. Let $G_1$ and $G_2$ be independent random matrices with i.i.d. Guassian entries each with variance $p$, and define the interpolated random matrices,
\begin{align}
\begin{split} 
    S_1(t) = \sqrt{t}S_1 + \sqrt{1-t}G_1 \\
    S_2(t) = \sqrt{t}S_2 + \sqrt{1-t}G_2
\end{split}
\end{align}
Let $f(M_1,M_2)=\tr(((M_1U)^T(M_2U)+(M_2U)^T(M_1U))^{2q})$.
Then, there exists a constant $c_{\ref*{lem:diffineq}}$ such that, for any $q \ge 2$, we have
\begin{align*}
\frac{d}{dt} \E[f(S_1(t),S_2(t))] \le& \max \limits_{4 \le k \le 2q} (c_{\ref*{lem:diffineq}}q)^k ((pm)^{\frac{1}{q}}\sqrt{\max\{pd,q\}})^{\frac{qk-2q}{q-1}}(\E[f(S_1(t),S_2(t))])^{1-\frac{k-2}{2q-2}}
\end{align*}
\end{lemma}
\begin{remark}
The proof of Lemma \ref{lem:diffineq} relies on a technical trace inequality, Lemma \ref{lem:decouptraceineq}. To illustrate the main idea, we present the proof of Lemma \ref{lem:diffineq} using Lemma \ref{lem:decouptraceineq} here first, and then prove Lemma \ref{lem:decouptraceineq} in the next section.
\end{remark}
\begin{proof}\textcolor{red}{TOPROVE 12}\end{proof}

\subsection{Proving the trace inequality needed to obtain the differential inequality for the derivative of the interpolant} \label{subsec:osnaptraceineq}

In this section, we explain the following trace inequality result which is the key to prove the bound (\ref{tracebound}) in the proof of Lemma \ref{lem:diffineq}.

\begin{lemma} [Trace Inequalities for OSNAP]\label{lem:decouptraceineq}
Let $S_1(t)$ and $S_2(t)$ be as in Lemma \ref{lem:diffineq} with both having the fully independent unscaled OSNAP distribution. Let
\begin{align*}\Gamma(t)=(S_1(t)U)^T(S_2(t)U)+(S_2(t)U)^T(S_1(t)U)\end{align*} Let $\mathcal{Z}=\{\xi_{(l,\gamma)}:(l,\gamma) \in [n] \times [pm]\} \cup \{\mu_{(l,\gamma)}:(l,\gamma) \in [n] \times [pm]\}$ be the family of mutually independent random variables generating an instance of $S_1$ with the fully independent  unscaled OSNAP distribution. Let $q \ge 2$ and $3 \le k \le 2q$. Let $\{\mathcal{Z}_{\lambda}\}_{\lambda \in [k]}$ be a family of (possibly dependent) random elements, where for each $\lambda \in [k]$, the random element
\begin{align*}
    \mathcal{Z}_{\lambda}=\{\xi_{(l,\gamma),\lambda}:(l,\gamma) \in [n] \times [pm]\} \cup \{\mu_{(l,\gamma),\lambda}:(l,\gamma) \in [n] \times [pm]\}
\end{align*}
has the same distribution as 
\begin{align*}
    \mathcal{Z}=\{\xi_{(l,\gamma)}:(l,\gamma) \in [n] \times [pm]\} \cup \{\mu_{(l,\gamma)}:(l,\gamma) \in [n] \times [pm]\}
\end{align*}
Let $Z_{(l,\gamma)}=\xi_{(l,\gamma)} e_{\mu_{(l, \gamma)}} e_l ^T$ and $Z_{(l,\gamma),\lambda}=\xi_{(l,\gamma),\lambda} e_{\mu_{(l, \gamma),\lambda}} e_l ^T$.
Let $\{\Upsilon_1,...,\Upsilon_k\}$ be a family of $L_{\infty}(S_{\infty}^d)$ random matrices.
Assume further that the collection $\{\mathcal{Z}_{\lambda}\}_{\lambda \in [k]}$ is independent of $S_1, S_2, G_1, G_2$, and $\{\Upsilon_1,...,\Upsilon_k\}$. (In other words, $\{\Upsilon_1,...,\Upsilon_k\}$ can possibly be dependent with $S_1, S_2, G_1, G_2$.)
For each $(l,\gamma) \in [n] \times [pm]$ and $\lambda \in k$, we define random vectors $\Theta_{(l,\gamma), \lambda, 1}, \Theta_{(l,\gamma), \lambda, 2} \in \R^d$ such that
\begin{align*}
    \Theta_{(l,\gamma), \lambda, 1} = \xi_{(l,\gamma),\lambda} u_l^T \text{ and } \Theta_{(l,\gamma), \lambda, 2} = e_{\mu_{(l,\gamma),\lambda}}^TS_2(t)U
\end{align*}
where $e_{\mu_{(l,\gamma),\lambda}}$ represents the $\mu_{(l,\gamma),\lambda}$th coordinate vector. Then, given $0 \le \beta_1,...,\beta_k \le +\infty$ such that $\sum \limits_{\lambda=1}^k \frac{1}{\beta_{\lambda}}=1-\frac{k}{2q}$, $\tau_1, \ldots, \tau_k \in \sym(\{1,2 \})$, there exists $c_{\ref*{lem:decouptraceineq}}>0$ such that
\begin{equation}\label{eq:traceineq}
\begin{aligned}
    &\sum_{(l,\gamma) \in [n] \times [pm]} \E[ \tr \Theta_{(l,\gamma), 1, \tau_1(1)}^T\Theta_{(l,\gamma), 1, \tau_1(2)}
	\Upsilon_1\Theta_{(l,\gamma), 2, \tau_2(1)}^T\Theta_{(l,\gamma), 2, \tau_2(2)} \\&\cdots
	\Upsilon_2\Theta_{(l,\gamma), k, \tau_k(1)}^T\Theta_{(l,\gamma), k, \tau_k(2)}\Upsilon_k]  \\ \le &
       (c_{\ref*{lem:decouptraceineq}}(pm)^{\frac{1}{q}}\sqrt{\max\{pd,q\}})^{\frac{2qk-4q}{2q-2}} (\E \tr((\Gamma(t))^{2q}))^{\frac{1}{q} \cdot \frac{2q-k}{2q-2}} \prod \limits_{\lambda=1}^k \norm{\Upsilon_{\lambda}}_{\beta_{\lambda}}
\end{aligned}
\end{equation}
\end{lemma}
\begin{remark}
The main idea of this lemma is simple. We first use matrix Holder inequality to transform the left hand side into smaller factors, and then we bound those small factors separately. Following this idea, there could be different variants of this lemma. However, not all of them will eventually lead to the optimal dependency of the sparsity on $\varepsilon$. We will explain the idea on how to choose the correct bound. As explained in Proposition \ref{prop:momestdecoupmatrix}, the sparsity requirement comes from the condition
\begin{align*}
    (\E[\tr \Gamma(1)^{2q}])^{\frac{1}{2q}}-(\E[\tr \Gamma(0)^{2q}])^{\frac{1}{2q}} \le \frac{1}{4} pm \varepsilon
\end{align*}
If we want this condition to be implied by a requirement of the type
\begin{align*}
    pm>\frac{C (\log(d))^{\text{(some power)}}}{\varepsilon}
\end{align*}
then a natural attempt would be to try to bound $(\E[\tr \Gamma(1)^{2q}])^{\frac{1}{2q}}-(\E[\tr \Gamma(0)^{2q}])^{\frac{1}{2q}}$ by just a constant times some power of $\log(d)$.

To bound $(\E[\tr \Gamma(1)^{2q}])^{\frac{1}{2q}}-(\E[\tr \Gamma(0)^{2q}])^{\frac{1}{2q}}$, we combine our differential inequality with Lemma 6.6 in \cite{brailovskaya2022universality}.

By Lemma 6.6 in \cite{brailovskaya2022universality}, we can get the bound of the type
\begin{align*}
    (\E[\tr \Gamma(1)^{2q}])^{\alpha}-(\E[\tr \Gamma(0)^{2q}])^{\alpha} \le C\alpha +K^{1-\alpha}
\end{align*}
if we have a differential inequality of the form
\begin{align*}
    |\frac{d}{dt}(\E[\tr \Gamma(t)^{2q}])| \le C \max\{(\E[\tr \Gamma(t)^{2q}])^{1-\alpha},K^{1-\alpha}\}
\end{align*}
So we mainly want to obtain a differential inequality where $C$ and $K$ only contain factors of $\log(d)$ but do not contain any positive powers of $pm$ or any negative power of $\varepsilon \approx \sqrt{d/m}$. To this end, we need to choose a variant of the bound for the left hand side of (\ref{eq:traceineq}) which does not contain any positive powers of $pm$ or any negative power of $\varepsilon \approx \sqrt{d/m}$.

Therefore, when choosing the different variants of the bound in Lemma \ref{lem:decouptraceineq}, we seek to remove those factors that we do not want. One natural attempt would be to use the method similar to the proof of the second inequality in Theorem 2.9 in \cite{brailovskaya2022universality}, where they first bound the small factors that arise after using Holder by some matrix parameters, and then replace those matrix parameters by the trace moments by Jensen inequality. In our case, this means to replace $\sqrt{pmpd}$ by $(\E[\tr \Gamma(t)^{2q}])^{\frac{1}{2q}}$, thereby removing the factors $pm$ and $pd$.

However, in our case, after bounding the small factors obtained by directly separating the left hand side of (\ref{eq:traceineq}) using Holder, we can only obtain factors of the form $\sqrt{\max\{pd,q\}}$ and $\sqrt{\max\{pm,q\}}$. (Recall that $q \sim \log(d/\varepsilon\delta)$). The product $\sqrt{\max\{pd,q\} \cdot \max\{pm,q\}}$ of these two factors can be interchanged by the factor $\sqrt{pmpd}$ only when $pd \ge \log(d/\varepsilon\delta)$. But this already means that $pm \ge O(\frac{\log(d/\varepsilon\delta)}{\varepsilon^2})$, since $\varepsilon=O(\sqrt{\frac{d}{m}})$.

To get rid of the unbalanced factor $\sqrt{\max\{pd,q\} \cdot \max\{pm,q\}}$, we develop a completely different method. We combine several factors at early stage and then use a key observation, Lemma \ref{zssz}, to replace this combined factor by $(\E[\tr \Gamma(t)^{2q}])^{\frac{1}{2q}}$ directly. Eventually this method allowed us to get a more precise upper estimation, namely the right hand side of (\ref{eq:traceineq}). In this upper estimation, although there are still some factors of $\max\{pd,q\}$, they are not harmful, because $\max\{pd,q\}$ is of smaller order than $\sqrt{pdpm}$ (see the proof of Proposition \ref{prop:momestdecoupmatrix} for details). 




\end{remark}
\begin{proof}\textcolor{red}{TOPROVE 13}\end{proof}









 \section{Leverage Score Sparsified Embeddings} \label{sec:lessproofs}

In this section, we prove our subspace embedding guarantee for the LESS-IC distribution, Theorem \ref{t:less-ic}. The proof is similar to the OSNAP case, and is accomplished via the following results,
\begin{itemize}
    \item Theorem \ref{t:less-ic} establishes the subspace embedding guarantee from a bound on the trace moments of the embedding error analogous to Theorem \ref{t:ose-full} in the OSNAP case.
    \item Lemma \ref{lem:decoupless} shows that it is sufficient to bound the moments of $(S_1U)^TS_2U + (S_2U)^TS_1U$ to control the trace moments of the embedding error, analogous to Lemma \ref{lem:decoup}. This is the decoupling step.
    \item Lemma \ref{lem:lessvar} establishes that the entries of the LESS-IC distribution are uncorrelated and have variance $p$, which means the Gaussian model that we interpolate with should have independent entries with variance $p$, just as in the case of OSNAP.
    \item Lemma \ref{prop:momestdecoupless} bounds the trace moments of the embedding error via interpolation after decoupling, analogous to Lemma \ref{prop:momestdecoupmatrix}.
    \item Lemma \ref{lem:diffineqless} establishes the differential inequality for the derivative of the interpolant, analogous to Lemma \ref{lem:diffineq} in the OSNAP and OSE-IE case.
    \item Lemma \ref{lem:traceineqless} establishes the trace inequality required to obtain the differential inequality in Lemma \ref{lem:diffineqless}, analogous to Lemma \ref{lem:decouptraceineq} in the case of OSNAP.
    \item The trace inequality in Lemma \ref{lem:traceineqless} in turn requires a bound for the row norm moments of $S(t)U$. This is provided by Lemma \ref{lem:lessrownormbound}, analogous to Lemma \ref{lem:rownormbound}.
\end{itemize}

Before we proceed, we state the formal definition of the LESS-IC distribution. The construction is similar to OSNAP, with some changes to reflect the different number and size of subcolumns and the different scaling for non-zero entries across columns.

\begin{definition}[LESS-IC]\label{def:lessindcol}
    Given $(\beta_1,\beta_2)$ leverage scores $z_1,...,z_n$, and $0 < p < 1 $, define
    \begin{align*}
        b_j &:= 
        \max \Big\{ \left\lfloor \frac{1}{\beta_1pz_j} \right\rfloor, 1 \Big\}\quad\text{and}\quad
        s_j := \left\lceil \frac{m}{b_j} \right\rceil.
    \end{align*} 
    An $m \times n$ random matrix $S$ is called a $K$-wise independent unscaled leverage score sparsified embedding with independent columns ($K$-wise independent unscaled LESS-IC), and also $\Pi = (1/\sqrt{pm})S$ is called a $K$-wise independent LESS-IC, corresponding to  $(\beta_1, \beta_2)$-approximate leverage scores  $(z_1,...,z_n)$ with parameter $p$ if it is distributed as
    \begin{align*}
    S &= \sum_{l=1}^n \sum_{\gamma_l=1}^{s_l} \alpha_{(l, \gamma_l)} \xi_{(l,\gamma_l)} e_{\mu_{(l, \gamma_l)}} e_l ^\top 
\end{align*}
where in this expression
\begin{itemize}
    \item the collections $\{ \xi_{(l,\gamma_l)} \}_{l \in [n], \gamma_l \in [s_l]}$ and $\{ \mu_{(l,\gamma_l)} \}_{l \in [n], \gamma_l \in [s_l]}$ are mutually independent;
    \item $\{ \xi_{l,\gamma_l} \}_{l \in [n], \gamma_l \in [s_l]}$ is a collection of $K$-wise independent Rademacher random variables;
    \item $\{ \mu_{(l,\gamma_l)} \}_{l \in [n], \gamma_l \in [s_l]}$ is a collection of $K$-wise independent random variables such that each $\mu_{(l,\gamma_l)}$ is uniformly distributed in $[b_l(\gamma_l-1)+1:\min \{ b_l\gamma_l, m \}]$;
    \item $\alpha_{(l, \gamma_l)} := \sqrt{p(\min \{ b_l\gamma_l, m \} - b_l(\gamma_l-1))}$;
    \item $e_{\mu_{(l, \gamma)}}$ and $e_l$ represent basis vectors in $\R^m$ and $\R^n$ respectively.
\end{itemize}
In addition, if all the random variables in the collections $\{ \xi_{(l,\gamma_{l})} \}_{l \in [n], \gamma \in [s]}$ and $\{ \mu_{(l,\gamma_l)} \}_{l \in [n], \gamma \in [s]}$ are fully independent, then $S$ is called a fully independent unscaled LESS-IC and $\Pi$ is called a fully independent LESS-IC.
\end{definition}


\lessicmainthm*

\begin{remark}
    We can obtain a subspace embedding guarantee for the LESS-IE distribution by following the proof of Theorem \ref{thm:oseiemain} suitably modified for the case of LESS. One can check that Theorem \ref{prop:momestdiag} holds even when $S$ has the LESS-IE distribution with the same values of $\sigma$ and $R$. Thus, a subspace embedding for the LESS-IE distribution holds under the same conditions as Theorem \ref{t:less-ic} with the additional requirement $pm \ge \frac{c \log (\frac{d}{\varepsilon \delta})}{\varepsilon^2}$.  
\end{remark}


\begin{comment}
\begin{theorem}[High Probability Bounds for the Embedding Error of LESS-IC]\label{thm:lessdecoup}
Let $S$ be an $m \times n$ matrix distributed according to the $8 \lceil\log (\frac{d}{\varepsilon \delta})\rceil$-wise independent unscaled LESS-IC distribution with parameter $p$ for some fixed matrix $U$ satisfying $U^TU=I$ with given $(\beta_1, \beta_2)$-approximate leverage scores. Then, there exist constants $c_{\ref*{thm:lessdecoup}.1}$ and $c_{\ref*{thm:lessdecoup}.2}$ such that for any $0 < \varepsilon, \delta < 1$ and $d>10$, we have 
\begin{align*}
\Pb \left( 1 - \varepsilon  \leq s_{\min}((1/\sqrt{pm})SU)   \leq s_{\max}((1/\sqrt{pm})SU) \leq 1 + \varepsilon \right) \geq 1-\delta
\end{align*}
when $m \ge c_{\ref*{thm:lessdecoup}.1}  \max \left\{ \frac{d + \log(d/\delta)^2 + \log(1/\varepsilon)}{\varepsilon^2} , \frac{\log(d/\delta)^3}{\varepsilon} \right\}$ and \begin{align*} pm \ge  \min \left \{  c_{\ref*{thm:lessdecoup}.2}\paren*{\frac{(\log ({d/\delta\varepsilon}))^{2.5}}{\varepsilon} + (\log ({d/\delta\varepsilon}))^{3}  }, m  \right \} \end{align*} The matrix $S$ has $O(\beta pmd + n)$ many non zero entries and can be applied to an $n\times d$ matrix $A$ in $O(\nnz(A) + \beta pmd^2)$ time.
\end{theorem}
\end{comment}

\begin{proof}\textcolor{red}{TOPROVE 14}\end{proof}

\begin{lemma}[Decoupling] \label{lem:decoupless}
When $S$ has the fully independent unscaled LESS-IC distribution,
\begin{align*}
    \E [ \tr (U^TS^TSU - pm\cdot I_d)^{2q} ] &= \E \left[ \tr \left( \sum_{i=1}^m \sum_{j,j' =1, j \neq j'}^n s_{ij}s_{ij'} u_ju_{j'}^T \right)^{2q} \right] \\
\end{align*}
Consequently,
\begin{align*}
    \E [ \tr (U^TS^TSU - pm\cdot I_d)^{2q} ] &\le \E_{S,S'} \left[ \tr \left(  2\paren*{(SU)^TS'U + (SU)^TS'U} \right)^{2q} \right]
\end{align*}
where $S'$ is an independent copy of $S$.
\end{lemma}
\begin{proof}\textcolor{red}{TOPROVE 15}\end{proof}

\begin{lemma}[Variance and Uncorrelatedness] \label{lem:lessvar}
Let $p = p_{m,n} \in (0,1]$ and $S=\{s_{ij}\}_{i \in [m], j \in [n]}$ be a $m \times n$ random matrix distributed according to the fully independent unscaled LESS-IC distributions. Then, $\E(s_{ij})=0$ and $\operatorname{Var}(s_{ij})=p$ for all $i \in [m], j \in [n]$, and $\cov(s_{i_1 j_1},s_{i_2 j_2})=0$ for any $\{i_1,i_2\} \subset [m], \{j_1,j_2\} \subset [n]$ and $ (i_1,j_1)\neq (i_2,j_2) $
\end{lemma}
\begin{proof}\textcolor{red}{TOPROVE 16}\end{proof}

 Using the decoupling result, we bound the trace moments of the embedding error by interpolating between LESS and its Gaussian model exactly as in the proof of Lemma \ref{prop:momestdecoupmatrix} in Section \ref{subsec:osnaptracemom}.

\begin{lemma} [Trace Moments of Embedding Error for LESS]\label{prop:momestdecoupless}
Let $S$ be an $m \times n$ matrix distributed according to the fully independent unscaled LESS-IC distribution with parameter $p$ for some fixed matrix $U$ satisfying $U^TU=I$ with given $(\beta_1, \beta_2)$-approximate leverage scores. Define $X = \frac{1}{\sqrt{pm}}SU$. Given $0< \varepsilon < 1$, there exist constants $c_{\ref*{prop:momestdecoupless}.1}, c_{\ref*{prop:momestdecoupless}.2}, c_{\ref*{prop:momestdecoupless}.3}$ such that for $m \geq c_{\ref*{prop:momestdecoupless}.1} \frac{d+q}{\varepsilon^2}$ and $q \in \N$ satisfying $2 \le q \le m$ and $ pm \ge \paren*{ \max \left\{ \frac{c_{\ref*{prop:momestdecoupless}.2} q^{5/2}}{\varepsilon}, c_{\ref*{prop:momestdecoupless}.3}  q^3 \right\}}^{1+\frac{2}{q-2}} $ ,
\begin{align*} \E[\tr(X^TX - I_d)^{2q}]^\frac{1}{2q} \leq  \varepsilon \end{align*}
\end{lemma}

\begin{proof}\textcolor{red}{TOPROVE 17}\end{proof}






Here we obtain the differential inequality that arises during interpolation in the proof of Lemma \ref{prop:momestdecoupless}. 
\begin{lemma}[Differential Inequality for LESS]\label{lem:diffineqless}
Let $p>0$. Let $S_1$ and $S_2$ be independent random matrices with the fully independent unscaled LESS-IC distribution with parameter $p$ for some fixed matrix $U$  satisfying $U^TU=I$ with given $(\beta_1, \beta_2)$-approximate leverage scores. Let $G_1$ and $G_2$ be independent random matrices with i.i.d. Guassian entries each with variance $p$, and define the interpolated random matrices,
\begin{align}
\begin{split} 
    S_1(t) = \sqrt{t}S_1 + \sqrt{1-t}G_1 \\
    S_2(t) = \sqrt{t}S_2 + \sqrt{1-t}G_2
\end{split}
\end{align}
Let $f(M_1,M_2)=\tr(((M_1U)^T(M_2U)+(M_2U)^T(M_1U))^{2q})$.
Then there exists $c_{\ref*{lem:diffineqless}} >0$ such that, for any $q\ge 2$,
\begin{align*}
    \frac{d}{dt} \E[f(S_1(t),S_2(t))] \le&  \max \limits_{4 \le k \le 2q} (c_{\ref*{lem:diffineqless}}q)^k ((pm)^\frac{1}{q}\sqrt{\max\{ pd,q^2\}})^{\frac{2qk-4q}{2q-2}}(\E[f(S_1(t),S_2(t))])^{1-\frac{k-2}{2q-2}}
\end{align*}
\end{lemma}


\begin{proof}\textcolor{red}{TOPROVE 18}\end{proof}

As in the oblivious case, a trace inequality is the key step in the proof of Lemma \ref{lem:diffineqless}.
\begin{lemma} [Trace Inequalities for LESS]\label{lem:traceineqless}
Let $S_1(t)$ and $S_2(t)$ be as in Lemma \ref{lem:diffineqless}. Let $\Gamma(t)=(S_1(t)U)^T(S_2(t)U)+(S_2(t)U)^T(S_1(t)U)$. Let $\mathcal{Z}=\{\xi_{(l,\gamma)}: l \in [n], \gamma_l \in [s_l] \} \cup \{\mu_{(l,\gamma)}:l \in [n], \gamma_l \in [s_l] \}$ be a family of mutually independent random variables be the family of mutually independent random variables generating an instance of $S_1$ with the unscaled LESS-IC distribution corresponding to some $(\beta_1, \beta_2)$-approximate leverage scores for $U$. Let $q \ge 2$ and $3 \le k \le 2q$. Let $\{\mathcal{Z}_{\lambda}\}_{\lambda \in [k]}$ be a family of (possibly dependent) random elements, where for each $\lambda \in [k]$, the random element
\begin{align*}
    \mathcal{Z}_{\lambda}=\{\xi_{(l,\gamma),\lambda}:l \in [n], \gamma_l \in [s_l]\} \cup \{\mu_{(l,\gamma),\lambda}:l \in [n], \gamma_l \in [s_l]\}
\end{align*}
has the same distribution as 
\begin{align*}
    \mathcal{Z}=\{\xi_{(l,\gamma)}:l \in [n], \gamma_l \in [s_l]\} \cup \{\mu_{(l,\gamma)}:l \in [n], \gamma_l \in [s_l]\}
\end{align*}
Let $Z_{(l,\gamma)}=\xi_{(l,\gamma)} e_{\mu_{(l, \gamma)}} e_l ^T$ and $Z_{(l,\gamma),\lambda}=\xi_{(l,\gamma),\lambda} e_{\mu_{(l, \gamma),\lambda}} e_l ^T$.
Let $\{\Upsilon_1,...,\Upsilon_k\}$ be a family of $L_{\infty}(S_{\infty}^d)$ random matrices.
Assume further that the collection $\{\mathcal{Z}_{\lambda}\}_{\lambda \in [k]}$ is independent of $S_1, S_2, G_1, G_2$, and $\{\Upsilon_1,...,\Upsilon_k\}$. (In other words, $\{\Upsilon_1,...,\Upsilon_k\}$ can possibly be dependent with $S_1, S_2, G_1, G_2$.)
For each $l \in [n], \gamma_l \in [s_l]$ and $\lambda \in k$, we define random vectors $\Theta_{(l,\gamma), \lambda, 1}, \Theta_{(l,\gamma), \lambda, 2} \in \R^d$ such that
\begin{align*}
    \Theta_{(l,\gamma), \lambda, 1} = \xi_{(l,\gamma),\lambda} \alpha_{(l,\gamma),\lambda} u_l^T \text{ and } \Theta_{(l,\gamma), \lambda, 2} = e_{\mu_{(l,\gamma),\lambda}}^TS_2(t)U
\end{align*}
where $e_{\mu_{(l,\gamma),\lambda}}$ represents the $\mu_{(l,\gamma),\lambda}$th coordinate vector. Then, given $0 \le \rho_1,...,\rho_k \le +\infty$ such that $\sum \limits_{\lambda=1}^k \frac{1}{\rho_{\lambda}}=1-\frac{k}{2q}$, $\tau_1, \ldots, \tau_k \in \sym(\{1,2 \})$, there exists $c_{\ref*{lem:traceineqless}} >0$ such that
\begin{align*}
    &\sum_{l \in [n], \gamma_l \in [s_l]} \E[ \tr \Theta_{(l,\gamma), 1, \tau_1(1)}^T\Theta_{(l,\gamma), 1, \tau_1(2)}
	\Upsilon_1\Theta_{(l,\gamma), 2, \tau_2(1)}^T\Theta_{(l,\gamma), 2, \tau_2(2)} \\&\cdots
	\Upsilon_2\Theta_{(l,\gamma), k, \tau_k(1)}^T\Theta_{(l,\gamma), k, \tau_k(2)}\Upsilon_k]  \\ \le &
     (c_{\ref*{lem:traceineqless}}(pm)^\frac{1}{q}\sqrt{\max\{\beta pd,q^2\}})^{\frac{2qk-4q}{2q-2}} (\E \tr((\Gamma(t))^{2q}))^{\frac{1}{q} \cdot \frac{2q-k}{2q-2}} \prod\limits_{\lambda  = 1}^{k} {\norm{\Upsilon_\lambda }_{\rho_{\lambda}}} 
\end{align*}


\end{lemma}
\begin{proof}\textcolor{red}{TOPROVE 19}\end{proof}


\begin{lemma}[Row Norm for LESS-IC]\label{lem:lessrownormbound}
    Let $S(t):= \sqrt{t}S + \sqrt{1-t}G$, where $S$ has the fully independent unscaled LESS-IC distribution as in Lemma \ref{lem:diffineqless} and $G$ is an $m \times n$ matrix with i.i.d. Gausian entries with variance $p$. Let $\mu$ be a random variable uniformly distributed in $\phi \neq I \subset [m]$ and independent of $S$ and $G$. Then, there exists $c_{\ref*{lem:lessrownormbound}} > 0$, such that 
    \begin{align*}
        \E_{\mu, S(t)} [ \norm{e_{\mu}^TS(t)U}^{q} ]^\frac{1}{q} &\le c_{\ref*{lem:lessrownormbound}}\sqrt{\max \{ pd, q^2 \}} 
    \end{align*}
\end{lemma}
\begin{proof}\textcolor{red}{TOPROVE 20}\end{proof} \section{Oblivious Subspace Embedding with Independent Entries} \label{sec:oseieproof}

In this section, we consider the subspace embedding property for the following classical model with independent entries in the matrix $S$.

\begin{definition}[OSE-IE]\label{def:oseie}
An $m \times n$ random matrix $S$ is called an unscaled oblivious subspace embedding with independent entries (unscaled OSE-IE) with parameter $p$ if $S$ has i.i.d. entries $s_{i,j}=\delta_{(i,j)} \xi_{(i,j)}$ where $\delta_{(i,j)}$ are i.i.d. Bernoulli random variables taking value 1 with probability $p \in (0,1]$ and $\xi_{(i,j)}$ are i.i.d. random variables independent with $\delta_{(i,j)}$ and satisfy $\Pb(\xi_{(i,j)}=1)=\Pb(\xi_{(i,j)}=-1)=1/2$. And in this case, $\Pi = (1/\sqrt{pm})S$ is call an OSE-IE with parameter $p$.
\end{definition}

For this model, we have the following subspace embedding guarantee,

\begin{restatable}[High Probability Bounds for the Embedding Error for OSE-IE]{theorem}{oseiemainthm} \label{thm:oseiemain}
   Let $S$ be an $m \times n$ matrix distributed according to the unscaled OSE-IE distribution with parameter $p$. Let $U$ be an arbitrary $n \times d$ deterministic matrix such that $U^TU=I$. Then, there exist constants $c_{\ref*{thm:oseiemain}.1}>0$ and $c_{\ref*{thm:oseiemain}.2}>0$ such that for any $0 < \varepsilon , \delta < 1$ and $d>10$,
we have
\begin{equation}
    \begin{aligned}
\Pb \left( 1 - \varepsilon  \leq s_{\min}((1/\sqrt{pm})SU)   \leq s_{\max}((1/\sqrt{pm})SU) \leq 1 + \varepsilon \right) \geq 1-\delta
\end{aligned}
\end{equation}
when $m > c_{\ref*{thm:oseiemain}.1}  \frac{d + \log(1/\varepsilon\delta)}{\varepsilon^2} $  and $$pm \geq \min \left\{ c_{\ref*{thm:oseiemain}.2}\paren*{\frac{(\log (d/\varepsilon\delta))^2}{\varepsilon}+\frac{(\log (d/\varepsilon\delta))}{\varepsilon^2} + (\log (d/\varepsilon\delta))^3 } ,m \right\}$$
\end{restatable}


The overall structure of the proof of Theorem \ref{thm:oseiemain} is the same as the proof in the OSNAP case and we only highlight the differences from the proof of the OSNAP case discussed in Section \ref{sec:osnap-proof}. A key difference between the above result and the corresponding result for OSNAP is the $1/\varepsilon^2$ dependence in the lower bound for sparsity. This arises due to our approach of decomposing $U^TS^TSU - pm\cdot I_d$ as
\[ U^TS^TSU - pm\cdot I_d  =  \sum_{j=1}^n \left( \sum_{i=1}^m s_{ij}^2 \right) u_ju_j^T  - pm\cdot I_d + \sum_{i=1}^m \sum_{\substack{j,j' =1 \\ j \neq j'}}^n s_{ij}s_{ij'} u_ju_{j'}^T  \]
where we label the former term as the diagonal term and the latter as the off diagonal term. In the OSNAP model, we have $\sum_{i=1}^m s_{ij}^2 = pm$ by construction and therefore the diagonal term vanishes, but when $S$ has the OSE-IE distribution, we need not necessarily have $\sum_{i=1}^m s_{ij}^2 = pm$, which means we need to analyze diagonal term in addition to the off-diagonal term analyzed in Lemma \ref{lem:decoup}. Observing that, 
\[ \sum_{j=1}^n \left( \sum_{i=1}^m s_{ij}^2 \right) u_ju_j^T  - pm\cdot I_d = \sum_{j=1}^n \left( \sum_{i=1}^m \paren*{s_{ij}^2 - p} \right) u_ju_j^T \]
and using Minkowski's inequality,
\begin{align*}
    \E [ \tr (U^TS^TSU - pm\cdot I_d)^{2q} ]^\frac{1}{2q} &\le \E \left[ \tr \left( \sum_{i=1}^m \sum_{j =1}^n (s_{ij}^2 - p)u_ju_{j}^T \right)^{2q} \right]^\frac{1}{2q} \\
    & \qquad + \E \left[ \tr \left( \sum_{i=1}^m \sum_{j,j' =1, j \neq j'}^n s_{ij}s_{ij'} u_ju_{j'}^T \right)^{2q} \right]^\frac{1}{2q} \\
\end{align*}
Proposition \ref{prop:momestdiag} controls the diagonal term (the former term), which gives rise to the $1/\varepsilon^2$ dependence, and is analyzed in Section \ref{subsec:oseiediag}. The analysis of the off-diagonal term (the latter term) is similar to the OSNAP case and is discussed in Section \ref{subsec:oseiemain}.

\subsection{Controlling the diagonal term when $S$ is the OSE-IE distribution} \label{subsec:oseiediag}

\begin{proposition}[Diagonal Term] \label{prop:momestdiag}
    Let $S$ be an $m \times n$ matrix distributed according to the unscaled OSE-IE distribution with parameter $p$. Let $U$ be an arbitrary $n \times d$ deterministic matrix such that $U^TU=I$. Given $0< \varepsilon < 1$ and $q \ge \log(d) $, there exist constants $c_{\ref*{prop:momestdiag}}$ such that for $m \geq d \ge 20$ and $pm \geq c_{\ref*{prop:momestdiag}} \frac{q }{\varepsilon^2} $,

\[ \E \left[ \tr \left( \frac{1}{pm }\sum_{i=1}^m \sum_{j =1}^n (s_{ij}^2 - p)u_ju_{j}^T \right)^{2q} \right]^\frac{1}{2q} \leq  \varepsilon \]

\end{proposition}
Proposition \ref{prop:momestdiag} shows that, to make the diagonal term small, it suffices to require the nonzero entries per column to be greater than or equal to $c\frac{\log(d)}{\varepsilon^2}$. Note that
    \[ \E \left[ \tr \left( \frac{1}{pm }\sum_{i=1}^m \sum_{j =1}^n (s_{ij}^2 - p)u_ju_{j}^T \right)^{2q} \right]^\frac{1}{2q} \ge  \E \left[ \tr \left( \frac{1}{pm }\sum_{i=1}^m \sum_{j =1}^n (s_{ij}^2 - p)u_ju_{j}^T \right)^{2} \right]^\frac{1}{2} \]
    and the latter can be of the order $1/\sqrt{pm}$ when $U$ corresponds to a $d$-dimensional coordinate subspace of $\R^n$. Thus, the $1/\varepsilon^2$ dependence in the lower bound on $pm$ is necessary when using this approach to prove the subspace embedding guarantee.

\begin{proof}\textcolor{red}{TOPROVE 21}\end{proof}




\subsection{Proving the subspace embedding guarantee for the OSE-IE distribution} \label{subsec:oseiemain}

In this section, we prove \ref{thm:oseiemain}.

\oseiemainthm*

\begin{proof}\textcolor{red}{TOPROVE 22}\end{proof}





\subsection{Trace Inequality in the OSE-IE Case}
The trace inequality required to obtain the differential inequality for the interpolant between the moments of $(S_1U)^TS_2U$ and $(G_1U)^TG_2U$ has a slightly different proof in the OSE-IE case than in the OSNAP case, even though both bounds are the same.

\begin{lemma} \label{lem:decouptraceineqose}
    Let $S_1(t)$ and $S_2(t)$ be as in Lemma \ref{lem:diffineq} with both having the OSE-IE distribution. Let $\Gamma(t)=(S_1(t)U)^T(S_2(t)U)+(S_2(t)U)^T(S_1(t)U)$. Let $\mathcal{Z}=\{\xi_{(l,\gamma)}:(l,\gamma) \in [n] \times [m]\} \cup \{\delta_{(l,\gamma)}:(l,\gamma) \in [n] \times [m]\}$ be the family of mutually independent random variables generating an instance of $S_1$ with the OSE-IE distribution. Let $q \ge 2$ and $3 \le k \le 2q$. Let $\{\mathcal{Z}_{\lambda}\}_{\lambda \in [k]}$ be a family of (possibly dependent) random elements, where for each $\lambda \in [k]$, the random element
\begin{align*}
    \mathcal{Z}_{\lambda}=\{\xi_{(l,\gamma),\lambda}:(l,\gamma) \in [n] \times [m]\} \cup \{\delta_{(l,\gamma),\lambda}:(l,\gamma) \in [n] \times [m]\}
\end{align*}
has the same distribution as 
\begin{align*}
    \mathcal{Z}=\{\xi_{(l,\gamma)}:(l,\gamma) \in [n] \times [m]\} \cup \{\delta_{(l,\gamma)}:(l,\gamma) \in [n] \times [m]\}
\end{align*}
Let $Z_{(l,\gamma)}=\xi_{(l,\gamma)} \delta_{(l,\gamma)} e_{\gamma} e_l^T$ and $Z_{(l,\gamma),\lambda}= \xi_{(l,\gamma), \lambda} \delta_{(l,\gamma), \lambda} e_{\gamma} e_l^T$.
Let $\{\Upsilon_1,...,\Upsilon_k\}$ be a family of $L_{\infty}(S_{\infty}^d)$ random matrices.
Assume further that the collection $\{\mathcal{Z}_{\lambda}\}_{\lambda \in [k]}$ is independent of $S_1, S_2, G_1, G_2$, and $\{\Upsilon_1,...,\Upsilon_k\}$. (In other words, $\{\Upsilon_1,...,\Upsilon_k\}$ can possibly be dependent with $S_1, S_2, G_1, G_2$.)
For each $(l,\gamma) \in [n] \times [m]$ and $\lambda \in k$, we define random vectors $\Theta_{(l,\gamma), \lambda, 1}, \Theta_{(l,\gamma), \lambda, 2} \in \R^d$ such that
\begin{align*}
    \Theta_{(l,\gamma), \lambda, 1} = \xi_{(l,\gamma),\lambda} \delta_{(l,\gamma), \lambda} \mathbf{u}_l^T \text{ and } \Theta_{(l,\gamma), \lambda, 2} = e_{\gamma}^TS_2(t)U
\end{align*}
where $e_{\gamma}$ represents the $\gamma$-th coordinate vector. Then, given $0 \le \beta_1,...,\beta_k \le +\infty$ such that $\sum \limits_{\lambda=1}^k \frac{1}{\beta_{\lambda}}=1-\frac{k}{2q}$, $\tau_1, \ldots, \tau_k \in \sym(\{1,2 \})$, there exists $c_{\ref*{lem:decouptraceineqose}}>0$ such that
\begin{align*}
    &\sum_{(l,\gamma) \in [n] \times [m]} \E[ \tr \Theta_{(l,\gamma), 1, \tau_1(1)}^T\Theta_{(l,\gamma), 1, \tau_1(2)}
	\Upsilon_1\Theta_{(l,\gamma), 2, \tau_2(1)}^T\Theta_{(l,\gamma), 2, \tau_2(2)} \\&\cdots
	\Upsilon_2\Theta_{(l,\gamma), k, \tau_k(1)}^T\Theta_{(l,\gamma), k, \tau_k(2)}\Upsilon_k]  \\ \le &
       (c_{\ref*{lem:decouptraceineqose}}(pm)^\frac{1}{q}\sqrt{\max\{pd,q\}})^{\frac{2qk-4q}{2q-2}} (\E \tr((\Gamma(t))^{2q}))^{\frac{1}{q} \cdot \frac{2q-k}{2q-2}} \prod \limits_{\lambda=1}^k \norm{\Upsilon_{\lambda}}_{\beta_{\lambda}}
\end{align*}
\end{lemma}

\begin{proof}\textcolor{red}{TOPROVE 23}\end{proof} 
\section{Conclusions}

We give an oblivious subspace embedding with optimal embedding dimension that achieves near-optimal sparsity, thus nearly matching a conjecture of Nelson and Nguyen in terms of the best sparsity attainable by an optimal oblivious subspace embedding. We also propose a fast algorithm for constructing low-distortion subspace embeddings, based on a new family of Leverage Score Sparsified embeddings with Independent Columns (LESS-IC). This new algorithm leads to speedups in downstream applications such as optimization problems based on constrained or regularized least squares. As a by-product of our analysis, we develop a new set of tools for matrix universality, combining a decoupling argument with a two-dimensional interpolation method, which are likely of independent interest. 


\printbibliography



\end{document}
