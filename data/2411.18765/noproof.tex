\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsfonts,latexsym,amsmath,amsthm,amssymb,amscd,euscript,mathrsfs,graphicx}
\usepackage{framed}
\usepackage{fullpage}
\usepackage{color}
\usepackage{enumitem}
\usepackage[obeyFinal,textsize=scriptsize,shadow]{todonotes}
\usepackage{tikz}
\usetikzlibrary{matrix}
\usepackage[hidelinks]{hyperref}
\usepackage{cleveref}
\usepackage{soul} 
\usepackage{todonotes}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}

\newenvironment{statement}[1]{\smallskip\noindent\color[rgb]{1.00,0.00,0.50} {\bf #1.}}{}



\theoremstyle{definition}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{defn}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{question}[theorem]{Question}
\newtheorem{problem}[theorem]{Problem}

\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\newtheorem*{notation}{Notation}
\newtheorem*{note}{Note}

\newcommand{\BC}{\mathbb C}
\newcommand{\BE}{\mathbb E}
\newcommand{\BF}{\mathbb F}
\newcommand{\BI}{\mathbb I}
\newcommand{\BN}{\mathbb N}
\newcommand{\BP}{\mathbb P}
\newcommand{\BQ}{\mathbb Q}
\newcommand{\BR}{\mathbb R}
\newcommand{\BZ}{\mathbb Z}
\newcommand{\eps}{\varepsilon}
\newcommand{\ten}{100}
\DeclareMathOperator{\tr}{Tr}
\DeclareMathOperator{\Bin}{Bin}
\DeclareMathOperator{\Pois}{Pois}
\DeclareMathOperator{\val}{val}
\DeclareMathOperator{\rev}{rev}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\polylog}{polylog}
\DeclareMathOperator*{\poly}{poly}

\newcommand{\fC}{\mathfrak{C}}
\newcommand{\fD}{\mathfrak{D}}


\newcommand{\ba}{\mathbf a}
\newcommand{\bb}{\mathbf b}

\newcommand{\allen}[1]{\textcolor{green}{\textbf{allen:} #1}}

\newcommand{\anders}[1]{\textcolor{orange}{\textbf{anders:} #1}}

\title{Near-Optimal Trace Reconstruction for Mildly Separated Strings}
\author{Anders Aamand\thanks{\texttt{aa@di.ku.dk}. University of Copenhagen. This work was supported by the DFF-International Postdoc Grant 0164-00022B and by the VILLUM Foundation grants 54451 and 16582.} \and Allen Liu\thanks{\texttt{cliu568@mit.edu}. Massachusetts Institute of Technology. This work was supported in part by an NSF Graduate Research Fellowship, a Hertz Fellowship, and a Citadel GQS Fellowship.} \and Shyam Narayanan\thanks{\texttt{shyam.s.narayanan@gmail.com}.Massachusetts Institute of Technology. Supported by the NSF TRIPODS Program, an NSF Graduate Fellowship, and a Google Fellowship.}}
\date{\today}


\begin{document}
\maketitle
\begin{abstract}
    In the \emph{trace reconstruction} problem our goal is to learn an unknown string $x\in \{0,1\}^n$ given independent \emph{traces} of $x$. A trace is obtained by independently deleting each bit of $x$ with some probability $\delta$ and concatenating the remaining bits. It is a major open question whether the trace reconstruction problem can be solved with a polynomial number of traces when the deletion probability $\delta$ is constant. The best known upper bound and lower bounds are respectively $\exp(\tilde O(n^{1/5}))$~\cite{Chase21b} and $\tilde \Omega(n^{3/2})~\cite{Chase21a}$. Our main result is that if the string $x$ is \emph{mildly separated}, meaning that the number of zeros between any two ones in $x$ is at least $\polylog n$, and if $\delta$ is a sufficiently small constant, then the trace reconstruction problem can be solved with $O(n \log n)$ traces and in polynomial time. 
\end{abstract}

\section{Introduction}
\emph{Trace reconstruction} is a well-studied problem at the interface of string algorithms and learning theory. Informally, the goal of trace reconstruction is to recover an unknown string $x$ given several independent noisy copies of the string.

Formally, fix an integer $n \ge 1$ and a deletion parameter $\delta \in (0, 1)$. Let $x \in \{0, 1\}^n$ be an unknown binary string with $x_i$ representing the $i$th bit of $x$. Then, a \emph{trace} $\tilde{x}$ of $x$ is generated by deleting every bit $x_i$ independently with probability $\delta$ (and retaining it otherwise), and concatenating the retained bits together. For instance, if $x = 01001$ and we delete the second and third bits, the trace would be $001$ (from the first, fourth, and fifth bits of $x$).
For a fixed string $x$, note that the trace follows some distribution over bitstrings, where the randomness comes from which bits are deleted.
In \emph{trace reconstruction}, we assume we are given $N$ i.i.d. traces $\tilde{x}^{(1)}, \dots, \tilde{x}^{(N)}$, and our goal is to recover the
original string $x$ with high probability.


The trace reconstruction problem has been a very well studied problem over the past two decades~\cite{Levenshtein01a, Levenshtein01b, BatuKKM04, KannanM05, HolensteinMPW08, ViswanathanS08, McGregorPV14, DeOS17, NazarovP17, PeresZ17, HartungHP18, HoldenL18, HoldenPP18, Chase21a, ChenDLSS21a, ChenDLSS21b, Chase21b, Rubinstein23}.
There have also been numerous generalizations or variants of trace reconstruction studied in the literature, including coded trace reconstruction~\cite{CheraghchiGMR19, BrakensiekLS19}, reconstructing mixture models~\cite{BanCFSS19, BanCSS19, Narayanan21}, reconstructing alternatives to strings~\cite{DaviesRR19, KrishnamurthyMMP21, NarayananR21, McGregorS22, SunY23, McGregorS24}, and approximate trace reconstruction~\cite{DaviesRSR21, ChaseP21, ChakrabortyDK21, ChenDLSS22, ChenDLSS23}.

In perhaps the most well-studied version of trace reconstruction, $x$ is assumed to be an arbitrary $n$-bit string and the deletion parameter $\delta$ is assumed to be a fixed constant independent of $n$. In this case, the best known algorithm requires $e^{\tilde{O}(n^{1/5})}$ random traces to reconstruct $x$ with high probability~\cite{Chase21b}. As we do not know of any polynomial-time (or even polynomial-sample) algorithms for trace reconstruction, there have been many works making distributional assumptions on the string $x$, such as $x$ being a uniformly random string~\cite{HolensteinMPW08, McGregorPV14, PeresZ17, HoldenPP18, Rubinstein23} or $x$ being drawn from a ``smoothed'' distribution~\cite{ChenDLSS21a}.
An alternative assumption is that the string $x$ is parameterized, meaning that $x$ comes from a certain ``nice'' class of strings that may be amenable to efficient algorithms~\cite{KrishnamurthyMMP21, DaviesRSR21}.

In this work, we also wish to understand parameterized classes of strings for which we can solve trace reconstruction efficiently. Indeed, we give an algorithm using polynomial traces and runtime, that works for a general class of strings that we call $L$-separated strings. This significantly broadens the classes of strings for which polynomial-time algorithms are known~\cite{KrishnamurthyMMP21}.



\paragraph{Main Result}
Our main result concerns trace reconstruction of strings that are \emph{mildly separated}. We say that a string $x$ is $L$-separated if the number of zeros between any two consecutive ones is at least $L$. Depicting a string $x\in \{0,1\}^n$ with $t$ ones as 
$$
\underbrace{0 \dots 0}_{a_0 \text{ times}} 1 \underbrace{0 \dots 0}_{a_1 \text{ times}} 1 \cdots 1 \underbrace{0 \dots 0}_{a_{t} \text{ times}},
$$
it is $L$-separated if and only if $a_i\geq L$ for each $i$ with $1\leq i\leq t-1$.
Note that we make no assumptions on $a_0$ or $a_t$. Our main result is as follows.
\begin{theorem}\label{thm:main}
There exists an algorithm that solves the trace reconstruction problem with high probability in $n$ on any $L$-separated string $x$, provided that $L\geq C(\log n)^8$ for a universal constant $C$, and that the deletion probability is at most some universal constant $c_0$. The algorithm uses $N=O(n\log n)$ independently sampled traces of $x$, $\tilde{x}^{(1)}, \dots, \tilde{x}^{(N)}$, and runs in $\poly(n)$ time.
\end{theorem}
We note that the number of traces is nearly optimal. Even distinguishing between two strings $x,x'$ which contain only a single one at positions $\lfloor n/2 \rfloor$ and $\lfloor n/2 \rfloor+1$ respectively, requires $\Omega(n)$ traces to succeed with probability $1/2+\Omega(1)$.

While trace reconstruction is known to be solvable very efficiently for random strings \cite{HoldenPP18, Rubinstein23}, there are certain structured classes of strings that appear to be natural hard instances for existing approaches.  Our algorithm can be seen as solving one basic class of hard instances.  It is worth noting the work by~\cite{ChenDLSS21a} which studies the trace reconstruction problem when the deletion probability $\delta$ is sub-constant. They show that the simple Bitwise Majority Alignment (BMA) algorithm from~\cite{BatuKKM04} can succeed with $1/n^{o(1)}$ deletion probability as long as the original string does not contain \emph{deserts} \--- which are highly repetitive blocks where some short substring is repeated many times.  They then combine this with an algorithm for reconstructing repetitive blocks \--- but this part of their algorithm requires a significantly smaller deletion probability of $\delta\leq 1/n^{1/3+\eps}$.  This suggests that strings containing many repetitive blocks are a natural hard instance and good test-bed for developing new algorithms and approaches.  $L$-separated strings can be thought of as the simplest class of highly repetitive strings (where the repeating is pattern is just a $0$), where every repetition has length at least $L$.  





\paragraph{Comparison to Related Work}
Most closely related to our work is the result by Krishnamurthy et al.~\cite{KrishnamurthyMMP21} stating that if $x$ has at most $k$ ones and if each pair of ones is separated by a~\emph{run} of zeros of length $\Omega(k\log n)$, then $x$ can be recovered in polynomial time from $\poly(n)$ many traces. In particular, for strings with $k=O((\log n)^7)$ ones, the required separation is milder than ours, albeit not below $\Omega(\log n)$. Our algorithm works in general assuming a $\polylog n$ separation of the ones but with no additional requirement on the number of ones: indeed, we could even have $\frac{n}{\polylog n}$ ones. Assuming no sparsity assumptions, \cite{KrishnamurthyMMP21} would need to set $L \ge \Omega(\sqrt{n \log n})$, as a $\sqrt{n \log n}$-separated string can be $\Theta(\sqrt{n/\log n})$-sparse in the worst case. The techniques of~\cite{KrishnamurthyMMP21} are also very different than ours. They recursively cluster the positions of the ones in the observed traces to correctly align a large fraction of the ones in the observed traces to ones in the string $x$. In contrast, our algorithm works quite differently and is of a more sequential nature processing the traces from left to right (or right to left). See~\Cref{sec:technical_contributions} for a discussion of our algorithm.

Another paper studying strings with large runs is by Davies et al.~\cite{DaviesRSR21}. They consider~\emph{approximate} trace reconstruction, specifically how many traces are needed to approximately reconstruct $x$ up to edit distance $\eps n$ under various assumptions on the lengths of the runs of zeros and ones in $x$. Among other results but most closely related to ours, they show that one can $\eps$-approximately reconstruct $x$ using $O((\log n)/\eps^2)$ traces if the runs of zeros have length $\gg \frac{\log n}{\eps}$ and if the runs of ones are all of length $\leq C \log n$  or $\gg 3C\log n$ for a constant $C$ (e.g. they could have length one as in our paper). However, for exact reconstruction, they would need to set $\eps < 1/n$, which means they do not provide any nontrivial guarantees in our setting.






\subsection{Technical Contributions}\label{sec:technical_contributions}
In this section, we give a high level overview of our techniques. Recall that we want to reconstruct a string $x\in \{0,1\}^n$ from independent traces $\tilde x$ where we assume that $x$ is mildly separated. More concretely, we assume that there are numbers $a_0,\dots,a_t \gg \polylog n$ such that $x$ consists of $a_0$ zeros followed by a one, followed by $a_1$ zeros followed by a one and so on, with the last $a_t$ bits of $x$ being zero. Writing $a_{\leq i}=\sum_{0\leq j\leq i} a_j$, we thus have that there are $t$ ones in $x$ at positions $a_{\leq i}+i+1$ for $0\leq i\leq t-1$. 

Note that a retained bit in a trace $\tilde x$ naturally corresponds to a bit in $x$. More formally, for a trace $\tilde x$ of length $\ell$, let $i_1<\cdots<i_\ell$ be the $\ell$ positions in $x$ where the bit was retained when generating $\tilde x$ so that $\tilde x=x_{i_1}\cdots x_{i_\ell}$. Then, the correspondence is defined by the map from $[\ell]$ to $[n]$ mapping $j\mapsto i_j$. We think of this map as the correct \emph{alignment} of $\tilde x$ to $x$.


Our main technical contribution is an alignment algorithm (see~\Cref{alg:findmthone}) which takes in some $m \leq t$ and estimates $b_0,\dots,b_{m-1}$ of $a_0,\dots,a_{m-1}$ satisfying that for all $i$, $|b_i-a_i|=O(\sqrt{a_i\log n})$, and correctly aligns the one in a trace $\tilde x$ corresponding to the $m$'th one of $x$ with probability $1-O(\delta)$ (where the randomness is over the draw of $\tilde x$ \---naturally, this requires that the $m$'th one of $x$ was not deleted). 

Moreover, we ensure that the alignment procedure has that with high probability, say $1-O(n^{-100})$, it never aligns a one in $\tilde x$ too far to the right in $x$: if the one in $\tilde x$ corresponding to the  $m_0$'th one of $x$ is aligned to the $m$'th one of $x$, then $m\leq m_0$. We will refer to this latter property by saying that the algorithm is \emph{never ahead} with high probability. If $m< m_0$, we say that the algorithm is \emph{behind}. Thus, to show that the algorithm correctly aligns the $m$'th one, it suffices to show that the probability that the algorithm is behind is $O(\delta)$. 


We first discuss how to implement this alignment procedure and then afterwards we discuss how to complete the reconstruction by using this alignment procedure.

\paragraph{The alignment procedure of~\Cref{alg:findmthone}.}
The main technical challenge of this paper is the analysis of~\Cref{alg:findmthone}. Let us first describe on a high level how the algorithm works.
For $0\leq j\leq j'\leq m$, we write $b_{j:j'}=\sum_{i=j}^{j'-1}b_j$. Suppose that the trace $\tilde x$ consists of $s_0$ zeros followed by a one followed by $s_1$ zeros followed by a one and so on. The algorithm first attempts to align the first one in $\tilde x$ with a one in $x$ by finding the minimal $j_0$ such that $(1-\delta)\cdot b_{0:j_0}$ is within $C\log n\sqrt{b_{0:j_0}}$ of $s_0$ for a sufficiently large $C$. Inductively, having determined $j_i$ (that is the alignment of the $i$'th one of $\tilde x$), it looks for the minimal $j_{i+1}> j_i$ satisfying that there is a $j_i \leq j'< j_{i+1}$ such that $b_{j':j_{i+1}}\cdot(1-\delta)$ is within $C\log n\sqrt{b_{j':j_{i+1}}}$ of $s_{i+1}$.
Intuitively, when looking at the $i$'th one in the trace, we want to find the \emph{earliest} possible location in the real string (which has gaps estimated by $b_0, b_1, \dots$) that could plausibly align with the one in the trace.

It is relatively easy to check that the algorithm is never ahead with very high probability. Indeed, by concentration bounds on the number of deleted zeros and the fact that $|b_j-a_j|=O(\sqrt{a_j\log n})$ for all $j\leq m$, it always has the option of aligning the $(i+1)$'st one in $\tilde x$ to the correct one in $x$. However, it might align to an earlier one in $x$ since it is looking for the \emph{minimum} $j_{i+1}$ such that an alignment is possible. For a very simple example, suppose that $a_0=n^{\Omega(1)}$ and $a_1=\cdots=a_m=b_1=\cdots=b_m=\polylog(n)$. If the first $k<m$ ones of $x$ are deleted and the $(k+1)$'st one is retained, the algorithm will align the retained one (which corresponds to the $(k+1)$'st one of $x$) with the first one of $x$ resulting in the aligning algorithm being $k$ steps behind. Moreover, the algorithm will remain $k$ steps behind all the way up to the $m$'th one of $x$. The probability of this happening is $\Theta(\delta^k)$. To prove that the probability of the algorithm being behind when aligning the $m$'th one of $x$ is at most $1-O(\delta)$, we prove a much stronger statement which is amenable to an inductive proof, essentially stating that this is the worst that can happen: The probability of the algorithm being $k$ steps behind at any fixed point is bounded by $(C\delta)^k$ for a constant $C$.
In particular, we show that there is a sort of amortization \--- whenever there is a substring that can cause the algorithm to fall further behind with some probability (i.e. if certain bits are deleted), the substring also helps the algorithm catch back up if it is already behind. 







\paragraph{Reconstructing $x$ using~\Cref{alg:findmthone}.}
Using~\Cref{alg:findmthone} we can iteratively get estimates $b_0,\dots,b_t$ with $|b_i-a_i|=O(\sqrt{a_i\log n})$. Namely, suppose that we have the estimates $b_0,\dots,b_m$. We then run~\Cref{alg:findmthone} on $O(\log n)$ independent traces and with high probability, for a $1-O(\delta)$ fraction of them, we have that the $m$'th and $(m+1)$'st one of $x$ are retained in $\tilde x$ and correctly aligned. In particular, with probability $1-O(\delta)$ we can identify both the $m$'th and $(m+1)$'st one of $x$ in $\tilde x$ and taking the median over the gaps between these (and appropriately rescaling by $\frac{1}{1-\delta}$), we obtain an estimate of $b_{m+1}$ such that $|b_{m+1}-a_{m+1}|=O(\sqrt{a_{m+1}\log n})$). Note that the success probability of $1-O(\delta)$ is enough to obtain the coarse estimates using the median approach but we cannot obtain a fine estimate by taking the average since with constant probability $O(\delta)$, we may have misaligned the gap completely and then our estimate can be arbitrarily off. 


To obtain fine estimates, we first obtain coarse estimates, say $b_0,\dots, b_t$, for all of the gaps. Next, we show that we can identify the $m$'th and $(m+1)$'st one in $x$ in a trace $\tilde x$ (if they are retained) and we can detect if they were deleted not just with probability $1-O(\delta)$ but with very high probability. The trick here is to run~\Cref{alg:findmthone} both from the left and from the right on $\tilde x$ looking for respectively the one in $\tilde x$ aligned to the $m$'th one in $x$ and the one in $\tilde x$ aligned to the $(m+1)$'st one in $x$ (which is the $(t-m)$'th one when running the algorithm from the right). If either of these runs fails to align a one in $\tilde x$ to respectively the $m$'th and $(m+1)$'st one in $x$ or the runs disagree on their alignment,
then we will almost certainly know. To see why, assuming that we are never ahead in the alignment procedure from the left, if we believe we have reached the $m$'th one in $x$, then we are truly at some $m_0$'th one where $m_0 \ge m$. By a symmetric argument, if we believe we have reached the $(m+1)$'st one in $x$ after running the procedure from the right, we are truly at the $m_1$'th one in $x$, where $m_1 \le m+1$. The key observation now is that $m_0 \le m_1$ \emph{if and only if} $m_0 = m$ and $m_1 = m+1$, meaning that both runs succeeding is equivalent to the one found in the left-alignment procedure being strictly earlier than the one found in the right-alignment procedure.
So, if we realize that either run fails to align the ones properly, we discard the trace and repeat on a newly sampled trace.



Finally, we can ensure that the success of the runs of the alignment algorithm is independent of the deletion of zeros between the $m$'th and $(m+1)$'st ones in $x$.  If a trace is not discarded, then with very high probability, the gap between the ones in $\tilde x$ aligned to the $m$'th and $(m+1)$'st ones in $x$ (normalized by $\frac{1}{1-\delta}$) is an unbiased estimator for $a_{m+1}$. By taking the average of the gap over $\tilde O(n)$ traces, normalizing by $\frac{1}{1-\delta}$, and rounding to the nearest integer, we determine $a_{m+1}$ exactly with very high probability. Doing so for each $m$, reconstructs $x$.





\paragraph{Road map of our paper}
In~\Cref{sec:notation}, we introduce notation. In~\Cref{sec:main-process}, we describe and analyse our main alignment procedure. We first prove that with high probability it is never ahead (\Cref{lem:not-ahead}). Second, in \Cref{sec:main-technical}, we bound the probability that it is behind (\Cref{lem:main-technical}). Finally, in~\Cref{sec:algorithm}, we describe our full trace reconstruction algorithm and prove~\Cref{thm:main}.








\section{Notation}\label{sec:notation}

We note a few notational conventions and definitions.

\begin{itemize}
    \item We recall that a bitstring $x$ is \emph{$L$-separated} if the gap between any consecutive $1$'s in the string contains at least $L$ $0$'s.
    \item Given an string $x$, we say that a \emph{run} is a contiguous sequence of $0$'s in $x$. For $x = \underbrace{0 \dots 0}_{a_0 \text{ times}} 1 \underbrace{0 \dots 0}_{a_1 \text{ times}} 1 \cdots 1 \underbrace{0 \dots 0}_{a_{t} \text{ times}},$ the $i$th run of $x$ is the sequence $\underbrace{0 \dots 0}_{a_i \text{ times}}$, and has length $a_i$.
    \item For any bitstring $x = x_1 x_2 \cdots x_n$, we use $\rev(x) := x_n x_{n-1} \cdots x_1$ to denote the string where the bits have been reversed.
    \item We use $\ba = a_0, a_1, \dots, a_{m-1}$ to denote an integer sequence of length $m$. For notational convenience, for any $0 \le j < j' \le m$, we write $\ba_{j:j'}$ to denote the subsequence $a_{j}, a_{j+1}, \dots, a_{j'-1}$, and $a_{j:j'} := \sum_{i = j}^{j'-1} a_i$.
\end{itemize}

We will define some sufficiently large constants $C_0, C_1, C_2, C_3$ and a small constant $c_0$. We will assume the separation parameter $L = C_3 \cdot \log^8 n$, and the deletion parameter $\delta \le c_0$, where $c_0 = \frac{1}{3 \cdot 10^6}$. We did not make significant effort to optimize the constant $c_0$ or the value $8$ in $\log^8 n$, though we believe that any straightforward modifications to our analysis will not obtain bounds such as $c_0 \ge \frac{1}{2}$ or a separation of $L = O(\log n)$.


\section{Main Alignment Procedure} \label{sec:main-process}

\subsection{Description and Main Lemmas} \label{sec:main-process-description}

In this section, we consider a probabilistic process that models a simpler version of the trace reconstruction problem that we aim to solve. In the simpler version of the trace reconstruction problem, suppose that we never delete any $0$'s, but delete each $1$ independently with $\delta$ probability. Let $a_0, \dots, a_{m-1} \le n$ represent the true lengths of the first $m$ gaps (so the first $1$ is at position $1+a_0$, the second $1$ is at position $2 + a_0 + a_1$, and so on). Moreover, suppose we have some current predictions $b_0, \dots, b_{m-1} \le n$ of the gaps $a_0, \dots, a_{m-1}$. The high level goal will be, given a single trace (where the trace means only $1$s are deleted), to identify the $m$th $1$ in the trace from the the original string with reasonably high probability. (Note that the $m$th $1$ is deleted with $\delta$ probability, in which case we cannot succeed.)

We will describe and analyze the probabilistic process, and then explain how the analysis of the process can help us solve the trace reconstruction problem in \Cref{sec:algorithm}.

In the process, we fix $m \le n$ and two sequences $\ba = a_0, \dots, a_{m-1}$ and $\bb = b_0, \dots, b_{m'-1}$ where $\ba$ has length $m$ but $\bb$ has some length $m'$ which may or may not equal $m$.
Moreover, we assume $L \le a_i \le n$ and $L \le b_j \le n$ for every term $a_i \in \ba$ and $b_j \in \bb.$ 



Now, for each $1 \le i \le m-1$, let $w_i \in \{0, 1\}$ be i.i.d. random variables, with $w_i = 1$ with $1-\delta$ probability and $w_i = 0$ with $\delta$ probability. Also, let $w_0 = w_m = 1$ with probability $1$.
For each $0 \le i \le m$ with $w_i = 1$, we define a value $f_i$ as follows. First, we set $f_0 = 0$. Next, for each index $i \ge 1$ such that $w_i = 1$, let $i_0$ denote the previous index with $w_{i_0} = 1$. We define $f_i$ to be the smallest index $j' > f_{i_0}$ such that there exists $f_{i_0} \le j < j'$ with $|b_{j:j'} - a_{i_0:i}| \le C_0 \cdot \log n \cdot \sqrt{b_{j:j'}}$, where $C_0$ is a sufficiently large constant. (If such an index does not exist, we set $f_i = \infty$.) 

Our goal will be for $f_m = m$. In general, for any $i$ with $w_i = 1$, we would like $f_i = i$. If $f_i < i$, we say that we are $i-f_i$ steps behind at step $i$, and if $f_i > i$, we say that we are $f_i-i$ steps ahead at step $i$.

First, we note the following lemma, which states that we will never be ahead with very high probability, as long as the sequences $\ba$ and $\bb$ are similar enough.

\begin{lemma} \label{lem:not-ahead}
    Set $C_1 = C_0/4$. Let $\ba, \bb$ be sequences of lengths $m, m'$, respectively, where $m' \ge m$.
    Suppose that $|b_{i}-a_{i}| \le C_1 \cdot \sqrt{b_{i} \log n}$ for all $0 \le i < m$. Then, with probability at least $1 - \frac{1}{n^{10}}$ (over the randomness of the $w_i$), for all $0 \le i \le m$ with $w_i = 1$, $f_i \le i$.
\end{lemma}

\begin{proof}\textcolor{red}{TOPROVE 0}\end{proof}

The main technical result will be showing that $f_m \ge m$ with reasonably high probability, i.e., with reasonably high probability we are not behind. This result will hold for \emph{any} choice of $\ba, \bb$ and does not require any similarity between these sequences. In other words, our goal is to prove the following lemma.

\begin{lemma} \label{lem:main-technical}
    Let $\ba, \bb$ be strings of length at most $n$ with every $\ba_i, \bb_j$ between $L$ and $n$, where $L = C \cdot \log^8 n$ for a sufficiently large constant $C$. Define $m = |\ba|$.
    Then, for any $\delta \le \frac{1}{3 \cdot 10^6}$, with probability at least $1-200 \cdot \delta$ over the randomness of $w_1, \dots, w_{m-1}$, $f_m \ge m$.
\end{lemma}

\subsection{Proof of \Cref{lem:main-technical}}\label{sec:main-technical}

In this section, we prove \Cref{lem:main-technical}.

We will set a parameter $K = C_2 \log n$, where $C_2$ is a sufficiently large constant.
For any $k \ge 0$, given the sequences $\ba = a_0, \dots, a_{m-1}$ and $\bb = b_0, \dots, b_{m'-1}$ (of possibly differing lengths), we define $p_k(\ba, \bb)$ to be the probability (over the randomness of $w_1, \dots, w_{m-1}$) that
\begin{itemize}
    \item $f_m \le m-k$.
    \item For any indices $0 \le i \le i' \le m$ with $w_i, w_{i'} = 1$, $f_{i'}-f_i \ge (i'-i)-K$.
\end{itemize}
Equivalently, this is the same as the probability that we fall behind at least $k$ steps from step $0$ to step $m$, but we never fall behind $K+1$ or more steps (relatively) from any (possibly intermediate) steps $i$ to $i'$.
For any $m \ge 1$, we define $p_k(m)$ to be the supremum value of $p_k(\ba, \bb)$ over any sequences $\ba, \bb$ where $\ba$ has length at most $m$ and every $a_i$ and $b_j$ is between $L$ and $n$, and we also define $p_k := \sup_{m \ge 1} p_k(m)$.

Note that for any $k > K$, $p_k(\ba, \bb) = 0$, as $f_m 
= m-k$ means $f_m - f_0 < (m-0) - K$. So, $p_k(m)$ and $p_k$ also equal $0$ for any $k > K$.


First, we note a simple proposition, that will only be useful for simplifying the argument at certain places.

\begin{proposition} \label{prop:p0-equals-1}
    For any $m \ge 1$, $p_0(m) = 1$.
\end{proposition}

\begin{proof}\textcolor{red}{TOPROVE 1}\end{proof}

We now aim to bound the probabilities $p_k$ for $k \le K$. We will do this via an inductive approach on the length of $m$, where the high-level idea is that if we fall back by $k$ steps, there is a natural splitting point where we can say first we fell back by $k_1$ steps, and then by $k_2$ steps, for some $k_1, k_2 > 0$ with $k_1+k_2 = k$ -- see Lemmas~\ref{lem:half-periodic-bound} and~\ref{lem:no-periodic-bound}. This natural splitting point will be based on the structure of the similarity of $\ba$ and $\bb$, and will not work if $\ba$ and $\bb$ share a $k$-periodic structure. But in the periodic case, we can give a more direct argument that we cannot fall back by $k$ steps (i.e., a full period), even with $\frac{1}{\poly(n)}$ probability -- see \Cref{lem:periodic-bound}. We can then compute a recursive formula for the probability of falling back $k$ steps, by saying we need to first fall back $k_1$ steps and then fall back $k_2$ steps. In \Cref{lem:computation}, we bound the terms of this recursion.

\begin{lemma} \label{lem:periodic-bound}
    Fix any $m \ge k \ge 1$ such that $k \le K$, and suppose that $L \ge C_3 \cdot \log^8 n$, where $C_3$ is a sufficiently large multiple of $C_0^2 \cdot C_2^6$. Suppose that $\ba, \bb$ are sequences such that for every $0 \le i < m-k,$ $|b_i-a_i| \le C_0 \log n \cdot \sqrt{b_i}$ and $|b_i-a_{i+k}| \le C_0 \log n \cdot \sqrt{b_i}$. Then, the probability $p_k(\ba, \bb) \le (2\delta)^{K}$.
\end{lemma}

\begin{proof}\textcolor{red}{TOPROVE 2}\end{proof}

\begin{lemma} \label{lem:half-periodic-bound}
    Fix any $m \ge k$ such that $k \le K$, and suppose that $L \ge C_3 \cdot \log^2 n \cdot K^6$. Suppose that $\ba, \bb$ are sequences of length $m$, such that for every $0 \le i < m-k,$ $|b_i-a_{i+k}| \le C_0 \log n \cdot \sqrt{b_i}$.
    Then, the probability
\[p_k(\ba, \bb) \le (2\delta)^{K} + \sum_{\substack{h_1, h_2, k_2, k_2 \ge 0 \\ h_1+h_2+k_1+k_2 \ge k \\ k_1, k_2 \le K \\ (h_1, h_2, k_1, k_2) \neq (0, 0, 0, k), (0, 0, k, 0)}} \delta^{h_1+h_2} p_{k_1}(m-1) p_{k_2}(m-1).\]
\end{lemma}

\begin{proof}\textcolor{red}{TOPROVE 3}\end{proof}

\begin{lemma} \label{lem:no-periodic-bound}
    Fix any $m \ge k$ such that $k \le K$, and suppose that $L \ge C_3 \cdot \log^2 n \cdot K^6$. Suppose that $\ba, \bb$ are sequences of length $m$.
    Then, the probability
\[p_k(\ba, \bb) \le (2\delta)^{K} + \sum_{\substack{h_1, h_2, k_2, k_2 \ge 0 \\ h_1+h_2+k_1+k_2 \ge k \\ k_1, k_2 \le K \\ (h_1, h_2, k_1, k_2) \neq (0, 0, 0, k), (0, 0, k, 0)}} \delta^{h_1+h_2} p_{k_1}(m-1) p_{k_2}(m-1).\]
\end{lemma}

\begin{proof}\textcolor{red}{TOPROVE 4}\end{proof}

Overall, this implies that 
\[p_k(m) \le (2\delta)^{K} + 2 \cdot \sum_{\substack{h_1, h_2, k_2, k_2 \ge 0 \\ h_1+h_2+k_1+k_2 \ge k \\ k_1, k_2 \le K \\ (h_1, h_2, k_1, k_2) \neq (0, 0, 0, k), (0, 0, k, 0)}} \delta^{h_1+h_2} p_{k_1}(m-1) p_{k_2}(m-1).\]

We now can universally bound $p_k$ for all $0 \le k \le K$. To do so, we first recall some basic properties of the Catalan numbers.

\begin{fact} \label{fact:catalan}
    For $n \ge 0$, the Catalan numbers $\fC_n$~\footnote{We use $\fC_n$ rather than the more standard $C_n$ to avoid confusion with the constants $C_0, C_1, \dots$ we have defined.} are defined as $\fC_n = {2n \choose n}/(n+1)$. They satisfy the following list of properties.
\begin{enumerate}
    \item $\fC_0 = 1$ and for all $n \ge 0$, $\fC_{n+1} = \sum_{i=0}^n \fC_i \fC_{n-i}$.
    \item For all $n \ge 1$, $2 \le \frac{\fC_{n+1}}{\fC_n} \le 4$.
    \item For all $n \ge 0$, $\fC_n \le 4^n$.
\end{enumerate}
\end{fact}

\begin{lemma} \label{lem:computation}
    Assume $\delta \le \frac{1}{3 \cdot \ten^3}$, and define $\fD_k := 100^{2k-1} \fC_k$ for $k \ge 1$ and $\fD_0 = 1$. Then, for all $0 \le k \le K$, $p_k \le \fD_k \cdot \delta^k$.
\end{lemma}

\begin{proof}\textcolor{red}{TOPROVE 5}\end{proof}

We are now ready to prove \Cref{lem:main-technical}.

\begin{proof}\textcolor{red}{TOPROVE 6}\end{proof}

\iffalse
    \begin{figure}[!ht]
    \centering
    \resizebox{\textwidth}{!}{\begin{circuitikz}
    \tikzstyle{every node}=[font=\small]
    \draw (3,1) to[short] (9.75,1);
    \draw (3,0) to[short] (8.5,0);
    \draw [short] (8.5,0) -- (9.75,1);
    \draw [short] (8,0) -- (9.25,1);
    \draw [short] (7.0,0) -- (8.25,1);
    \draw [short] (4.25,0) -- (5.5,1);
    \draw [short] (4,0) -- (4.75,1);
    \node [font=\tiny] at (9.55,1.2) {$a_{m}$};
    \node [font=\tiny] at (8.3,-0.25) {$b_{m-k}$};
    \node [font=\tiny] at (8.75,1.2) {$a_{m-1}$};
    \node [font=\tiny] at (7.4,-0.25) {$b_{m-k-1}$};
    \node [font=\small] at (6.25,0.5) {$\mathbf{\cdots}$};
    \node [font=\tiny] at (5.15,1.2) {$a_{m-h}$};
    \node [font=\tiny] at (4.1,-0.25) {$b_{m-k-h}$};
    \node [font=\small] at (3.6,0.5) {$\mathbf{\cdots}$};
    \end{circuitikz}
    }\label{fig:my_label}
    \end{figure}
\fi


\section{Full algorithm/analysis} \label{sec:algorithm}

Let us depict the true string $x \in \{0, 1\}^n$ as $\underbrace{0 \dots 0}_{a_0 \text{ times}} 1 \underbrace{0 \dots 0}_{a_1 \text{ times}} 1 \cdots 1 \underbrace{0 \dots 0}_{a_t \text{ times}},$ i.e., there are $t-1$ ones, and the string starts and ends with a run of $0$'s. This assumption can be made WLOG by padding the string with $L$ $0$'s at the front and the end. For any $L$-separated string, doing this padding maintains the $L$-separated property, and we can easily simulate the padded trace by adding $\Bin(L, 1-\delta)$ $0$'s at the front and $\Bin(L, 1-\delta)$ $0$'s at the back. Once we reconstruct the padded string, we remove the padding to get $x$.


We assume we know the value of $t$. Indeed, the number of $1$'s in a single trace $\tilde{x}$ is distributed as $\Bin(t, 1-\delta)$. So, by averaging the number of $1$'s over $O(n \log n)$ random traces and dividing by $1-\delta$, we get an estimate of $t-1$ that is accurate within $0.1$ with $1-\frac{1}{n^{10}}$ probability. Thus, by rounding, we know $t$ exactly with $1-\frac{1}{n^{10}}$ probability.

The main goal is now to learn the lengths $a_0, a_1, \dots, a_t$. If we learn these exactly just using the traces, this completes the proof. Our algorithm runs in two phases: a coarse estimation phase and a fine estimation phase. In the coarse estimation phase, we sequentially learn each $a_i$ up to error $O(\sqrt{a_i \log n})$. In the fine estimation phase, we learn each $a_i$ exactly, given the coarse estimates.

\subsection{Coarse estimation}

Fix some $0 \le m \le t$, and suppose that for all $i < m$, we have estimates $b_i$ satisfying $|b_i-(1-\delta) a_i| \le 10 \sqrt{a_i}$. (If $m = 0$, then we have no estimates yet.) Our goal will be to provide an estimate $b_m$ such that $|b_m-(1-\delta) a_m| \le 10 \sqrt{a_m}$.

Consider a trace $\tilde{x}$ of $x$. Let $w_0 = w_{t+1} = 1$ and for each $1 \le i \le t$, let $w_i$ be the indicator that the $i$th $1$ is retained. Next, for each $0 \le i \le t$, let $\tilde{a}_i \sim \Bin(a_i, 1-\delta)$ represent the number of $0$s in the $i$th run that were not deleted. Note that with at least $0.99$ probability, $|\tilde{a}_i-(1-\delta) a_i| \le 10 \sqrt{\log n \cdot a_i}$ for all $i$. Since $|b_i-(1-\delta) a_i| \le 10 \sqrt{a_i}$ for all $i < m$, this implies that $|\tilde{a}_i-b_i| \le 20 \sqrt{\log n \cdot b_i}$ for all $i < m$.

Now, even though we have no knowledge of $\tilde{a}_i$ or $a_i$, we can still simulate the probabilistic process of \Cref{sec:main-process}. Let $0 = i_0 < i_1 < \cdots < i_h = t+1$ be the list of all indices $i: 0 \le i \le t+1$ with $w_i = 1$. While we do not know the values $\tilde{a}_i$, for every pair of consecutive indices $i_q, i_{q+1}$, the value $\tilde{a}_{i_q:i_{q+1}}$ is exactly the number of $0$'s between the $q$th and $(q+1)$st $1$ in the trace $\tilde{x}$ (where we say that the $0$th $1$ is at position $0$ and the $(t+1)$st $1$ is at position $|\tilde{x}|+1$). In other words, if $r_q$ represents the position of the $q$th $1$, then $\tilde{a}_{i_q:i_{q+1}} = r_{q+1}-r_q-1$. Hence, because computing each $f_{i_{q+1}}$ only requires knowledge of $\bb$ and the value of $\tilde{a}_{i_q:i_{q+1}}$, and since $f_{i_0} = f_0 = 0$, the algorithm can in fact compute $g_q := f_{i_q}$ for all $0 \le q \le h$, using the same process as described in \Cref{sec:main-process}, even if the values $i_q$ are not known.

Algorithm~\ref{alg:findmthone} simulates this process, assuming knowledge of $m$, $b_0, \dots, b_{m-1}$, a single trace $\tilde{x}$, and $t$. In Algorithm~\ref{alg:findmthone}, we use the variable $\val$ to represent $g_q = f_{i_q}$, i.e., the current prediction of the position $i_q$. In other words, $\val - \, i_q$ equals the number of steps ahead (or $i_q - \val$ equals the number of steps behind) we are. 

\begin{algorithm}
\caption{Locate the $m$th and $(m+1)$st $1$ in $x$, in the trace $\tilde{x}$, and return the position and length of the gap.}
\label{alg:findmthone}
\begin{algorithmic}[1]
\Procedure{Align}{$\tilde{x}, t, m, b_0, \dots, b_{m-1}$}
\State Let $r_q$ be the position of the $q$th $1$ in $\tilde{x}$, for each $1 \le q \le t-1$.
\State $r_0 \leftarrow 0$, $r_t \leftarrow |\tilde{x}|+1$.
\State $\val \leftarrow 0$, $q \leftarrow 0$
\While{$\val < m$}
    \State Find the smallest $j'$ such that $\exists j, j'$ with $\val \le j < j'$ and $|(r_{q+1}-r_{q}-1) - b_{j:j'}| \le C_0 \log n \cdot \sqrt{b_{j:j'}}.$
    \If{no such $j, j'$ exist}
        \State \textbf{Return FAIL}
    \EndIf
    \State $\val \leftarrow j'$
    \State $q \leftarrow q + 1$
    \EndWhile
\If{$\val = m$}
    \State \textbf{Return} $(q, r_{q+1}-r_{q}-1)$.
\Else
    \State \textbf{Return FAIL}
\EndIf
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{lemma} \label{lem:crude-main-analysis}
    Fix $b_0, \dots, b_{m-1}$ such that $|b_i-(1-\delta) a_i| \le 10 \sqrt{a_i}$ for all $0 \le i \le m-1$. With probability at least $0.98$ over the randomness of $\tilde{x}$, we have that \Cref{alg:findmthone} returns $q$ such that the $q$th $1$ in $\tilde{x}$ corresponds to the $m$th $1$ in $x$. Moreover, conditioned on this event holding, the distribution $r_{q+1}-r_q-1$ exactly follows $\Bin(a_m, 1-\delta)$.
\end{lemma}

\begin{proof}\textcolor{red}{TOPROVE 7}\end{proof}

Given this, we can crudely estimate every gap, in order. Namely, assuming that that we have estimates $b_0, \dots, b_{m-1}$ (where $0 \le m \le t$), we can run the \textsc{Align} procedure on $O(\log n)$ independent traces. By a Chernoff bound, with $\frac{1}{n^{15}}$ failure probability, at least $0.9$ fraction of the traces will have the desired property of \Cref{lem:crude-main-analysis}, so will output some $(q, b)$ where $b \sim \Bin(a_m, 1-\delta)$. Since $\Bin(a_m, 1-\delta)$ is in the range $a_m(1-\delta) \pm 10 \sqrt{a_m}$ with at least $0.99$ probability, at least $0.75$ fraction of the outputs $(q, b)$ will satisfy $|b - (1-\delta) a_m| \le 10 \sqrt{a_m}$, with $\frac{1}{n^{15}}$ failure probability. Thus, by defining $b_m$ to be the median value of $b$ across the randomly drawn traces, we have that $|b_m-(1-\delta) a_m| \le 10 \sqrt{a_m}$ with at least $1 - \frac{1}{n^{10}}$ probability.

By running this procedure iteratively to provide estimates $b_0, b_1, \dots, b_{t}$, we obtain Algorithm~\ref{alg:crude}. The analysis in the above paragraph implies the following result.

\begin{theorem}[Crude Approximation] \label{thm:crude}
    Algorithm~\ref{alg:crude} uses $O(n \log n)$ traces and polynomial time, and learns estimates $b_0, b_1, \dots, b_{t}$ such that with at least $1 - \frac{1}{n^9}$ probability, $|b_m-(1-\delta) a_m| \le 10 \sqrt{a_m}$ for all $0 \le m \le t$.
\end{theorem}

\begin{algorithm}
\caption{Crude Estimation of all gaps}
\label{alg:crude}
\begin{algorithmic}[1]
\Procedure{Crude}{}
\State Use $O(n \log n)$ traces to compute $t$, where $t$ equals the number of $0$s in $x$.
\For{$m = 0$ to $t$}
    \For{$i = 1$ to $O(\log n)$}
        \State Draw trace $\tilde{x}^{(i)}$.
        \State $(q^{(i)}, b^{(i)}) \leftarrow \textsc{Align}(\tilde{x}^{(i)}, t, m, b_0, \dots, b_{m-1})$
    \EndFor
    \State Let $b_m$ be the median of $b^{(1)}, \dots, b^{(O(\log n))}$ \Comment{Some of the outputs $(q^{(i)}, b^{(i)})$ may be \textbf{FAIL}, but we can let $b^{(i)}$ be an arbitrary real number if \textsc{Align} failed on $\tilde{x}^{(i)}$, so that the median is well-defined.}
\EndFor
\State \textbf{Return} $(b_0, b_1, \dots, b_{t})$
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{Fine estimation}

In this section, we show how to exactly compute each $a_m$ with high probability, given the crude estimates $b_0, b_1, \dots, b_{t-1}$. This will again be done using an alignment procedure, but this time running the alignment both ``forward and backward''.

Namely, given a trace $\tilde{x}$, we will try to identify the $m$th and $(m+1)$st $1$'s from the original string, but we try to identify the $m$th $1$ by running \textsc{Align} on $\tilde{x}$ and the $(m+1)$st $1$ by running \textsc{Align} on the reverse string $\rev(\tilde{x}) := \tilde{x}_{|\tilde{x}|} \cdots \tilde{x}_2 \tilde{x}_1$. The idea is: assuming that we never go ahead in the alignment procedure, if we find some index $q$ in the forward alignment procedure with $g_q = f_{i_q} = m$, then the true position $i_q$ must be at least $m$. Likewise, if we do the alignment procedure in reverse until we believe we have found the $(t-m)$th $1$ from the back (equivalently, the $(m+1)$th $1$ from the front), the true position must be at most $m+1$.

So, the true positions of the index found in the forward alignment procedure can only be earlier than that of the index from the backward alignment procedure, if the true positions were exactly $m$ and $m+1$, respectively. Thus, by comparing the indices, we can effectively verify that the positions are correct, with negligible failure probability (rather than with $1-O(\delta)$ failure probability). This is the key towards obtaining the fine estimate of $a_m$, rather than just a coarse estimate that may be off by $O(\sqrt{a_m})$. 

Algorithm~\ref{alg:fine} formally describes the fine alignment procedure, using $N = O(n \log n)$ traces, assuming we have already done the coarse estimation to find $b_0, b_1, \dots, b_t$. 

\begin{algorithm}
\caption{Fine Estimation of all gaps}
\label{alg:fine}
\begin{algorithmic}[1]
\Procedure{Fine}{$t, b_0, \dots, b_t$}
\State Draw $N = O(n \log n)$ traces $\tilde{x}^{(1)},\dots,\tilde{x}^{(N)}$.
\For{$m = 0$ to $t$}
    \State Initialize $b^{(1)}, b^{(2)}, \dots, b^{(N)} \leftarrow \textbf{NULL}$.
    \For{$i = 1$ to $N$}
        \State $\tilde{m} \leftarrow$ number of $1$'s in $\tilde{x}$.
        \State $(q_{\text{f}}, b_{\text{f}}) \leftarrow \textsc{Align}(\tilde{x}^{(i)}, t, m, b_0, b_1 \dots, b_t)$.
        \State $(q_{\text{b}}, b_{\text{b}}) \leftarrow \textsc{Align}(\rev(\tilde{x}^{(i)}), t, t-m, b_t, b_{t-1}, \dots, b_0)$.
        \If{$q_{\text{f}} + q_{\text{b}} = \tilde{m}$}
            \State $b^{(i)} \leftarrow b_{\text{f}}$
        \EndIf
        \State Set $a_m$ to be $\frac{1}{1-\delta}$ times the average of all non-null $b^{(i)}$'s, rounded to the nearest integer.
    \EndFor
\EndFor
\State \textbf{Return} $(a_0, a_1, \dots, a_{t})$
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{lemma} \label{lem:word-pushing}
    Suppose that $|b_i-(1-\delta) a_i| \le 10 \sqrt{a_i}$ for all $1 \le 0 \le t$. Fix indices $0 \le m \le t$ and $1 \le i \le N$, and for simplicity of notation, let $\tilde{x} := \tilde{x}^{(i)}$. Let $\tilde{m}$ be the number of $1$'s in $\tilde{x}$. Then, the probability that $q_{\text{f}}+q_{\text{b}} = \tilde{m}$, but either the forward or backward iterations finds an index in $\tilde{x}$ which does not correspond to the $m$th $1$ or $(m+1)$th $1$, respectively, from $x$, is at most $2n^{-10}$. Moreover, if the forward and backward iterations find indices in $\tilde{x}$ corresponding to the $m$th $1$ and $(m+1)$th $1$, respectively, then $q_{\text{f}}+q_{\text{b}} = \tilde{m}$. Finally, the probability of finding both corresponding indices is at least $0.98$.
\end{lemma}

\begin{proof}\textcolor{red}{TOPROVE 8}\end{proof}

We are now ready to prove \Cref{thm:main}. Indeed, given the accuracy of the crude estimation procedure, it suffices to check that for each $m$, we compute $a_m$ correctly, with at least $1 - n^{-5}$ probability.

\begin{theorem}[Fine Estimation] \label{thm:fine}
    Assume that $t$, the number of ones in $x$, is computed correctly, and for all $0 \le m \le t$, $|b_m-(1-\delta) a_m| \le 10 \sqrt{a_m}$.

    Then, for any fixed $m: 0 \le m \le t$, with at least $1 - n^{-5}$ probability, we compute the gap $a_m$ correctly.
\end{theorem}

\begin{proof}\textcolor{red}{TOPROVE 9}\end{proof}



\bibliographystyle{alpha}
\bibliography{biblio}


\end{document}
