\NeedsTeXFormat{LaTeX2e}
\documentclass[10pt,reqno]{amsart}
\usepackage{hyperref}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{enumerate}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{latexsym}
\usepackage{fixmath}
\usepackage[usenames,dvipsnames]{color}
\usepackage{mathdots}
\usepackage{multicol}
\usepackage{cleveref}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{subcaption} \usepackage[boxed, linesnumbered, noend, noline]{algorithm2e}
\usepackage{comment}
\usepackage{float}


\usepackage[T1]{fontenc}
\usepackage{fourier}
\usepackage{bbm}
\usepackage{color}
\usepackage{fullpage}


\newcommand\aco[1]{\textcolor{red}{#1}}
\newcommand\lk[1]{\textcolor{magenta}{#1}}
\newcommand\mk[1]{\textcolor{cyan}{#1}}
\newcommand\achat[1]{\textcolor{ForestGreen}{#1}}

\renewcommand\aco[1]{#1}
\renewcommand\lk[1]{#1}
\renewcommand\mk[1]{#1}
\renewcommand\achat[1]{#1}

\newcommand\mr[1]{\textcolor{Goldenrod}{#1}}

\numberwithin{equation}{section}

\newcommand{\ism}{\cong}
\newcommand{\eqsig}{\stackrel{\times}{=}}
\newcommand{\lastar}{\lambda^*}

	\newcommand{\pasti}{\fF_{t_i}}
	\newcommand{\pastim}{\fF_{t_i-1}}
	\newcommand{\pasthm}{\fF_{t_h-1}}
\newcommand{\tox}{\cC}
\newcommand{\toxlt}{\tox}
\newcommand{\toxl}{\tox}
\newcommand{\errt}{\vec\rho_{t}}
\newcommand{\everrt}{\cR_{t}}
\newcommand{\everrti}{\cR_{t_i}}
\newcommand{\everrth}{\cR_{t_h}}
\newcommand{\RTS}{Ricci-Tersenghi and Semerjian}
\renewcommand{\ln}{\log}
\renewcommand{\vec}[1]{\boldsymbol{#1}}
\renewcommand{\Im}{\mathrm{im}}
\renewcommand{\subset}{\subseteq}

\newcommand\dmin{d_{\mathrm{min}}}
\newcommand\dcore{d_{\mathrm{core}}}
\newcommand\dsat{d_{\mathrm{sat}}}
\newcommand{\formula}{\PHI}
\newcommand{\FBP}[1]{\PHI_{\mathrm{BP},{#1}}}
\newcommand{\FUC}[1]{\PHI_{\mathrm{UC},{#1}}}
\newcommand{\FDC}[1]{\PHI_{\mathrm{DC},{#1}}}
\newcommand{\ADC}[1]{\vA_{#1}}
\newcommand{\amax}{\alpha_{\max}}
\newcommand{\lcond}{\lambda_{\mathrm{cond}}}
\newcommand{\tcond}{\theta_{\mathrm{cond}}}
\newcommand{\frz}{V_0}
\newcommand{\trans}{\top}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\frozen}{\mathtt{f}}
\newcommand{\unfrozen}{\mathtt{u}}
\newcommand{\nll}{\mathtt{n}}
\newcommand{\fzn}{\frozen}
\newcommand{\uzn}{\unfrozen}
\newcommand{\zero}{\mathtt{\emptyset}}
\newcommand\sm{\setminus}
\newcommand\disteq{\stacksign{dist}{=}}
\newcommand\myfrac[2]{#1\bigg/#2}
\newcommand{\BP}{\ensuremath{\mathtt{BP}}}
\newcommand{\WP}{\ensuremath{\mathtt{WP}}}
\newcommand{\BPGD}{\ensuremath{\mathtt{BPGD}}}
\newcommand{\UCP}{\ensuremath{\mathtt{UCP}}}
\newcommand{\SPGD}{\ensuremath{\mathtt{SPGD}}}
\newcommand\CPC{Combinatorics, Probability and Computing}
\newcommand{\GG}{\mathbb G}
\newcommand{\TT}{\mathbb T}
\newcommand{\HH}{\mathbb H}
\newcommand\ALPHA{\vec\alpha}
\newcommand\BETA{\vec\beta}
\newcommand\MU{\vec\mu}
\newcommand\ETA{\vec\eta}
\newcommand\NU{\vec\nu}
\newcommand\hG{\hat\G}
\newcommand\CHI{{\vec\chi}}
\newcommand\DELTA{{\vec\Delta}}
\newcommand\GAMMA{{\vec\gamma}}
\newcommand\G{\vec G}
\newcommand\T{\vec T}
\newcommand\OMEGA{\vec\omega}
\newcommand\PSI{\vec\psi}
\newcommand\RHO{{\vec\rho}}
\newcommand\PHI{\vec F}
\newcommand\nix{\,\cdot\,}
\newcommand\dd{\partial}
\newcommand\bem{\bf\em}
\newcommand\bemph[1]{{\bf\em #1}}
\newcommand\KL[2]{D_{\mathrm{KL}}\bc{{{#1}\|{#2}}}}
\newcommand\SIGMA{\vec\sigma}
\newcommand\SIGBP{\vec\sigma_{\mathrm{BP}}}
\newcommand\SIGUC{\vec\sigma_{\mathrm{UC}}}
\newcommand\SIGDEC{\vec\sigma_{\mathrm{DC}}}
\newcommand\SIGDC{\SIGDEC}
\newcommand\TAU{\vec\tau}
\newcommand\cA{\mathcal A}
\newcommand\cB{\mathcal B}
\newcommand\cC{\mathcal C}
\newcommand\cD{\mathcal D}
\newcommand\cE{\mathcal E}
\newcommand\cF{\mathcal F}
\newcommand\cG{\mathcal G}
\newcommand\cH{\mathcal H}
\newcommand\cI{\mathcal I}
\newcommand\cJ{\mathcal J}
\newcommand\cK{\mathcal K}
\newcommand\cL{\mathcal L}
\newcommand\cM{\mathcal M}
\newcommand\cN{\mathcal N}
\newcommand\cO{\mathcal O}
\newcommand\cP{\mathcal P}
\newcommand\cQ{\mathcal Q}
\newcommand\cR{\mathcal R}
\newcommand\cS{\mathcal S}
\newcommand\cT{\mathcal T}
\newcommand\cU{\mathcal U}
\newcommand\cV{\mathcal V}
\newcommand\cW{\mathcal W}
\newcommand\cX{\mathcal X}
\newcommand\cY{\mathcal Y}
\newcommand\cZ{\mathcal Z}
\newcommand\fA{\mathfrak A}
\newcommand\fB{\mathfrak B}
\newcommand\fC{\mathfrak C}
\newcommand\fD{\mathfrak D}
\newcommand\fE{\mathfrak E}
\newcommand\fF{\mathfrak F}
\newcommand\fG{\mathfrak G}
\newcommand\fH{\mathfrak H}
\newcommand\fI{\mathfrak I}
\newcommand\fJ{\mathfrak J}
\newcommand\fK{\mathfrak K}
\newcommand\fL{\mathfrak L}
\newcommand\fM{\mathfrak M}
\newcommand\fN{\mathfrak N}
\newcommand\fO{\mathfrak O}
\newcommand\fP{\mathfrak P}
\newcommand\fQ{\mathfrak Q}
\newcommand\fR{\mathfrak R}
\newcommand\fS{\mathfrak S}
\newcommand\fT{\mathfrak T}
\newcommand\fU{\mathfrak U}
\newcommand\fV{\mathfrak V}
\newcommand\fW{\mathfrak W}
\newcommand\fX{\mathfrak X}
\newcommand\fY{\mathfrak Y}
\newcommand\fZ{\mathfrak Z}
\newcommand\fa{\mathfrak a}
\newcommand\fb{\mathfrak b}
\newcommand\fc{\mathfrak c}
\newcommand\fd{\mathfrak d}
\newcommand\fe{\mathfrak e}
\newcommand\ff{\mathfrak f}
\newcommand\fg{\mathfrak g}
\newcommand\fh{\mathfrak h}
\newcommand\fj{\mathfrak j}
\newcommand\fk{\mathfrak k}
\newcommand\fl{\mathfrak l}
\newcommand\fm{\mathfrak m}
\newcommand\fn{\mathfrak n}
\newcommand\fo{\mathfrak o}
\newcommand\fp{\mathfrak p}
\newcommand\fq{\mathfrak q}
\newcommand\fr{\mathfrak r}
\newcommand\fs{\mathfrak s}
\newcommand\ft{\mathfrak t}
\newcommand\fu{\mathfrak u}
\newcommand\fv{\mathfrak v}
\newcommand\fw{\mathfrak w}
\newcommand\fx{\mathfrak x}
\newcommand\fy{\mathfrak y}
\newcommand\fz{\mathfrak z}
\newcommand\vA{\vec A}
\newcommand\vB{\vec B}
\newcommand\vC{\vec C}
\newcommand\vD{\vec D}
\newcommand\vE{\vec E}
\newcommand\vF{\vec F}
\newcommand\vG{\vec G}
\newcommand\vH{\vec H}
\newcommand\vI{\vec I}
\newcommand\vJ{\vec J}
\newcommand\vK{\vec K}
\newcommand\vL{\vec L}
\newcommand\vM{\vec M}
\newcommand\vN{\vec N}
\newcommand\vO{\vec O}
\newcommand\vP{\vec P}
\newcommand\vQ{\vec Q}
\newcommand\vR{\vec R}
\newcommand\vS{\vec S}
\newcommand\vT{\vec T}
\newcommand\vU{\vec U}
\newcommand\vV{\vec V}
\newcommand\vW{\vec W}
\newcommand\vX{\vec X}
\newcommand\vY{\vec Y}
\newcommand\vZ{\vec Z}
\newcommand\va{\vec a}
\newcommand\vb{\vec b}
\newcommand\vc{\vec c}
\newcommand\vd{\vec d}
\newcommand\ve{\vec e}
\newcommand\vf{\vec f}
\newcommand\vg{\vec g}
\newcommand\vh{\vec h}
\newcommand\vi{\vec i}
\newcommand\vj{\vec j}
\newcommand\vk{\vec k}
\newcommand\vl{\vec l}
\newcommand\vm{\vec m}
\newcommand\vn{\vec n}
\newcommand\vo{\vec o}
\newcommand\vp{\vec p}
\newcommand\vq{\vec q}
\newcommand\vr{\vec r}
\newcommand\vs{\vec s}
\newcommand\vt{\vec t}
\newcommand\vu{\vec u}
\newcommand\vv{\vec v}
\newcommand\vw{\vec w}
\newcommand\vx{\vec x}
\newcommand\vy{\vec y}
\newcommand\vz{\vec z}
\def\bC{{\bf C}}
\def\bT{{\bf T}}
\def\bM{{\bf M}}
\newcommand\atom{\delta}
\newcommand\contig{\triangleleft}
\newcommand\thet{\vartheta}
\newcommand\eul{\mathrm{e}}
\newcommand\eps{\varepsilon}
\newcommand\del{\delta}
\renewcommand\AA{\mathbb{A}}
\newcommand\BB{\mathbb{B}}
\newcommand\QQ{\mathbb{Q}}
\newcommand\ZZ{\mathbb{Z}}
\newcommand\FF{\mathbb{F}}
\newcommand\field{\FF}
\newcommand\NN{\mathbb{N}}
\newcommand\ZZpos{\mathbb{Z}_{\geq0}}
\newcommand\Var{\mathrm{Var}}
\newcommand\Cov{\mathrm{Cov}}
\newcommand\Erw{\mathbb{E}}
\newcommand\ex{\Erw}
\newcommand{\vecone}{\mathbb{1}}
\newcommand{\Vol}{\mathrm{Vol}}
\newcommand{\Po}{{\rm Po}}
\newcommand{\Bin}{{\rm Bin}}
\newcommand{\Mult}{{\rm Mult}}
\newcommand{\Be}{{\rm Be}}
\newcommand\TV[1]{\left\|{#1}\right\|_{\mathrm{TV}}}
\newcommand\tv[1]{\|{#1}\|_{\mathrm{TV}}}
\newcommand\dTV{d_{\mathrm{TV}}}
\newcommand{\bink}[2] {{\binom{#1}{#2}}}
\newcommand\bc[1]{\left({#1}\right)}
\newcommand\cbc[1]{\left\{{#1}\right\}}
\newcommand\bcfr[2]{\bc{\frac{#1}{#2}}}
\newcommand{\bck}[1]{\left\langle{#1}\right\rangle}
\newcommand\brk[1]{\left\lbrack{#1}\right\rbrack}
\newcommand\scal[2]{\bck{{#1},{#2}}}
\newcommand\norm[1]{\left\|{#1}\right\|}
\newcommand\abs[1]{\left|{#1}\right|}
\newcommand\uppergauss[1]{\left\lceil{#1}\right\rceil}
\newcommand\lowergauss[1]{\left\lfloor{#1}\right\rfloor}
\newcommand\ug[1]{\left\lceil{#1}\right\rceil}
\newcommand\RR{\mathbb{R}}
\newcommand\RRpos{\RR_{\geq0}}
\newcommand{\Whp}{W.h.p.}
\newcommand{\whp}{w.h.p.}
\newcommand{\wupp}{w.u.p.p.}
\newcommand{\stacksign}[2]{{\stackrel{\mbox{\scriptsize #1}}{#2}}}
\newcommand{\tensor}{\otimes}
\newcommand{\Karonski}{Karo\'nski}
\newcommand{\Erdos}{Erd\"os}
\newcommand{\Renyi}{R\'enyi}
\newcommand{\Lovasz}{Lov\'asz}
\newcommand{\Juhasz}{Juh\'asz}
\newcommand{\Bollobas}{Bollob\'as}
\newcommand{\Furedi}{F\"uredi}
\newcommand{\Komlos}{Koml\'os}
\newcommand{\Luczak}{Luczak}
\newcommand{\Mezard}{M\'ezard}
\newcommand{\Szemeredi}{Szemer\'edi}
\newcommand{\Faadi}{Fa\`a di Bruno}
\newcommand\pr{\mathbb{P}} 
\renewcommand\Pr{\pr} 
\newcommand\Lem{Lemma}
\newcommand\Prop{Proposition}
\newcommand\Thm{Theorem}
\newcommand\Def{Definition}
\newcommand\Cor{Corollary}
\newcommand\Sec{Section}
\newcommand\Chap{Chapter}
\newcommand\Rem{Remark}
\newcommand\algstyle{\small\sffamily}
\newcommand\id{\mathrm{id}}

\newtheorem{definition}{Definition}[section]
\newtheorem{claim}[definition]{Claim}
\newtheorem{example}[definition]{Example}
\newtheorem{remark}[definition]{Remark}
\newtheorem{theorem}[definition]{Theorem}
\newtheorem{lemma}[definition]{Lemma}
\newtheorem{proposition}[definition]{Proposition}
\newtheorem{corollary}[definition]{Corollary}
\newtheorem{Algo}[definition]{Algorithm}
\newtheorem{fact}[definition]{Fact}
\newtheorem{hypothesis}[definition]{Hypothesis}
\newtheorem{experiment}[definition]{Experiment}
\newtheorem{conjecture}[definition]{Conjecture}

\DeclareMathOperator{\vol}{vol}
\DeclareMathOperator{\Tr}{tr}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\nul}{nul}
\DeclareMathOperator{\rank}{rk}
\DeclareMathOperator{\rate}{rate}
\DeclareMathOperator{\sign}{sign}

\newcommand{\floor}[1]{\left\lfloor#1\right\rfloor}
\newcommand{\ceil}[1]{\left\lceil#1\right\rceil}
\newcommand{\rk}{\rank}
\def\bin{{\bf Bin}}
\newcommand{\supp}{{\mathrm{supp}}}
\newcommand{\ind}[1]{{\pmb 1}\{#1\}}
\def\geo{{\bf Geo}}
\def\con{{\bf Con}}

\newcommand\A{\vA}

\def\B{{\mathcal B}}
\def\C{{\mathcal C}}
\def\D{{\mathcal D}}
\def\E{{\mathcal E}}
\def\F{{\mathcal F}}
\def\H{{\mathcal H}}
\def\I{{\mathcal I}}
\def\J{{\mathcal J}}
\def\K{{\mathcal K}}
\def\L{{\mathcal L}}
\def\M{{\mathcal M}}
\def\N{{\mathcal N}}
\def\O{{\mathcal O}}
\def\P{{\mathcal P}}
\def\Q{{\mathcal Q}}
\def\R{{\mathcal R}}
\def\S{{\mathcal S}}
\def\T{{\mathcal T}}
\def\U{{\mathcal U}}
\def\V{{\mathcal V}}
\def\W{{\mathcal W}}
\def\X{{\mathcal X}}
\def\Y{{\mathcal Y}}
\def\Z{{\mathcal Z}}

\def\po{{\bf Po}}
\def\pr{{\mathbb P}}
\newcommand\expc[1]{{\left< #1\right>}}

\def\bfa{{\bf a}}
\def\bfb{{\bf b}}
\def\bfc{{\bf c}}
\def\bfd{{\vec d}}
\def\bfe{{\bf e}}
\def\bff{{\bf f}}
\def\bfg{{\bf g}}
\def\bfh{{\bf h}}
\def\bfi{{\bf i}}
\def\bfj{\vec j}
\def\bfk{\vk}
\def\bfl{{\bf l}}
\def\bfm{{\vec m}}
\def\bfn{{\vec n}}
\def\bfo{{\bf o}}
\def\bfp{{\bf p}}
\def\bfq{{\bf q}}
\def\bfr{{\bf r}}
\def\bfs{{\bf s}}
\def\bft{{\bf t}}
\def\bfu{{\bf u}}
\def\bfv{{\bf v}}
\def\bfw{{\bf w}}
\def\bfx{{\bf x}}
\def\bfy{{\bf y}}
\def\bfz{{\bf z}}
\def\bfA{{\bf A}}
\def\bfB{{\bf B}}
\def\bfC{{\bf C}}
\def\bfD{{\bf D}}
\def\bfE{{\bf E}}
\def\bfF{{\bf F}}
\def\bfG{{\bf G}}
\def\bfH{{\bf H}}
\def\bfI{{\bf I}}
\def\bfJ{{\bf J}}
\def\bfK{{\bf K}}
\def\bfL{{\bf L}}
\def\bfM{{\bf M}}
\def\bfN{{\bf N}}
\def\bfO{{\bf O}}
\def\bfP{{\bf P}}
\def\bfQ{{\bf Q}}
\def\bfR{{\bf R}}
\def\bfS{{\bf S}}
\def\bfT{{\bf T}}
\def\bfU{{\bf U}}
\def\bfV{{\bf V}}
\def\bfW{{\bf W}}
\def\bfX{{\bf X}}
\def\bfY{{\bf Y}}
\def\bfZ{{\bf Z}}
\def\cH{{\mathcal H}}

\newcommand{\la}{\lambda}
\newcommand{\ph}{\phi_{d,k,\lambda}}
\newcommand{\Ph}{\Phi_{d,k,\lambda}}
\newcommand{\zt}{\zeta_\lambda}
\newcommand{\ztt}[1]{\zeta_{#1}}
\newcommand{\expld}{\exp(-\lambda-d z^{k-1})}
\renewcommand{\aa}{\alpha_\ast}
\newcommand{\ab}{\alpha^{\ast}}
\newcommand{\dstar}{d^*}
\newcommand{\zstar}{z^*}
\newcommand{\lstar}{\lambda^*}
\newcommand{\set}[1]{ \left\{ {#1} \right\} }

\makeatletter
\newcommand{\definetitlefootnote}[1]{\newcommand\addtitlefootnote{\makebox[0pt][l]{$^{*}$}\footnote{\protect\@titlefootnotetext}
	}\newcommand\@titlefootnotetext{\spaceskip=\z@skip $^{*}$#1}}
\makeatother

\begin{document}
\bibliographystyle{plainurl}
\definetitlefootnote{An Extended abstract appeared in the proceedings of ICALP 2025}
\title{Belief Propagation guided decimation on random $k$-XORSAT\addtitlefootnote}
\author{Arnab Chatterjee, Amin Coja-Oghlan, Mihyun Kang, Lena Krieg, Maurice Rolvien, Gregory B. Sorkin}

\address{Arnab Chatterjee, {\tt arnab.chatterjee@tu-dortmund.de}, TU Dortmund, Faculty of Computer Science, 12 Otto-Hahn-St, 44227 Dortmund, Germany.}
\address{Amin Coja-Oghlan, {\tt amin.coja-oghlan@tu-dortmund.de}, TU Dortmund, Faculty of Computer Science and Faculty of Mathematics, 12 Otto-Hahn-St, 44227 Dortmund, Germany.}
\address{Mihyun Kang, {\tt kang@math.tugraz.at}, TU Graz, Institute of Discrete Mathematics, Steyrergasse 30, 8010 Graz, Austria.}
\address{Lena Krieg, {\tt lena.krieg@tu-dortmund.de}, TU Dortmund, Faculty of Computer Science, 12 Otto-Hahn-St, 44227 Dortmund, Germany.}
\address{Maurice Rolvien, {\tt maurice.rolvien@tu-dortmund.de}, University of Hamburg, Department of Informatics, Vogt-Kölln-Str. 30, 22527 Hamburg, Germany.}
\address{Gregory B. Sorkin, {\tt g.b.sorkin@lse.ac.uk}, The London School of Economics and Political Science, Department of Mathematics, Columbia House, Houghton St, London WC2A 2AE, United Kingdom}
\maketitle
\begin{abstract}
	We analyse the performance of {\em Belief Propagation Guided Decimation}, a physics-inspired message passing algorithm, on the random $k$-XORSAT problem.
	Specifically, we derive an explicit threshold up to which the algorithm succeeds with a strictly positive probability $\Omega(1)$ that we compute explicitly, but beyond which the algorithm with high probability fails to find a satisfying assignment.
	In addition, we analyse a thought experiment called the {\em decimation process} for which we identify a (non-)reconstruction and a condensation phase transition.
	The main results of the present work confirm physics predictions from [\RTS: J.\ Stat.\ Mech.\ 2009] that link the phase transitions of the decimation process with the performance of the algorithm, and improve over partial results from a recent article [Yung: Proc.\ ICALP 2024].
\hfill MSc: 60B20, 68W20
\end{abstract}
\section{Introduction and results}\label{Sec_not_intro}

\subsection{Background and motivation}\label{sec_background}
The random $k$-XORSAT problem shares many characteristics of other intensely studied random constraint satisfaction problems (`CSPs') such as random $k$-SAT.
For instance, as the clause/variable density increases, random $k$-XORSAT possesses a sharp satisfiability threshold preceded by a reconstruction or `shattering' phase transition that affects the geometry of the set of solutions~\cite{AchlioptasMolloy,DuboisMandler,pnas,PittelSorkin}.
As in random $k$-SAT, these transitions appear to significantly impact the performance of certain classes of algorithms~\cite{BetterAlg,Ibrahimi}.
At the same time, random $k$-XORSAT is more amenable to mathematical analysis than, say, random $k$-SAT.
This is because the XOR operation is equivalent to addition modulo two, which is why a $k$-XORSAT instance translates into a linear system over $\FF_2$.
In effect, $k$-XORSAT can be solved in polynomial time by means of Gaussian elimination.
In addition, the algebraic nature of the problem induces strong symmetry properties that simplify its study~\cite{Ayre}.

Because of its similarities with other random CSPs combined with said relative amenability, random $k$-XORSAT provides an instructive benchmark.
This was noticed not only in combinatorics, but also in the statistical physics community, which has been contributing intriguing `predictions' on random CSPs since the early 2000s~\cite{MM,MRTZ}.
Among other things, physicists have proposed a message passing algorithm called {\em Belief Propagation Guided Decimation} (`\BPGD') that, according to computer experiments, performs impressively on various random CSPs~\cite{MPZ}.
Furthermore, \RTS~\cite{RTS} put forward a heuristic analysis of \BPGD\ on random $k$-SAT and $k$-XORSAT.
Their heuristic analysis proceeds by way of a thought experiment based on an idealised version of the algorithm.
We call this thought experiment the {\em decimation process}.
Based on physics methods \RTS\ surmise that the decimation process undergoes two phase transitions, specifically a reconstruction and a condensation transition.
A key prediction of \RTS\ is that these phase transitions are directly linked to the performance of the \BPGD\ algorithm.
Due to the linear algebra-induced symmetry properties, in the case of random $k$-XORSAT all of these conjectures come as elegant analytical expressions.

The aim of this paper is to verify the predictions from~\cite{RTS} on random $k$-XORSAT mathematically.
Specifically, our aim is to rigorously analyse the \BPGD\ algorithm on random $k$-XORSAT, and to establish the link between its performance and the phase transitions of the decimation process.
A first step towards a rigorous analysis of \BPGD\ on random $k$-XORSAT was undertaken in a recent contribution by Yung~\cite{Yung}.
However, Yung's analysis turns out to be not tight.
Specifically, apart from requiring spurious lower bounds on the clause length $k$, Yung's results do not quite establish the precise connection between the decimation process and the performance of \BPGD.
One reason for this is that~\cite{Yung} relies on `annealed' techniques, i.e., essentially moment computations.
Here we instead harness `quenched' arguments that were partly developed in prior work on the rank of random matrices over finite fields~\cite{Ayre,Maurice}.

Throughout we let $k\geq3$ and $n\geq k$ be integers and $d>0$ a positive real.
Let $\vm\disteq\Po(dn/k)$ and let $\PHI=\PHI(n,d,k)$ be a random $k$-XORSAT formula\footnote{
		Two random variables $\vX, \vY$ are equal in distribution $\vX\disteq \vY$ if they have the same distribution functions. 
		Thus, $\vm$ follows a Poisson distribution with mean $dn/k$.}
	with variables $x_1,\ldots,x_n$ and $\vm$ random clauses of length $k$.
	
To be precise, every clause of $\PHI$ is an XOR of precisely $k$ distinct variables, each of which may or may not come with a negation sign.
The $\vm$ clauses are drawn uniformly and independently out of the set of all $2^k\binom nk$ possibilities.
Thus, $d$ equals the average number of clauses that a given variable $x_i$ appears in.
An event $\cE$ occurs \emph{with high probability} (`\whp') if $\lim_{n\to\infty}\pr\brk{\cE}=1$.
We always keep $d,k$ fixed as $n\to\infty$.


\subsection{Belief Propagation Guided Decimation}\label{sec_bpgd}
The first result vindicates the predictions from~\cite{RTS} concerning the success probability of \BPGD\ algorithm.
\BPGD\ sets its ambitions higher than merely finding a solution to the $k$-XORSAT instance $\PHI$: the algorithm attempts to sample a solution uniformly at random.
To this end \BPGD\ assigns values to the variables $x_1,\ldots,x_n$ of $\PHI$ one after the other.
In order to assign the next variable the algorithm attempts to compute the marginal probability that the variable is set to `true' under a random solution to the $k$-XORSAT instance, given all previous assignments.
More precisely, suppose \BPGD\ has assigned values to the variables $x_1,\ldots,x_t$ already.
Write $\SIGBP(x_1),\ldots,\SIGBP(x_t)\in\{0,1\}$ for their values, with $1$ representing `true' and $0$ `false'.
Further, let $\FBP{t}$ be the simplified formula obtained by substituting $\SIGBP(x_1),\ldots,\SIGBP(x_t)$ for $x_1,\ldots,x_t$.
We drop any clauses from $\FBP{t}$ that contain variables from $\{x_1,\ldots,x_t\}$ only, deeming any such clauses satisfied.
Thus, $\FBP{t}$ is a XORSAT formula with variables $x_{t+1},\ldots,x_n$.
Its clauses contain at least one and at most $k$ variables, as well as possibly a constant (the XOR of the values substituted in for $x_1,\ldots,x_t$).

Let $\SIGMA_{\FBP{t}}$ be a uniformly random solution of the XORSAT formula $\FBP{t}$, assuming that $\FBP{t}$ remains satisfiable.
Then \BPGD\ aims to compute the marginal probability $\pr\brk{\SIGMA_{\FBP{t}}(x_{t+1})=1\mid\FBP{t}}$ that a random satisfying assignment of $\FBP{t}$ sets $x_{t+1}$  to true. 
This is where Belief Propagation (`BP') comes in.
An efficient message passing heuristic for computing precisely such marginals, BP returns an `approximation' $\mu_{\FBP{t}}$ of $\pr\brk{\SIGMA_{\FBP{t}}(x_{t+1})=1\mid\FBP{t}}$.
We will recap the mechanics of BP in \Sec~\ref{sec_bp} (the value $\mu_{\FBP{t}}$ is defined precisely in \eqref{eqmuBPGD}).
Having computed the BP `approximation', \BPGD\ proceeds to assign $x_{t+1}$ the value `true' with probability $\mu_{\FBP{t}}$, otherwise sets $x_{t+1}$ to `false', then moves on to the next variable.
The pseudocode is displayed as Algorithm~\ref{alg_bpgd}.

\begin{algorithm}[h!]
 \KwData{a random $k$-XORSAT formula $\PHI$ with variables $x_1,\ldots,x_n$ conditioned on being satisfiable} 
\For{$t=0,\ldots,n-1$}{compute the BP approximation $\mu_{\FBP{t}}$\;
		set
		$\SIGBP(x_{t+1})=\begin{cases}
			1&\mbox{with probability }\mu_{\FBP{t}}\\
			0&\mbox{with probability }1-\mu_{\FBP{t}}
					\end{cases}$\;
		}
	\Return $\SIGBP$\;
 \caption{The \BPGD\ algorithm.}\label{alg_bpgd}
 \label{Alg_SC}
\end{algorithm}

Let us pause for a few remarks.
First, if the BP approximations are exact, i.e., if $\FBP{t}$ is satisfiable and $\mu_{\FBP{t}}=\pr\brk{\SIGMA_{\FBP{t}}(x_{t+1})=1\mid\FBP{t}}$ for all $t$, then Bayes' formula shows that \BPGD\ outputs a uniformly random solution of $\PHI$.
However, there is no universal guarantee that BP returns the correct marginals.
Accordingly, the crux of analysing \BPGD\ is precisely to figure out whether this is the case.
Indeed, the heuristic work of~\cite{RTS} ties the accuracy of BP to a phase transition of the decimation process thought experiment, to be reviewed momentarily.

Second, the strategy behind the \BPGD\ algorithm, particularly the message passing heuristic for `approximating' the marginals, generalises well beyond $k$-XORSAT.
For instance, the approach applies to $k$-SAT verbatim.
That said, due to the algebraic nature of the XOR operation, \BPGD\ is {\em far} easier to analyse on $k$-XORSAT.
In fact, in XORSAT the marginal probabilities are guaranteed to be half-integral as seen in Fact \ref{fact_halfint}, i.e., 
	\begin{align}\label{eqHalfInt}
		\pr\brk{\SIGMA_{\FBP{t}}(x_{t+1})=1\mid\FBP{t}}\in\{0,1/2,1\}.
	\end{align}
As a consequence, on XORSAT the \BPGD\ algorithm effectively reduces to a purely combinatorial algorithm called Unit Clause Propagation~\cite{MM,RTS} as per \Prop\ \ref{prop_UCP}, a fact that we will exploit extensively \mk{(see \Sec\ \ref{sec_alg})}.

\subsection{A tight analysis of $\BPGD$}\label{sec_results}
In order to state the main results we need to introduce a few threshold values.
To this end, given $d,k$ and an additional real parameter $\lambda\geq0$ that depends on the time $t$, consider the functions
\footnote{The function $\Ph$ is known in physics parlance as the ``Bethe free entropy'' \cite{Maurice, MM}. The stationary points of $\Ph$ coincide with the fixed points of $\ph$, as we will verify in \Sec\ \ref{sec_calc}.}
\begin{align} \label{eqphi}
	\ph:&[0,1]\to[0,1],& z&\mapsto 1 - \exp\bc{-\lambda-dz^{k-1}},\\
	\Ph:&[0,1]\to\RR,&z &\mapsto \exp\bc{-\lambda-dz^{k-1}}-\frac{d(k-1)}kz^k+dz^{k-1}-\frac dk.\label{eqPhi}
\end{align}
Let $\alpha_*(\lambda)=\alpha_*(d,k,\lambda)\in[0,1]$ be the smallest and $\alpha^*(\lambda)=\alpha^*(d,k,\lambda)\geq\alpha_*(d,k,\lambda)\in[0,1]$ the largest fixed point of $\ph$.
Figure \ref{fig_rainbow} visualises $\Phi(z)$ for different values of $\theta \sim t/n$.
Further, define
\begin{align}\label{eqds} \dmin(k)&=\bcfr{k-1}{k-2}^{k-2},&
	\dcore(k)&=\sup\cbc{d>0:\alpha^*(0)=0},&
	\dsat(k)&=\sup\cbc{d>0:\Phi_{d,k,0}(\alpha^*(0))\leq\Phi_{d,k,0}(0)}.
\end{align}

The value $\dsat(k)$ is the random $k$-XORSAT satisfiability threshold~\cite{Ayre,DuboisMandler,PittelSorkin}.
Thus, for $d<\dsat(k)$ the random $k$-XORSAT formula $\PHI$ possesses satisfying assignments \whp, while $\PHI$ is unsatisfiable for $d>\dsat(k)$ \whp\
Furthermore, $\dcore(k)$ equals the threshold for the emergence of a giant 2-core within the $k$-uniform hypergraph induced by $\PHI$~\cite{Ayre,Molloy}.
This implies that for $d<\dcore(k)$ the set of solutions of $\PHI$ is contiguous in a certain well-defined way, while for $\dcore(k)<d<\dsat(k)$ the set of solutions shatters into an exponential number of well-separated clusters~\cite{Ibrahimi,MM}.
Moreover, a simple linear time algorithm is known to find a solution \whp\ for $d<\dcore(k)$~\cite{Ibrahimi}.
The relevance of $\dmin(k)$ will emerge in \Thm\ \ref{thm_bpgd}. 
A bit of calculus reveals that
\begin{align}\label{eqdrel}
	0<\dmin(k)<\dcore(k)<\dsat(k)<k.
\end{align}

The following theorem determines the precise clause-to-variable densities where \BPGD\ succeeds/fails.
To be precise, in the `successful' regime \BPGD\ does not actually succeed with {\em high} probability, but with an explicit probability strictly between zero and one, which is displayed in Figure~\ref{Psuccess} for $k=3,4,5$.

\noindent
\begin{minipage}{0.5\linewidth}
	\begin{figure}[H]
		\includegraphics[height=40mm]{./rainbow.pdf}
		\caption{$\Phi_{d,k,\lambda}$ for $k = 3$ and $d = 2.4$, for $\lambda$ from $\textcolor{red}{0}$ to $\textcolor{Dandelion}{0.3}$ (maximum at $z=0$) and from $\color{teal}{0.4}$ to $\textcolor{blue}{0.9}$   }\label{fig_rainbow}
	\end{figure}
\end{minipage}
\begin{minipage}{0.5\linewidth}
	\begin{figure}[H]
		\includegraphics[height=40mm]{./Psuccess.pdf}
		\caption{Success probability of \BPGD\ for $0<d<\dmin(k)$ and various $k$.}\label{Psuccess}
	\end{figure}
\end{minipage}







\begin{theorem}\label{thm_bpgd}
	Let $k\geq3$.
	\begin{enumerate}[(i)]
		\item If $d<\dmin(k)$, then 
			\begin{align}\label{eqthm_bpgd}
		\lim_{n\to\infty}\pr\brk{\BPGD(\PHI)\mbox{ finds a satisfying assignment}}&=
		\exp\bc{-\frac{d^2(k-1)^2}4\int_0^1\frac{z^{2k-4}(1-z)}{1-d(k-1)z^{k-2}(1-z)}\,\mathrm d z}.
	\end{align}
\item If $\dmin(k)<d<\dsat(k)$, then  
$$
	\pr\brk{\BPGD(\PHI)\mbox{ finds a satisfying assignment}}=o(1).$$
\end{enumerate}
\end{theorem}

\Thm~\ref{thm_bpgd} vindicates the predictions from \RTS~\cite[\Sec~4]{RTS} as to the performance of \BPGD, and improves over the results from Yung~\cite{Yung}.
Specifically, \Thm~\ref{thm_bpgd}~(i) verifies the formula for the success probability from~\cite[Eq.~(38)]{RTS}. 
Combinatorially, the formula \eqref{eqthm_bpgd} results from the possible presence of bounded length cycles (so-called toxic cycles) that may cause the algorithm to run into contradictions.
This complements Yung's prior work, that has no positive result on the performance of \BPGD. 
Moreover, Yung's negative results~\cite[\Thm s~2--3]{Yung} only apply to $k\geq9$ and to $d>\dcore(k)$, while \Thm~\ref{thm_bpgd}~(ii) covers all $k\geq3$ and kicks in at the correct threshold $\dmin(k)<\dcore(k)$ predicted in~\cite{RTS}.

\subsection{The decimation process}\label{sec_dc}
In addition to the \BPGD\ algorithm itself, the heuristic work~\cite{RTS} considers an idealised version of the algorithm, the {\em decimation process}.
This thought experiment highlights the conceptual reasons behind the success/failure of \BPGD.
Just like \BPGD, the decimation process assigns values to variables one after the other for good.
But instead of the BP `approximations' the decimation process uses the {\em actual} marginals given its previous decisions.
To be precise, suppose that the input formula $\PHI$ is satisfiable and that variables $x_1,\ldots,x_t$ have already been assigned values $\SIGDC(x_1),\ldots,\SIGDC(x_t)$ in the previous iterations.
Obtain $\FDC{t}$ by substituting the values $\SIGDC(x_1),\ldots,\SIGDC(x_t)$ for $x_1,\ldots,x_t$ and dropping any clauses that do not contain any of $x_{t+1},\ldots,x_n$.
Thus, $\FDC t$ is a XORSAT formula with variables $x_{t+1},\ldots,x_n$.
Let $\SIGMA_{\FDC{t}}$ be a random satisfying assignment of $\FDC{t}$.
Then the decimation process sets $x_{t+1}$ according to the true marginal $\pr\brk{\SIGMA_{\FDC{t}}(x_{t+1})=1\mid\FDC t}$, thus ultimately returning a uniformly random satisfying assignment of $\PHI$.

\begin{algorithm}[h!]
 \KwData{a random $k$-XORSAT formula $\PHI$, conditioned on being satisfiable}
\For{$t=0,\ldots,n-1$}{compute $\pi_{\FDC{t}}=\pr\brk{\SIGMA_{\FDC{t}}(x_{t+1})=1\mid\FDC t}$\;
		set
		$\SIGDC(x_{t})=\begin{cases}
			1&\mbox{with probability }\pi_{\FDC{t}}\\
			0&\mbox{with probability }1-\pi_{\FDC{t}}
					\end{cases}$\;
		}
	\Return $\SIGDC$\;
 \caption{The decimation process.}\label{alg_dec}
 \label{decimationprocess}
\end{algorithm}

Clearly, if indeed the BP `approximations' are correct, then the decimation process and \BPGD\ are identical.
Thus, a key question is for what parameter regimes the two process coincide or diverge, respectively.
As it turns out, this question is best answered by parametrize not only in terms of the average variable degree $d$, but also in terms of the `time' parameter $t$ of the decimation process.

\subsection{Phase transitions of the decimation process}\label{sec_results_dc}
\RTS\ heuristically identify several phase transitions in terms of $d$ and $t$ that the decimation process undergoes.
We will confirm these predictions mathematically and investigate how they relate to the performance of \BPGD.

The first set of relevant phase transitions concerns the so-called non-reconstruction property.
Roughly speaking, non-reconstruction means that the marginal $\pi_{\FDC{t}}=\pr\brk{\SIGMA_{\FDC{t}}(x_{t+1})=1\mid\FDC t}$ is determined by short-range rather than long-range effects.
Since Belief Propagation is essentially a local algorithm, one might expect that the (non-)reconstruction phase transition coincides with the threshold up to which \BPGD\ succeeds; cf.\ the discussions in~\cite{Braunstein,pnas}.


To define (non-)reconstruction precisely, we associate a bipartite graph $G(\FDC{t})$ with the formula $\FDC{t}$.
The vertices of this graph are the variables and clauses of $\FDC{t}$.
Each variable is adjacent to the clauses in which it appears.
For a (variable or clause) vertex $v$ of $G(\FDC{t})$ let $\partial v$ be the set of neighbours of $v$ in $G(\FDC{t})$.
More generally, for an integer $\ell\geq1$ let $\partial^\ell v$ be the set of vertices of $G(\FDC{t})$ at shortest path distance precisely $\ell$ from $v$.
Following~\cite{pnas}, we say that $\FDC{t}$ has the {\em non-reconstruction property} if
\begin{align}\label{eqnonrec}
	\lim_{\ell\to\infty}\limsup_{n\to\infty}\ex\brk{\abs{\pr\brk{\SIGMA_{\FDC{t}}(x_{t+1})=1\,\Big|\,\FDC{t},\,\cbc{\SIGMA_{\FDC{t}}(y)}_{y\in\partial^{2\ell}x_{t+1}}}-\pr\brk{\SIGMA_{\FDC{t}}(x_{t+1})=1\mid\FDC{t}}}\,\big|\,\PHI\mbox{ satisfiable}}&=0.
\end{align}
Conversely, $\FDC t$ has the {\em reconstruction property} if 
\begin{align}\label{eqrec}
	\liminf_{\ell\to\infty}\,\liminf_{n\to\infty}\,\ex\brk{\abs{\pr\brk{\SIGMA_{\FDC{t}}(x_{t+1})=1\,\Big|\,\FDC{t},\,\cbc{\SIGMA_{\FDC{t}}(y)}_{y\in\partial^{2\ell}x_{t+1}}}-\pr\brk{\SIGMA_{\FDC{t}}(x_{t+1})=1\mid\FDC{t}}}\,\big|\,\PHI\mbox{ sat.}}&>0.
\end{align}

To parse \eqref{eqnonrec}, notice that in the left probability term we condition on both the outcome $\FDC{t}$ of the first $t$ steps of the decimation process and on the values $\SIGMA_{\FDC{t}}(y)$ that the random solution $\SIGMA_{\FDC t}$ assigns to the variables $y$ at distance exactly $2\ell$ from $x_{t+1}$.
By contrast, in the right probability term we only condition on $\FDC t$.
Thus, the second probability term matches the probability $\pi_{\FDC{t}}$ from the decimation process.
Hence, \eqref{eqnonrec} compares the probability that a random solution sets $x_{t+1}$ to one given the values $\SIGMA_{\FDC{t}}(y)$ of {\em all} variables $y$ at distance $2\ell$ from $x_{t+1}$ with plain marginal probability that $x_{t+1}$ is set to one.
What~\eqref{eqnonrec} asks is that these two probabilities be asymptotically equal in the limit of large $\ell$, with high probability over the choice of $\PHI$ and the prior steps of the decimation process.
Thus, so long as non-reconstruction holds `long-range effects', meaning anything beyond distance $2\ell$ for large enough but fixed $\ell$, are negligible.

Confirming the predictions from~\cite{RTS}, the following theorem identifies the precise regimes of $d,t$ where (non-)reconstruction holds.
To state the theorem, we need to know that for $\dmin(k)<d<\dsat(k)$ the polynomial $d(k-1)z^{k-2}(1-z)-1$ has precisely two roots $0<z_*=z_*(d,k)<z^*=z^*(d,k)<1$; we are going to prove this as part of \Prop~\ref{prop_greg} below.
Let
\begin{align}\label{eqlambdas}
	\lambda_*&=\lambda_*(d,k)=-\log(1-z_*)-\frac{z_*}{(k-1)(1-z_*)}>\lambda^*=\lambda^*(d,k)=\max\cbc{0,-\log(1-z^*)-\frac{z^*}{(k-1)(1-z^*)}}\geq0,\\
	\	\theta_*&=\theta_*(d,k)=1-\exp(-\lambda_*)>\theta^*=\theta^*(d,k)=1-\exp(-\lambda^*).
\label{eqthetas}
\end{align}
Additionally, let $\lcond(d,k)$ be the solution to the ODE
\begin{align}\label{eqLena}
	\frac{\partial\lcond(d,k)}{\partial d}&=-\frac{\alpha^*(\lcond(d,k))^k-\alpha_*(\lcond(d,k))^k}{k(\alpha^*(\lcond(d,k))-\alpha_*(\lcond(d,k)))},& \lcond(\dsat(k),k)&=0
\end{align}
on the interval $(\dmin,\dsat]$ and set $\tcond=\tcond(d,k)=1-\exp(-\lcond(d,k))$.
Note that $$\theta^*<\tcond < \theta_*.$$

\begin{theorem}\label{thm_recnonrec}
	Let $k\geq3$ and let $0\leq t=t(n)\leq n$ be a sequence such that $\lim_{n\to\infty}t/n=\theta\in(0,1)$.
	\begin{enumerate}[(i)]
		\item If $d<\dmin(k)$, then $\FDC{t}$ has the non-reconstruction property \whp
		\item If $\dmin(k)<d<\dsat(k)$ and $\theta<\theta^*$ or $\theta>\tcond$, then $\FDC{t}$ has the non-reconstruction property \whp\ \item If $\dmin(k)<d<\dsat(k)$ and $\theta^*<\theta<\tcond$, then $\FDC{t}$ has the reconstruction property \whp
	\end{enumerate}
\end{theorem}

\Thm~\ref{thm_recnonrec} shows that $\dmin(k)$ marks the precise threshold of $d$ up to which the decimation process $\FDC{t}$ exhibits non-reconstruction for {\em all} $0\leq t\leq n$ \whp\
By contrast, for $\dmin(k)<d<\dsat(k)$ there is a regime of $t$ where reconstruction occurs.
In fact, as \Prop~\ref{prop_greg} shows, for $d>\dcore(k)$ we have $\theta^*=0$ and thus reconstruction holds even at $t=0$, i.e., for the original, undecimated random formula $\PHI$.
Prior to the contribution~\cite{RTS}, it had been suggested that this precise scenario (reconstruction on the original problem instance) is the stone on which \BPGD\ stumbles~\cite{Braunstein}.
In fact, Yung's negative result kicks in at this precise threshold $\dcore(k)$.
However, \Thm s~\ref{thm_bpgd} and~\ref{thm_recnonrec} show that matters are more subtle.
Specifically, for $\dmin(k)<d<\dcore(k)$ reconstruction, even though absent in the initial formula $\PHI$, occurs at a later `time' $t>0$ as decimation proceeds, which suffices to trip \BPGD\ up.
Also, remarkably, \Thm~\ref{thm_recnonrec} shows that non-reconstruction is not `monotone'.
The property holds for $\theta<\theta^*$ and then again for $\theta>\tcond$, but not on the interval $(\theta^*,\tcond)$ as visualised in Figure \ref{fig_lena}.

But there is one more surprise.
Namely, \Thm~\ref{thm_recnonrec}~(ii) might suggest that for $\dmin(k)<d<\dsat(k)$ Belief Propagation manages to compute the correct marginals for $t/n\sim \theta >\tcond$, as non-reconstruction kicks back in.
But remarkably, this is not quite true.
Despite the fact that non-reconstruction holds, \BPGD\ goes astray because the algorithm starts its message passing process from a mistaken, oblivious initialisation.
As a consequence, for $t/n\sim\theta\in(\tcond,\theta_*)$ the BP `approximations' remain prone to error.
To be precise, the following result identifies the precise `times' where BP succeeds/fails.
To state the result let $\mu_{\FDC{t}}$ denote the BP `approximation' of the true marginal $\pi_{\FDC{t}}$ of variable $x_{t+1}$ in the formula $\FDC t$ created by the decimation process (see \Sec~\ref{sec_bp} for a reminder of the definition).
Also recall that $\pi_{\FDC t}$ denotes the correct marginal as used by the decimation process.

\begin{theorem}\label{thm_cond}
	Let $k\geq3$ and let $0\leq t=t(n)\leq n$ be a sequence such that $\lim_{n\to\infty}t/n=\theta\in(0,1)$.
	\begin{enumerate}[(i)]
		\item If $0<d<\dmin(k)$ then $\mu_{\FDC{t}}=\pi_{\FDC{t}}$ \whp\
		\item If $\dmin(k)<d<\dsat(k)$ and $\theta<\tcond$ or $\theta>\theta_*$, then $\mu_{\FDC{t}}=\pi_{\FDC{t}}$ \whp
		\item If $\dmin(k)<d<\dsat(k)$ and $\tcond<\theta<\theta_*$, then $\ex\abs{\mu_{\FDC{t}}-\pi_{\FDC{t}}}=\Omega(1).$
	\end{enumerate}
\end{theorem}

The upshot of \Thm s~\ref{thm_recnonrec}--\ref{thm_cond} is that the relation between the accuracy of BP and reconstruction is subtle.
Everything goes well so long as $d<\dmin$ as non-reconstruction holds throughout and the BP approximations are correct.
But if $\dmin<d<\dsat$ and $\theta^*<\theta<\tcond$, then \Thm~\ref{thm_recnonrec}~(iii) shows that reconstruction occurs.
Nonetheless, \Thm~\ref{thm_cond}~(ii) demonstrates that the BP approximations remain valid in this regime.
By contrast, for $\tcond<\theta<\theta_*$ we have non-reconstruction by \Thm~\ref{thm_recnonrec}~(iii), but \Thm~\ref{thm_cond}~(iii) shows that BP misses its mark with a non-vanishing probability.
Finally, for $\theta>\theta_*$ everything is in order once again as BP regains its footing and non-reconstruction holds.
Unfortunately \BPGD\ is unlikely to reach this happy state because the algorithm is bound to make numerous mistakes at times $t/n \sim \theta \in(\tcond,\theta_*)$.

\begin{figure} 
	\begin{subfigure}[b]{0.3\textwidth}
	\includegraphics[height=41mm]{phase_hatched_3.pdf}
	\caption{$k=3$}
	\end{subfigure}
	\hspace{5mm}
	\begin{subfigure}[b]{0.3\textwidth}
	\includegraphics[height=41mm]{phase_hatched_4.pdf}
	\caption{$k=4$}
	\end{subfigure}	
	\hspace{5mm}
	\begin{subfigure}[b]{0.3\textwidth}
	\includegraphics[height=41mm]{phase_hatched_5.pdf}
	\caption{$k=5$}
	\end{subfigure}
	\caption{
		The phase diagrams for $k=3,4,5$ with $d\in(\dmin,\dsat)$ on the horizontal and $\theta$ on the vertical axis.
			The hatched area displays the regime $\theta<\theta^*$ and $\tcond<\theta$ where non-reconstruction holds.
			In the non-hatched area, where $\theta^*<\theta<\tcond$, we have reconstruction.
			Similarly, the blue area displays $\theta<\tcond$ and $\theta>\theta_*$ where BP is correct whereas in the orange area, BP is inaccurate. 
}\label{fig_lena}
\end{figure}

\Thm s~\ref{thm_recnonrec} and~\ref{thm_cond} confirm the predictions from~\cite[\Sec~4]{RTS}.
To be precise, while $\tcond$ matches the predictions of \RTS, the ODE formula~\eqref{eqLena} for the threshold, which is easy to evaluate numerically, does not appear in~\cite{RTS}.
Instead of the ODE formulation, \RTS\ define $\lcond$ as the (unique) $\lambda\geq0$ such that $\Ph(\alpha_*)=\Ph(\alpha^*)$; \Prop~\ref{prop_greg} below shows that both are equivalent.
Illustrating \Thm s~\ref{thm_recnonrec}--\ref{thm_cond}, Figure~\ref{fig_lena} displays the phase diagram in terms of $d$ and $\theta \sim t/n$ for $k=3,4,5$.



\section{Overview}\label{sec_over}

\noindent
This section provides an overview of the proofs of \Thm s~\ref{thm_bpgd}--\ref{thm_cond}.
In the final paragraph we conclude with a discussion of further related work.
{\em We assume throughout that $k\geq3$ is an integer and that $0<d<\dsat(k)$. Moreover, $t=t(n)$ denotes an integer sequence $0\leq t(n)\leq n$ such that $\lim_{n\to\infty}t(n)/n=\theta\in(0,1)$.}


\subsection{Fixed points and thresholds}\label{sec_calc}
The first item on our agenda is to study the functions $\ph,\Ph$ from~\eqref{eqphi}--\eqref{eqPhi}.
Specifically, we are concerned with the maxima of $\Ph$ and the fixed points of $\ph$, the combinatorial relevance of which will emerge as we analyse \BPGD\ and the decimation process.
We begin by observing that the fixed points of $\ph$ are precisely the stationary points of $\Ph$.

\begin{fact}\label{lem_max}
	For any $d>0,\lambda\geq0$ the stationary points $z\in(0,1)$ of $\Phi_{d,k,\lambda}$ coincide with the fixed points of $\phi_{d,k,\lambda}$ in $(0,1)$.
	Furthermore, for a fixed point $z\in(0,1)$ of $\phi_{d,k,\lambda}$ we have
	\begin{align}\label{eq_lem_max}
		\Phi''_{d,k,\lambda}(z)&\begin{cases}
			<0&\mbox{ if }\phi'_{d,k,\lambda}(z)<1,\\
			=0&\mbox{ if }\phi'_{d,k,\lambda}(z)=1,\\
			>0&\mbox{ if }\phi'_{d,k,\lambda}(z)>1.
			\end{cases}
	\end{align}
\end{fact}
\begin{proof}\textcolor{red}{TOPROVE 0}\end{proof}

We recall that $0\leq\alpha_*=\alpha_*(d,k,\lambda)\leq\alpha^*=\alpha^*(d,k,\lambda)\leq1$ are the smallest and the largest fixed point of $\ph$ in $[0,1]$, respectively.
Fact~\ref{lem_max} shows that $\Ph$ attains its global maximum in $[0,1]$ at $\alpha_*$ or $\alpha^*$.
Let $$\amax=\amax(d,k,\lambda)\in\{\alpha_*,\alpha^*\}$$ be the maximiser of $\Ph$; if $\Ph(\alpha_*)=\Ph(\alpha^*)$, set $\amax=\alpha_*$.
The following proposition characterises the fixed points of $\ph$ and the maximiser $\amax$.

\begin{proposition}\label{prop_greg}
~
	\begin{enumerate}[(i)]
		\item If $d<\dmin(k)$, then for all $\lambda>0$ we have $\alpha_*(d,k,\lambda)=\alpha^*(d,k,\lambda)$, the function $\lambda\in(0,\infty)\mapsto\alpha_*(d,k,\lambda)\in(0,1)$ is analytic, and $\alpha_*(d,k,\lambda)$ is the unique stable fixed point of $\ph$.
		\item If $\dmin(k)<d<\dsat(k)$, then the polynomial $d(k-1)z^{k-2}(1-z)-1$ has precisely two roots $0<z_*<z^*<1$, the numbers $\lambda_*,\lambda^*$ from~\eqref{eqlambdas} satisfy $0\leq \lambda^*<\lambda_*$ and the following is true.
			\begin{enumerate}[(a)]
				\item If $\lambda<\lambda^*$ or $\lambda>\lambda_*$, then $\alpha_*(d,k,\lambda)=\alpha^*(d,k,\lambda)\in(0,1)$ is the unique stable fixed point of $\ph$.
				\item If $\lambda^*<\lambda<\lambda_*$, then $0<\alpha_*(d,k,\lambda)<\alpha^*(d,k,\lambda)<1$ are the only stable fixed points of $\ph$.
				\item  The functions $\lambda\in(0,\lambda_*)\mapsto\alpha_*(d,k,\lambda)$ and $\lambda\in(\lambda^*,\infty)\mapsto\alpha^*(d,k,\lambda)$ are analytic.
\item If $\dmin(k)<d<\dsat(k)$, then the solution $\lcond$ of~\eqref{eqLena} satisfies $\lambda^*<\lcond=\lcond(d)<\lambda_*$ and
			\begin{align*}
				\amax(d,k,\lambda)&=\begin{cases}
					\alpha_*(d,k,\lambda)&\mbox{ if }\lambda<\lcond,\\
					\alpha^*(d,k,\lambda)&\mbox{ if }\lambda>\lcond.
				\end{cases}
			\end{align*}
			Furthermore, $\Ph(\alpha^*(d,k,\lambda))\neq\Ph(\alpha_*(d,k,\lambda))$ unless $\lambda=\lcond$.
			Thus, the function $\lambda\mapsto\amax(d,k,\lambda)$ is analytic on $(0,\lcond)$ and on $(\lcond,\infty)$, but discontinuous at $\lambda=\lcond$.
		\end{enumerate}
		\end{enumerate}
	\end{proposition}


\begin{figure}
\begin{subfigure}[b]{0.45\textwidth}
		\includegraphics[height=50mm]{amax.pdf}
		\caption{$\amax$ }
	\end{subfigure}
\begin{subfigure}[b]{0.45\textwidth}		
		\includegraphics[height=50mm]{Phisalphas.pdf}
\caption{$\Phi(\amax)$}
	\end{subfigure}
	\caption{$\amax$ and $\Phi(\amax) = \Phi_{d,k,\lambda}(\amax)$ for $d=2.4$ and $k=3$ from $\theta^*$ to $\theta_*$.}
	\label{fig_amax}
\end{figure}

\subsection{Belief Propagation}\label{sec_bp}
Having done our analytic homework, we proceed to
recall how Belief Propagation computes the `approximations' $\mu_{\FBP{t}}$ that the \BPGD\ algorithm relies upon.
We will see that due to the inherent symmetries of XORSAT the Belief Propagation computations simplify and boil down to a simpler message passing process called Warning Propagation.
Subsequently we will explain the connection between Warning Propagation and the fixed points $\alpha_*,\alpha^*$ of $\ph$.

It is probably easiest to explain BP on a general XORSAT instance $F$ with a set $V(F)$ of variables and a set $C(F)$ of clauses of lengths between one and $k$.
As in \Sec~\ref{sec_results_dc} we consider the graph $G(F)$ induced by $F$, with vertex set $V(F)\cup  C(F)$ and an edge $xa$ between $x\in V(F)$ and $a\in C(F)$ iff $a$ contains $x$.
Let $\partial v=\partial_Fv$ be the set of neighbours of $v\in V(F)\cup C(F)$.
Additionally, given an assignment $\tau\in\{0,1\}^{\partial a}$ of the variables that appear in $a$, we write $\tau\models a$ iff $\tau$ satisfies $a$.

With each variable/clause pair $x,a$ such that $x\in\partial a$ Belief Propagation associates two sequences of `messages' $(\mu_{F,x\to a,\ell})_{\ell\geq0}$, $(\mu_{F,a\to x,\ell})_{\ell\geq0}$ directed from $x$ to $a$ and from $a$ to $x$, respectively.
These messages are probability distributions on $\{0,1\}$, i.e., 
\begin{align}\label{eqBPmsg1}
	\mu_{F,x\to a,\ell}=(\mu_{F,x\to a,\ell}(0),\mu_{F,x\to a,\ell}(1)),\,\mu_{F,a\to x,\ell}=(\mu_{F,a\to x,\ell}(0),\mu_{F,a\to x,\ell}(1)),\\
	\mu_{F,x\to a,\ell}(0)+\mu_{F,x\to a,\ell}(1)=\mu_{F,a\to x,\ell}(0)+\mu_{F,a\to x,\ell}(1)&=1.\label{eqBPmsg2}
	\end{align}
The initial messages are uniform, i.e.,
\begin{align}\label{eqBPupdate0}
	\mu_{F,x\to a,0}(s)=\mu_{F,a\to x,0}(s)&=1/2&&(s\in\{0,1\}).
\end{align}
Further, the messages at step $\ell+1$ are obtained from the messages at step $\ell$ via the {\em Belief Propagation equations}
\begin{align}\label{eqBPupdate1}
	\mu_{F,a\to x,\ell+1}(s)&\propto\sum_{\tau\in\{0,1\}^{\partial a}}\vecone\{\tau_{x}=s,\,\tau\models a\}\prod_{y\in\partial a\setminus\{x\}}\mu_{F,y\to a,\ell}(\tau_y),\\
\mu_{F,x\to a,\ell+1}(s)&\propto\prod_{b\in\partial x\setminus\{a\}}\mu_{F,b\to x,\ell}(s).\label{eqBPupdate2}
\end{align}
In \eqref{eqBPupdate1}--\eqref{eqBPupdate2} the $\propto$-symbol represents the normalisation required to ensure that the updated messages satisfy~\eqref{eqBPmsg2}.
In the case of~\eqref{eqBPupdate2} such a normalisation may be impossible because the expressions on the r.h.s.\ could vanish for both $s=0$ and $s=1$.
In this event we agree that
	\begin{align*}
		\mu_{F,x\to a,\ell+1}(s)=\begin{cases}
			\mu_{F,x\to a,\ell}(s)&\mbox{ if }\mu_{F,x\to a,\ell}(s)\neq1/2\\
			\vecone\{s=0\}&\mbox{ otherwise}
		\end{cases}&&(s\in\{0,1\});
	\end{align*}
in other words, we retain the messages from the previous iteration unless its value was $1/2$, in which case we set $\mu_{F,x\to a,\ell+1}(0)=1$. 
The same convention applies to $\mu_{F,a\to x,\ell+1}(s)$. 
Further, at any time $t$ the BP messages render a heuristic `approximation' of the marginal probability that a random solution to the formula $F$ sets a variable $x$ to $s\in\{0,1\}$:
\begin{align}\label{eqBPmarg}
	\mu_{F,x,\ell}(s)&\propto\prod_{b\in\partial x}\mu_{F,b\to x,\ell}(s).
\end{align}
We set $\mu_{F,x,\ell}(0)=1-\mu_{F,x,\ell}(1)=1$ if the normalisation in~\eqref{eqBPmarg} fails, i.e., if $\sum_{s\in\{0,1\}}\prod_{b\in\partial x}\mu_{F,b\to x,\ell}(s)=0$.

\begin{fact}\label{fact_halfint}
	The BP messages and marginals are half-integral for all $t$, i.e., for all $t\geq0$ and $s\in\{0,1\}$ we have
	\begin{align}\label{eqfact_halfint}
			\mu_{F,x\to a,\ell}(s),\mu_{F,a\to x,\ell}(s),\mu_{F,x,\ell}(s)\in\{0,1/2,1\}.
		\end{align}
		Furthermore, for all $\ell>2\sum_{a\in C(F)}|\partial a|$ we have $\mu_{F,x,\ell}(s)=\mu_{F,x,\ell+1}(s)$.
\end{fact}
\begin{proof}\textcolor{red}{TOPROVE 1}\end{proof}

\noindent
Finally, in light of Fact~\ref{fact_halfint} it makes sense to define the approximations for \BPGD\ by letting
\begin{align}\label{eqmuBPGD}
	\mu_{\FBP t}&=\lim_{\ell\to\infty}\mu_{\FBP t,x_{t+1},\ell}(1),&
	\mu_{\FDC t}&=\lim_{\ell\to\infty}\mu_{\FDC t,x_{t+1},\ell}(1).
\end{align}


\subsection{Warning Propagation}\label{sec_wp}
Thanks to the half-integrality~\eqref{eqfact_halfint} of the messages, Belief Propagation is equivalent to a purely combinatorial message passing procedure called {\em Warning Propagation} (`WP')~\cite{MM}.
Similar as BP, WP also associates two message sequences $(\omega_{F,x\to a,\ell},\omega_{F,a\to x,\ell})_{\ell\geq0}$ with every adjacent variable/clause pair.
The messages take one of three possible discrete values $\{\fzn,\uzn,\nll\}$ (`frozen', `uniform', `null').
Essentially, $\nll$ indicates that the value of a variable is determined by unit clause propagation.
Moreover, $\fzn$ indicates that a variable is forced to take the value $0$ once all variables in the $2$-core of the hypergraph representation of the formula are set to $0$.
The remaining label $\uzn$ indicates that neither of the above applies.
To trace the BP messages from \Sec~\ref{sec_bp} actually only the two values $\{\nll,\uzn\}$ would be necessary.
However, the third value $\fzn$ will prove useful in order to compare the BP approximations with the actual marginals.
Perhaps unexpectedly given the all-uniform initialisation~\eqref{eqBPupdate0}, we launch WP from all-frozen start values:
\begin{align}\label{eqWPupdate0}
	\omega_{F,x\to a,0}=\omega_{F,a\to x,0}=\fzn&&\mbox{for all }a,x.
\end{align}
Subsequently the messages get updated according to the rules
\begin{align}\label{eqWP1}
	\omega_{F,a\to x,\ell+1}&=\begin{cases}
		\nll&\mbox{ if }\omega_{F,y\to a,\ell}=\nll\mbox{ for all }y\in\partial a\setminus\{x\},\\
		\fzn&\mbox{ if }\omega_{F,y\to a,\ell}\neq\uzn\mbox{ for all }y\in\partial a\setminus\{x\}
		\mbox{ and }\omega_{F,y\to a,\ell}\neq\nll\mbox{ for at least one }y\in\partial a\setminus\{x\},\\
		\uzn&\mbox{ otherwise},
		\end{cases}\\
	\omega_{F,x\to a,\ell+1}&=\begin{cases}
		\nll&\mbox{ if }\omega_{F,b\to x,\ell}=\nll\mbox{ for at least one }b\in\partial x\setminus\{a\},\\
		\fzn&\mbox{ if }\omega_{F,b\to x,\ell}\neq\nll\mbox{ for all }b\in\partial x\setminus\{a\}
		\mbox{ and }\omega_{F,b\to x,\ell}=\fzn\mbox{ for at least one }b\in\partial x\setminus\{a\},\\
		\uzn&\mbox{ otherwise}.
		\end{cases}\label{eqWP2}
\end{align}
In addition to the messages we also define the {\em mark} of variable node $x$ by letting
\begin{align}\label{eqmarks}
	\omega_{F,x,\ell}&=\begin{cases}
		\nll&\mbox{ if }\omega_{F,b\to x,\ell}=\nll\mbox{ for at least one }b\in\partial x,\\
		\fzn&\mbox{ if }\omega_{F,b\to x,\ell}\neq\nll\mbox{ for all }b\in\partial x
		\mbox{ and }\omega_{F,b\to x,\ell}=\fzn\mbox{ for at least one }b\in\partial x,\\
		\uzn&\mbox{ otherwise}.
		\end{cases}
\end{align}
The following statement summarises the relationship between BP and WP.

\begin{fact}\label{fact_wp}
	For all $t\geq0$ and all $x,a$ we have
\begin{align}\label{eqfact_wp1}
		\mu_{x\to a,\ell}(1)&=1/2&&\Leftrightarrow&\omega_{F,x\to a,\ell}&\neq\nll,\\
		\mu_{a\to x,\ell}(1)&=1/2&&\Leftrightarrow&\omega_{F,a\to x,\ell}&\neq\nll,\label{eqfact_wp2}\\
		\mu_{x,\ell}(1)&=1/2&&\Leftrightarrow&\omega_{F,x,\ell}&\neq\nll.\label{eqfact_wp3}
		\end{align}
	Moreover, for all $\ell>2|C(F)|$ we have $\omega_{F,x\to a,\ell}=\omega_{F,x\to a,\ell+1}$ and $\omega_{F,a\to x,\ell}=\omega_{F,a\to x,\ell+1}$.
\end{fact}
\begin{proof}\textcolor{red}{TOPROVE 2}\end{proof}

Fact~\ref{fact_wp} implies that the WP messages and marks `converge' in the limit of large $\ell$, in the sense that eventually they do not change any more.
Let $\omega_{F,x\to a},\omega_{F,a\to x},\omega_{F,x}\in\{\fzn,\uzn,\nll\}$ be these limits.
Furthermore, let $V_{\fzn,\ell}(F)$, $V_{\uzn,\ell}(F)$, $V_{\nll,\ell}(F)$ be the sets of variables with the respective mark after $\ell\geq0$ iterations. 
Also let $V_{\fzn}(F),V_{\uzn}(F),V_{\nll}(F)$ be the sets of variables where the limit $\omega_{F,x}$ takes the respective value.
The following statement traces WP on the random formula $\FDC{t}$ produced by the decimation process.

\begin{proposition}\label{prop_WP}
	Let $\eps>0$ and assume that $d>0$, $t=t(n)\lk{\sim \theta n}$ satisfy one of the following conditions:
	\begin{enumerate}[(i)]
		\item $d<\dmin$, or
		\item $d>\dmin$ and $\theta\not\in\{\theta_*,\theta^*\}$.
	\end{enumerate}
	Then there exists $\ell_0=\ell_0(d,\theta,\eps)>0$ such that for any fixed $\ell\geq\ell_0$ with $\lambda=-\log(1-\theta)$ \whp\ we have 
	\begin{align}\label{eqprop_WP}
		\abs{t+|V_{\nll,\ell}(\FDC{t})|-\alpha_*n}&<\eps n,&\abs{t+|V_{\fzn,\ell}(\FDC{t})|-(\alpha^*-\alpha_*)n}&<\eps n,&
		\abs{V_{\nll}(\FDC{t})\triangle V_{\nll,\ell}(\FDC{t}) }&<\eps n. \end{align}
\end{proposition}
	
\subsection{The check matrix}\label{sec_check}
Since the XOR operation is equivalent to addition modulo two, a XORSAT formula $F$ with variables $x_1,\ldots,x_n$ and clauses $a_1,\ldots,a_m$ translates into a linear system over $\FF_2$, as follows.
Let $A_F$ be the $m\times n$-matrix over $\FF_2$ whose $(i,j)$-entry equals one iff variable $x_j$ appears in clause $a_i$.
Adopting coding parlance, we refer to $A_F$ as the {\em check matrix} of $F$.
Furthermore, let $y_F\in\FF_2^m$ be the vector whose $i$th entry is one plus the sum of any constant term and the number of negation signs of clause $a_i$ mod two.
Then the solutions $\sigma\in\FF_n^n$ of the linear system $A_F\sigma=y_F$ are precisely the satisfying assignments of $F$.

The algebraic properties of $A_F$ therefore have a direct impact on the satisfiability of $F$.
For example, if $A_F$ has rank $m$, we may conclude immediately that $F$ is satisfiable.
Furthermore, the set of solutions of $F$ is an affine subspace of $\FF_2^n$ (if non-empty).
In effect, if $F$ is satisfiable, then the number of satisfying assignments equals the size of the kernel of $A_F$.
Hence the nullity $\nul A_F=\dim\ker A_F$ of the check matrix is a key quantity.

Indeed, the single most significant ingredient towards turning the heuristic arguments from~\cite{RTS} into rigorous proofs is a formula for the nullity of the check matrix of the XORSAT instance $\FDC{t}$ from the decimation process.
To unclutter the notation set $\ADC{t}=A_{\FDC{t}}$.
We derive the following proposition from a recent general result about the nullity of random matrices over finite fields~\cite[\Thm~1.1]{Maurice}.
The proposition clarifies the semantics of the function $\Ph$ and its maximiser $\amax$.
In physics jargon $\Ph$ is known as the Bethe free entropy.

\begin{proposition}\label{prop_nul}
	Let $d>0$ and $\lambda=-\log(1-\theta)$.
	Then
	\begin{align*}
		\lim_{n\to\infty}\nul\ADC{t}&=\Phi_{d,k,\lambda}(\amax)&&\mbox{in probability}.
	\end{align*}
\end{proposition}

\subsection{Null variables}\label{sec_frozen}
\Prop~\ref{prop_nul} enables us to derive crucial information about the set of satisfying assignments of $\FDC{t}$.
Specifically, for any XORSAT instance $F$ with variables $x_1,\ldots,x_n$ let $V_0(F)$ be the set of variables $x_i$ such that $\sigma_i=0$ for all $\sigma\in\ker A_F$.
We call the variables $x_i\in V_0(F)$ {\em null variables}.
Since the set of solutions of $F$, if non-empty, is a translation of $\ker A_F$, any two solutions $\sigma,\sigma'$ of $F$ set the variables in $V_0(F)$ to exactly the same values.
The following proposition shows that WP identifies certain variables as null.

\begin{proposition}\label{prop_nlluzn}
\Whp\ the following two statements are true for any fixed integer $\ell>0$.
	\begin{enumerate}[(i)]
		\item We have $V_{\nll,\ell}(\FDC t)\subset\frz(\FDC t)$.
		\item We have $|V_{\uzn,\ell}(\FDC{t})\cap\frz(\FDC{t})|=o(n)$.
	\end{enumerate}
\end{proposition}

\Prop s~\ref{prop_nul} and~\ref{prop_nlluzn} enable us to calculate the number of null variables of $\FDC{t}$, so long as we remain clear of the point $\tcond$ where $\amax$ is discontinuous.

\begin{proposition}\label{cor_frz}
	If $\theta\neq\tcond$ then $|\frz(\FDC{t})|=\amax n+o(n)$ \whp
\end{proposition}

Let us briefly summarise what we have learned thus far.
First, because all Belief Propagation messages are half-integral, BP reduces to WP.
Second, \Prop~\ref{prop_WP} shows that the fixed points $\alpha_*,\alpha^*$ of $\ph$ determine the number of variables marked $\nll$ or $\fzn$ by WP.
Third, the function $\Ph$ and its maximiser $\amax$ govern the nullity of the check matrix and thereby the number of null variables of $\FDC{t}$.
Clearly, the null variables $x_i$ are precisely the ones whose actual marginals $\pr\brk{\SIGMA_{\FDC t}(x_i)=s\mid\FDC{t}}$ are {\em not} uniform.
As a next step, we investigate whether BP/WP identify these variables correctly.

In light of \Prop~\ref{prop_WP}, in order to investigate the accuracy of BP it suffices to compare the {\em numbers} of variables marked $\nll$ by WP with the true marginals.
The following corollary summarises the result.

\begin{corollary}\label{cor_nll}
	For any $d$, $\theta$ the following statements are true.
	\begin{enumerate}[(i)]
		\item If $d<\dmin$, \emph{or} $d>\dmin$ and $\theta<\tcond$, \emph{or} $d>\dmin$ and $\theta>\theta_*$, then $|\frz(\FDC{t})\triangle V_{\nll}(\FDC{t})|=o(n)$ \whp
		\item If $d>\dmin$ and $\tcond<\theta<\theta_*$, then $|\frz(\FDC{t})\triangle V_{\nll}(\FDC{t})|=\Omega(n)$ \whp
	\end{enumerate}
\end{corollary}

Thus, so long as $d<\dmin$ or $d>\dmin$ and $\theta<\tcond$ or $\theta>\theta_*$, the BP/WP approximations are mostly correct.
By contrast, if $d>\dmin$ and $\tcond<\theta<\theta_*$, the BP/WP approximations are significantly at variance with the true marginals \whp\
Specifically, \whp\ BP deems $\Omega(n)$ frozen variables unfrozen, thereby setting itself up for failure.
Indeed, \Cor~\ref{cor_nll} easily implies \Thm~\ref{thm_cond}, which in turn implies \Thm~\ref{thm_bpgd}~(ii) without much ado.

In addition, to settle the (non-)reconstruction thresholds set out in \Thm~\ref{thm_recnonrec} we need to investigate the {\em conditional} marginals given the values of variables at a certain distances from $x_{t+1}$ as in~\eqref{eqnonrec}.
This is where the extra value $\fzn$ from the construction of WP enters.
Indeed, for a XORSAT instance $F$ with variables $x_1,\ldots,x_n$ and an integer $\ell$ let $V_{0,\ell}(F)$ be the set of variables $x_i$ such that $\sigma_i=0$ for all $\sigma\in\ker A_F$ for which $\sigma_h=0$ for all variables $x_h\in\partial^\ell x_i$.
Hence, $V_{0,\ell}(F) \subset V_0(F)$ is the set of variables whose $\ell$-neighbourhood is contained in $V_0(F)$. 

\begin{corollary}\label{lem_fzn}
	Assume that $d>\dmin$ and let $\eps>0$. 
	\begin{enumerate}[(i)]
		\item If $\theta<\tcond$, then for any fixed $\ell$ we have $|V_{\fzn,\ell}(\FDC{t})\cap V_{0,\ell}(\FDC{t})|<\eps n$ \whp\
		\item If $\theta>\tcond$, then there exists $\ell_0=\ell_0(d,\theta,\eps)$ such that for any fixed $\ell>\ell_0$ we have
			\begin{align*}
				|(V_{\nll,\ell}(\FDC{t})\cup V_{\fzn,\ell}(\FDC{t}))\triangle V_{0,\ell}(\FDC{t})|<\eps n\qquad\mbox\whp
			\end{align*}
	\end{enumerate}
\end{corollary}

\noindent
Comparing the number of actually frozen variables with the ones marked $\fzn$ by WP, we obtain \Thm~\ref{thm_recnonrec}.

\subsection{Proving \BPGD\ successful}\label{sec_bpgd_success}
We are left to prove \Thm~\ref{thm_bpgd}.
First, we need to compute the (strictly positive) success probability of \BPGD\ for $d<\dmin$.
At this point, the fact that \BPGD\ has a fair chance of succeeding for $d<\dmin$ should not come as a surprise.
Indeed, \Cor~\ref{cor_nll} implies that the BP approximations of the marginals are mostly correct for $d<\dmin$, at least on the formula $\FDC t$ created by the decimation process.
Furthermore, so long as the marginals are correct, the decimation process $\FDC{t}$ and the execution of the \BPGD\ algorithm $\FBP{t}$ move in lockstep.
The sole difficulty in analysing \BPGD\ lies in proving that the estimates of the algorithm are not just mostly correct, but correct up to only a {\em bounded} expected number of discrepancies over the entire execution of the algorithm.
To prove this fact we combine the method of differential equations with a subtle analysis of the sources of the remaining bounded number of discrepancies.
These discrepancies result from the presence of short (i.e., bounded-length) cycles in the graph $G(\PHI)$.
Finally, the proof of the second (negative) part of \Thm~\ref{thm_bpgd} follows by coupling the execution of \BPGD\ with the decimation process, and invoking \Thm~\ref{thm_cond}.
The details of both arguments can be found in \Sec~\ref{sec_alg}.

\subsection{Discussion}\label{sec_discussion}
The thrust of the present work is to verify the predictions from~\cite{RTS} on the \BPGD\ algorithm and the decimation process rigorously.
Concerning the decimation process, the main gap in the deliberations of \RTS\ \cite{RTS} that we needed to plug is the proof of \Prop~\ref{cor_frz} on the actual number of null variables in the decimation process.
The proof of \Prop~\ref{cor_frz}, in turn, hinges on the formula for the nullity from \Prop~\ref{prop_nul}, whereas \RTS\ state the (as it turns out, correct) formulas for the nullity and the number of null variables based on purely heuristic arguments.

Regarding the analysis of the \BPGD\ algorithm, \RTS\ state that they rely on the heuristic techniques from the insightful article~\cite{DeroulersMonasson} to predict the formula~\eqref{eqthm_bpgd}, but do not provide any further details; the article~\cite{DeroulersMonasson} principally employs heuristic arguments involving generating functions.
By contrast, the method that we use to prove~\eqref{eqthm_bpgd} is a bit more similar to that of Frieze and Suen~\cite{FriezeSuen} for the analysis of a variant of the unit clause algorithm on random $k$-SAT instances, for which they also obtain the asymptotic success probability.
Yet by comparison to the argument of Frieze and Suen, we pursue a more combinatorially explicit approach that demonstrates that certain small sub-formulas that we call `toxic cycles' are responsible for the failure of \BPGD.
Specifically, the proof of~\eqref{eqthm_bpgd} combines the method of differential equations with Poissonisation.
Finally, the proof of \Thm~\ref{thm_bpgd}~(ii) is an easy afterthought of the analysis of the decimation process.

Yung's work~\cite{Yung} on random $k$-XORSAT is motivated by the `overlap gap paradigm'~\cite{Gamarnik}, the basic idea behind which is to show that a peculiar clustered geometry of the set of solutions is an obstacle to certain types of algorithms.
Specifically, Yung only considers the Unit Clause Propagation algorithm and (a truncated version of) \BPGD.
Following the path beaten in~\cite{Mora}, Yung performs moment computations to establish the overlap gap property.
However, moment computations (also called `annealed computations' in physics jargon) only provide one-sided bounds.
Yung's results require spurious lower bounds on the clause length $k$ ($k\geq9$ for Unit Clause and $k\geq13$ for \BPGD).
By contrast, the present proof strategy pivots on the number of null variables rather than overlaps, and \Prop~\ref{cor_frz} provides the precise `quenched' count of null variables. 
A further improvement over~\cite{Yung} is that the present analysis pinpoints the {\em precise} threshold up to which \BPGD\ (as well as Unit Clause) succeeds for any $k\geq3$.
Specifically, Yung proves that these algorithms fail for $d>\dcore$, while \Thm~\ref{thm_bpgd} shows that failure occurs already for $d>\dmin$ with $\dmin<\dcore$.
Conversely, \Thm~\ref{thm_bpgd} shows that the algorithms succeed with a non-vanishing probability for $d<\dmin$. 
Thus, \Thm~\ref{thm_bpgd} identifies the correct threshold for the success of \BPGD, as well as the correct combinatorial phenomenon that determines this threshold, namely the onset of reconstruction in the decimation process (\Thm s~\ref{thm_recnonrec} and~\ref{thm_cond}).

The \BPGD\ algorithm as detailed in \Sec~\ref{sec_bp} applies to a wide variety of problems beyond random $k$-XORSAT.
Of course, the single most prominent example is random $k$-SAT.
Lacking the symmetries of XORSAT, random $k$-SAT does not allow for the simplification to discrete messages; in particular, the BP messages are not generally half-integral.
In effect, BP and WP are no longer equivalent.
In addition to random $k$-XORSAT, the article~\cite{RTS} also provides a heuristic study of \BPGD\ on random $k$-SAT.
But once again due to the lack of half-integrality, the formulas for the phase transitions no longer come as elegant finite-dimensional expressions.
Instead, they now come as infinite-dimensional variational problems.
Furthermore, the absence of half-integrality also entails that the present proof strategy does not extend to $k$-SAT.

The lack of inherent symmetry in random $k$-SAT can partly be compensated by assuming that the clause length $k$ is sufficiently large (viz.\ larger than some usually unspecified constant $k_0$).
Under this assumption the random $k$-SAT version of both the decimation process and the \BPGD\ algorithm have been analysed rigorously~\cite{BP,Angelica}.
The results are in qualitative agreement with the predictions from~\cite{RTS}.
In particular, the \BPGD\ algorithm provably fails to find satisfying assignments on random $k$-SAT instances even below the threshold where the set of satisfying assignments shatters into well-separated clusters~\cite{Barriers,pnas}.
Furthermore, on random $k$-SAT a more sophisticated message passing algorithm called Survey Propagation Guided Decimation has been suggested~\cite{MPZ,RTS}.
While on random XORSAT Survey Propagation and Belief Propagation are equivalent, the two algorithms are substantially different on random $k$-SAT.
One might therefore hope that Survey Propagation Guided Decimation outperforms \BPGD\ on random $k$-SAT and finds satisfying assignments up to the aforementioned shattering transition.
A negative result to the effect that Survey Propagation Guided Decimation fails asymptotically beyond the shattering transition point for large enough $k$ exists~\cite{Hetterich}.
Yet a complete analysis of Belief/Survey Propagation Guided Decimation on random $k$-SAT for any $k\geq3$ in analogy to the results obtained here for random $k$-XORSAT remains an outstanding challenge.

Finally, returning to random $k$-XORSAT, a question for future work may be to investigate the performance of various types of algorithms such as greedy, message passing or local search that aim to find an assignment that violates the least possible number of clauses.
Of course, this question is relevant even for $d>\dsat(k)$.
A first step based on the heuristic `dynamical cavity method' was recently undertaken by Maier, Behrens and Zdeborov\'a~\cite{MBZ}.








\subsection{Preliminaries and notation}\label{sec_notation}
Throughout we assume that $k\geq3$ and $0<d<\dmin$ and $\theta\in(0,1)$ are fixed independently of $n$.
We always let $t=t(n)\in\{0,1,\ldots,n\}$ be an integer sequence such that $\lim_{n\to\infty}t/n=\theta$.
Unless specified otherwise we tacitly assume that $n$ is sufficiently large for our various estimates to hold.
Asymptotic notation such as $O(\nix)$ refers to the limit of large $n$ by default, with $k,d,\theta$ fixed.
We continue to denote by $\alpha_*=\alpha_*(\lambda)=\alpha_*(d,k,\lambda)$ and $\alpha^*=\alpha^*(\lambda)=\alpha^*(d,k,\lambda)$ the smallest/largest fixed points of $\ph$ in $[0,1]$ and by $\lambda_*=\lambda_*(d,k)$, $\lambda^*=\lambda^*(d,k)$, $\theta_*=\theta_*(d,k)$, $\theta^*=\theta^*(d,k)$ the quantities defined in~\eqref{eqlambdas}--\eqref{eqthetas}.

For a formula $F$ and a partial assignment $\sigma:U\to\{0,1\}$ with $U\subset V(F)$ let $F[\sigma]$ be the simplified formula obtained by substituting constants for the variables in $U$.
The {\em length} of a clause of $F[\sigma]$ is defined as the number of variables from $V(F)\setminus U$ that the clause contains.

	The following fact provides the correctness of BP on formulas represented by acyclic graphs $G(F)$.
	\begin{fact}[{\cite[Chapter 14]{MM}}]\label{fact_BP_acyclic}
		For a XORSAT Formula $F$ with an acyclic bipartite graph $G(F)$ the BP marginals as defined in \eqref{eqBPmarg} are exact, i.e.
		\begin{align*}
			\lim_{\ell \to \infty}\mu_{F,x,\ell}(1) &=\pr\brk{\SIGMA_{F}(x)=1}.
		\end{align*}
	\end{fact}


\subsection{Organisation}\label{sec_org}
The rest of the paper is organised as follows.
\Sec~\ref{sec_greg} contains the proof of \Prop~\ref{prop_greg}.
Subsequently in \Sec~\ref{sec_prop_WP} we investigate Warning Propagation to prove \Prop s~\ref{prop_WP} and~\ref{prop_nlluzn}.
Furthermore, \Sec~\ref{sec_nul} deals with the study of the check matrix; here we prove \Prop s~\ref{prop_nul} and~\ref{cor_frz} as well as Corollaries~\ref{cor_nll} and~\ref{lem_fzn}.
Additionally, with all these preparations completed we put all the pieces together to complete the proofs of \Thm s~\ref{thm_recnonrec} and~\ref{thm_cond} in \Sec~\ref{sec_thm_recnonrec}.
Finally, \Sec~\ref{sec_alg} contains the proof of \Thm~\ref{thm_bpgd}.


\section{Proof of \Prop~\ref{prop_greg}}\label{sec_greg}

\noindent
Even though a few steps are mildly intricate, the proof of \Prop~\ref{prop_greg} mostly consists of `routine calculus'.
As a convenient shorthand we introduce $$\zeta_\lambda(z)=\zeta_{d,k,\lambda}(z)=\phi_{d,k,\lambda}(z)-z = \mk{1-\exp\bc{-\lambda -d z^{k-1}} - z}.$$
Its derivatives read
\begin{align}
\zt'(z)  
 &=d(k-1)  \; z^{k-2} \;  \expld
 \; -1   \;
 \label{zt'}
\quad\text{and} \\
\zt''(z)
&=
  d (k-1) \; z^{k-3} \; \expld \; 
  \left[  (k-2) - d(k-1) z^{k-1} \right] .
\label{zt''}
\end{align}
Also let
\begin{align}\label{inflection}
    z_0 &= z_0(d,k) = \left(\frac{k-2}{d(k-1)}\right)^{\frac{1}{{k-1}}}. 
\end{align}
We begin by investigating the zeros of $\zt$, obviously identical with fixed points of $\ph$.

\begin{lemma} \label{Claim13}
	Assume that $\lambda>0$.
	\begin{enumerate}[(i)]
		\item The function $\zt$ has either one or three zeros in $z \in [0,1]$, possibly including multiple zeros.
			If $\zt$ has three zeros, then at least one lies in the interval $[0,z_0]$ and at least one lies in the interval $[z_0,1]$.
		\item Also, $\zt$ has at most two stationary points, a minimum and a maximum, and if it has both, the minimum occurs left of the maximum.
		\item If $\zt$ has a unique zero, then $\alpha_*$ is a stable fixed point of $\ph$ and $\sup_{z\in[0,1]}\ph'(z)<1$.
		\item If $\zt$ has three zeros but no double zero, then $\alpha_*,\alpha^*$ are stable fixed points of $\ph$. Additionally, $\ph$ possesses an unstable fixed point $\alpha_\mathrm{u}\in(\alpha_*,\alpha^*)$.
			Furthermore, there exists $\eps=\eps(d,\lambda)>0$ such that
			\begin{align*}
				\sup_{z\in[0,\alpha_*+\eps]}\ph'(z)&<1,&\sup_{z\in[\alpha^*-\eps,1]}\ph'(z)&<1.
			\end{align*}
	\end{enumerate}
\end{lemma}
\begin{proof}\textcolor{red}{TOPROVE 3}\end{proof}

The following statement implies that $\ph$ has only a single fixed point if $d<\dmin$.

\begin{lemma}\label{lem_belowdmin}
	Let $\lambda>0$.
	If $d<\dmin$, then $\zt$ has a unique zero and is strictly decreasing.
\end{lemma}
\begin{proof}\textcolor{red}{TOPROVE 4}\end{proof}

Proceeding to average degrees $d>\dmin$, we verify that the values $\lambda_*,\lambda^*$ from \Sec~\ref{sec_results_dc} are well defined and satisfy the inequality~\eqref{eqlambdas}.

\begin{lemma}\label{lem_abovedmin}
	If $d>\dmin$, then the polynomial $d(k-1)z^{k-2}(1-z)-1$ has precisely two roots $0<z_*<z^*<1$ and the values $\lambda_*,\lambda^*$ defined in~\eqref{eqlambdas} satisfy $\lambda_*>\lambda^*$.
	Furthermore, $\dcore>\dmin$ and $\lambda^*=0$ iff $d\geq\dcore$.
\end{lemma}
\begin{proof}\textcolor{red}{TOPROVE 5}\end{proof}

We are ready to identify the zeros of $\zt$ for $d>\dmin$, depending on the regime of $\lambda$.

\begin{lemma}\label{lem_zero}
	Let $\lambda>0$ and assume that $d>\dmin$.
	\begin{enumerate}[(i)]
		\item If  $\lambda<\lambda^*$, then $\zt$ has a unique zero.
		\item If  $\lambda^*<\lambda<\lambda_*$, then $\zt$ has three distinct zeros.
		\item If  $\lambda>\lambda_*$, then $\zt$ has a unique zero.
	\end{enumerate}
\end{lemma}
\begin{proof}\textcolor{red}{TOPROVE 6}\end{proof}

Combining \Lem s~\ref{lem_abovedmin} and~\ref{lem_zero}, we can now verify the analytic properties of the function $\lambda\mapsto\alpha_*$ and $\lambda\mapsto\alpha^*$.

\begin{lemma}\label{lem_alphas}
	Let $0<d<\dsat$ and $\lambda>0$.
	\begin{enumerate}[(i)]
		\item If $d<\dmin$, then the function $\lambda\in(0,\infty)\mapsto\alpha_*=\alpha^*$ is analytic with derivative
		\begin{align}\label{eq_alphas}
	\frac{\partial\alpha_*}{\partial \lambda}&=\frac{1- \alpha_*}{1-d(k-1)\alpha_*^{k-2}(1-\alpha_*)}<1.
	\end{align}
		\item If $d>\dmin$, then $\lambda\in(0,\lambda_*)\mapsto\alpha_*$ is analytic with derivative \eqref{eq_alphas}.
\item If $d>\dmin$, then $\lambda\in(\lambda^*,\infty)\mapsto\alpha^*$ is analytic differentiable with derivative
\begin{align*}
	\frac{\partial\alpha^*}{\partial \lambda}&=\frac{1- \alpha^*}{1-d(k-1)\alpha^{*\,k-2}(1-\alpha^*)}.
	\end{align*}
	\end{enumerate}
\end{lemma}
\begin{proof}\textcolor{red}{TOPROVE 7}\end{proof}

As a final preparation towards the proof of \Prop~\ref{prop_greg} we investigate the solution $\lcond$ to the differential equation~\eqref{eqLena}; notice that \Lem~\ref{lem_alphas} shows that this ODE does indeed possess a unique solution on $(\dmin,\dsat]$.

\begin{lemma}\label{lem_maxPhi}
	For any $0<d<\dsat$ we have $0<\lcond<\lambda_*$.
	Furthermore, for all $0<\lambda<\lcond$ we have $\Phi_{d,k,\lambda}(\alpha_*)>\Phi_{d,k,\lambda}(\alpha^*)$, while $\Phi_{d,k,\lambda}(\alpha_*)<\Phi_{d,k,\lambda}(\alpha^*)$ for $\lambda_*>\lambda>\lcond$. 
\end{lemma}
\begin{proof}\textcolor{red}{TOPROVE 8}\end{proof}

\begin{proof}\textcolor{red}{TOPROVE 9}\end{proof}


\section{Warning Propagation and local weak convergence}\label{sec_prop_WP}


\noindent
In this section we prove \Prop s~\ref{prop_WP} and~\ref{prop_nlluzn}.
The proofs rely on the concept of local weak convergence.
Specifically, we are going to set up a Galton-Watson process that mimics the local topology of the graph $G(\FDC t)$ up to any fixed depth $\ell$.
Subsequently we will analyse WP on the Galton-Watson tree and argue that the result extends to $G(\FDC t)$.

\subsection{Local weak convergence}\label{sec_lwc}
The construction of the Galton-Watson process $\TT=\TT(d,k,t)$ is pretty straightforward.
The process has two types called {\em variable nodes} and {\em check nodes}.
The process starts with a single variable node $v_0$.
Furthermore, each variable node begets a $\Po(d)$ number of check nodes as offspring, while the offspring of a check node is a $\Bin(k-1,1-t/n)$ number of variable nodes.

Let $\TT$ be the Galton-Watson tree rooted at $v_0$ that this process generates; $\TT$ may be infinite.
Hence, for an integer $\ell$ obtain $\TT^{(\ell)}$ from $\TT$ by deleting all variable/check nodes at distance greater than $2\ell$ from $v_0$.
Thus, $\TT^{(\ell)}$ is a finite random tree rooted at $v_0$.
For any graphs $T,T'$ rooted at $v,v'$, respectively, we write $T\ism T'$ if there is a graph isomorphism $\iota:T\to T'$ such that $\iota(v)=v'$.
Furthermore, for a vertex $v$ of $G(\FDC{t})$ and an integer $\ell$ we let $\partial^{\leq\ell}_{\FDC t}v$ be the subgraph obtained from $G(\FDC t)$ by deleting all vertices at distance greater than $2\ell$ from $v$, rooted at $v$.
Finally, for a rooted graph $g$ and an integer $\ell$ we let $\vN_t^{(\ell)}(g)$ be the number of vertices $v$ of $G(\FDC{t})$ such that $\partial^{\leq\ell}_{\FDC t}v\ism g$.

\begin{lemma}\label{lem_lwc}
	For any rooted tree $g$ we have
	\begin{align}\label{eq_lem_lwc}
		\ex\abs{\vN_t^{(\ell)}(g)-(n-t)\pr\brk{\TT^{(\ell)}\ism g}}=o(n).
	\end{align}
\end{lemma}
\begin{proof}\textcolor{red}{TOPROVE 10}\end{proof}

\subsection{Proof of \Prop~\ref{prop_WP}}\label{sec_proof_prop_WP}
To prove \Prop~\ref{prop_WP} we estimate the sizes $|V_{\nll,\ell}(\FDC{t})|,|V_{\fzn,\ell}(\FDC{t})|$ separately.
Recall that $\theta \sim t/n$.


\begin{lemma}\label{lem_WP_n}
	Let $\eps>0$ and assume that one of the following conditions is satisfied:
	\begin{enumerate}[(i)]
		\item $d<\dmin$, or
		\item $d>\dmin$ and $|\theta_*-\theta|>\eps$. \end{enumerate}
	Then there exists $\ell_0=\ell_0(d,\eps)>0$ such that for any fixed $\ell\geq\ell_0$ with $\lambda=-\log(1-\theta)$ \whp\ we have
	$$\abs{t+|V_{\nll,\ell}(\FDC{t})|-\alpha_*n}<\eps n.$$
\end{lemma}
\begin{proof}\textcolor{red}{TOPROVE 11}\end{proof}


\begin{lemma}\label{lem_WP_f}
	Let $\eps>0$ and assume that $d>0$, $t=t(n)$ are such that one of the following conditions is satisfied:
	\begin{enumerate}[(i)]
		\item $d<\dmin$, or
		\item $d>\dmin$, $|\theta_*-\theta|>\eps$ and $|\theta^*-\theta|>\eps$.
	\end{enumerate}
	Then there exists $\ell_0=\ell_0(d,\eps)>0$ such that for any fixed $\ell\geq\ell_0$ with $\lambda=-\log(1-\theta)$ \whp\ we have 
	$$
	\abs{|V_{\fzn,\ell}(\FDC{t})|-(\alpha^*-\alpha_*)n}<\eps n.
	$$
\end{lemma}
\begin{proof}\textcolor{red}{TOPROVE 12}\end{proof}

Finally, we compare the set $V_{\nll,\ell}(\FDC{t})$ obtained after a (large but) bounded number of iterations with the ultimate sets $V_{\nll}(\FDC{t})$ obtained upon convergence of WP.
The proof of the following lemma is an adaptation of the argument from~\cite{Molloy} for cores of random hypergraphs.

\begin{lemma}\label{lem_WP_n_infty}
	Assume that $\theta\in(0,1)\setminus\{\theta_*,\theta^*\}$.
	Then for any $\eps>0$ there exists $\ell_0=\ell_0(d,\theta, \eps)$ such that for all $\ell>\ell_0$ we have $|V_{\nll,\ell}(\FDC{t})\triangle V_{\nll}(\FDC{t})|<\eps n$ \whp
\end{lemma}
\begin{proof}\textcolor{red}{TOPROVE 13}\end{proof}



\begin{proof}\textcolor{red}{TOPROVE 14}\end{proof}


\subsection{Proof of \Prop~\ref{prop_nlluzn}}\label{sec_prop_nlluzn}
We deal with the two claims separately.
Towards the first claim we establish the following stronger, deterministic statement.

\begin{lemma}\label{lem_nll}
	For any XORSAT instance $F$ with variables $V_n=\{x_1,\ldots,x_n\}$ and any integer $\ell\geq0$ we have $V_{\nll,\ell}(F)\subset\frz(F)$.
\end{lemma}
\begin{proof}\textcolor{red}{TOPROVE 15}\end{proof}

The following lemma deals with the variables that WP marks $\uzn$.

\begin{lemma}\label{lem_uzn}
	For any fixed $\ell\geq0$ we have $|V_{\uzn,\ell}(\FDC{t})\cap\frz(\FDC{t})|=o(n)$ \whp 
\end{lemma}

\newcommand{\nb}{\partial^{\leq\ell}}
\begin{proof}\textcolor{red}{TOPROVE 16}\end{proof}

\begin{proof}\textcolor{red}{TOPROVE 17}\end{proof}

\section{Analysis of the check matrix}\label{sec_nul}

\noindent
In this section we prove \Prop s~\ref{prop_nul} and~\ref{cor_frz}.
\Prop~\ref{prop_nul} is an easy consequence of~\cite[\Thm~1.1]{Maurice}.
Furthermore, \Prop~\ref{cor_frz} follows from \Prop~\ref{prop_nul} by interpolating on the parameter $\lambda$; a related argument was recently used in~\cite{fullrank} to show that certain random combinatorial matrices have full rank \whp\
In addition, we prove Corollaries~\ref{cor_nll} and~\ref{lem_fzn} and subsequently complete the proofs of \Thm s~\ref{thm_recnonrec}--\ref{thm_cond}.

\subsection{Proof of \Prop~\ref{prop_nul}}\label{sec_prop_nul}
We use a general result \cite[\Thm~1.1]{Maurice} about the rank of sparse random matrices from a fairly universal class of distributions.
The definition of this general random matrix goes as follows.
Let $\fd,\fk\geq0$ be integer-values random variables such that $0<\ex[\fd^3]+\ex[\fk^3]<\infty$.
Moreover, let $(\fd_i,\fk_i)_{i\geq0}$ be families of mutually independent random variables such that $\fd_i\disteq\fd$ and $\fk_i\disteq\fk$.
Let $\bar\fd=\ex[\fd]$ and $\bar\fk=\ex[\fk]$ and for an integer $n>0$ let $\fm=\fm_n\disteq\Po(\bar\fd n/\bar\fk)$.
The sequence $(\fm_n)_n$ is independent of $(\fd_i,\fk_i)_{i\geq0}$.
Further, let $\fS_n$ be the event that
\begin{align}\label{eqDegreeMatch}
	\sum_{i=1}^n\fd_i&=\sum_{i=1}^{\fm_n}\fk_i.
\end{align}
It is a known fact that $\pr\brk{\fS_n}=\Omega(n^{-1/2})$~\cite[\Prop~1.10]{Maurice}.
Given that $\fS_n$ occurs, create a simple random bipartite graph $\fG_n$ with a set $\fV_n=\{\fx_1,\ldots,\fx_n\}$ of {\em variable nodes} and a set $\fC_n=\{\fc_1,\ldots,\fc_{\fm_n}\}$ of {\em check nodes} uniformly at random subject to the condition that $\fx_j$ has degree $\fd_j$ and $\fc_i$ has degree $\fk_i$ for all $1\leq j\leq n$ and $1\leq i\leq\fm_n$.
Finally, let $\fA_n$ be the biadjacency matrix of $\fG_n$.
Thus, $\fA_n$ has size $\fm_n\times n$ and its $(i,j)$-entry equals $1$ iff $\fx_j$ and $\fc_i$ are adjacent in $\fG_n$.

\begin{theorem}[{special case of \cite[\Thm~1.1]{Maurice}}]\label{thm_maurice}
	Let $\fD(z)=\sum_{h=0}^\infty\pr\brk{\fd=h}z^h$ and $\fK(z)=\sum_{h=0}^\infty\pr\brk{\fk=h}z^h$ be the probability generating functions of $\fd,\fk$, respectively.
	Furthermore, let 
	\begin{align}\label{eq_thm_maurice}
		\fF&:[0,1]\to\RR,&z\mapsto\fD(1-\fK'(z)/\fK'(1))-\frac{\fD'(1)}{\fK'(1)}(1-\fK(z)-(1-z)\fK'(z)).
	\end{align}
	Then 
		\begin{align*}
			\lim_{n\to\infty}\frac1n\nul\fA_n&=\max_{z\in[0,1]}\fF(z)&&\mbox{in probability.}
		\end{align*}
\end{theorem}

We now derive \Prop~\ref{prop_nul} from \Thm~\ref{thm_maurice} by identifying suitable distributions $\fd,\fk$ such that $\fA_n$ resembles $\vA_t$.

\begin{proof}\textcolor{red}{TOPROVE 18}\end{proof}


\subsection{Proof of \Prop~\ref{cor_frz}}\label{sec_cor_frz}
We continue to work with the random matrix $\vA[\lambda]$ from the above proof of \Prop~\ref{prop_nul}.
As we recall, this matrix is obtained by adding $\vl=\Po(\lambda n)$ stochastically independent new rows to the matrix $A(\PHI)$ that each contain a single non-zero entry in a uniformly random position.
Combining~\eqref{eq_prop_nul_1}--\eqref{eq_prop_nul_2}, we see that
\begin{align}\label{eqAA}
	\abs{\ex[\nul\vA[\lambda]]-\ex[\nul\vA_t]}&=o(n)&&\mbox{ for }\lambda=-\log(1-\theta).
\end{align}
Towards the proof of \Prop~\ref{cor_frz} we observe that $\nul\vA[\lambda],\nul\vA_t$ concentrate about their expectations.

\begin{lemma}\label{cor_nul}
	We have 
	\begin{align}\label{eq_cor_nul}
		\pr\brk{|\nul\vA_t-\ex[\nul\vA_t]|>\sqrt n\log n}&=o(n^{-10}),&
		\pr\brk{|\nul\vA\brk\lambda-\ex[\nul\vA\brk\lambda]|>\sqrt n\log n}&=o(n^{-10}).
	\end{align}
\end{lemma}
\begin{proof}\textcolor{red}{TOPROVE 19}\end{proof}




We are going to estimate $|V_0(\FDC t)|$ by way of estimating changes of $\nul\vA[\lambda]$ as $\lambda$ varies.
Since $\nul\vA[\lambda]/n$ converges to $\Ph(\amax)$ by \Prop~\ref{prop_nul}, we thus need to estimate the derivative $\frac\partial{\partial\lambda}\Ph(\amax)$.

\begin{lemma}\label{lem_nul}
	Let $d>0$ and assume that
	\begin{enumerate}[(i)]
		\item $d<\dmin$, or
		\item $d>\dmin$ and $\lambda\in(0,\infty)\setminus\{\lcond\}$.
	\end{enumerate}
Then
\begin{align}\label{eqlem_nul}
	\frac{\partial}{\partial\lambda}\Phi_{d,k,\lambda}(\amax)=\amax-1.
\end{align}
\end{lemma}
\begin{proof}\textcolor{red}{TOPROVE 20}\end{proof}

Complementing the analytic formula~\eqref{eqlem_nul}, we now derive a combinatorial interpretation of the derivative of the nullity.
For a matrix $A$ of size $m\times n$ let $\frz(A)$ be the set of all indices $i\in[n]$ such that $\sigma_i=0$ for all $\sigma\in\ker A$.

\begin{lemma}\label{lem_diff_nul}
	For any $d,\lambda>0$ we have
	\begin{align*}
		\frac\partial{\partial\lambda}\ex[\nul \vA[\lambda]]&=\frac{\ex|\frz(\vA[\lambda])|}n-1.
	\end{align*}
\end{lemma}
\begin{proof}\textcolor{red}{TOPROVE 21}\end{proof}

With these preparations in place we can now derive the desired formulas for $|V_0(\vA_t)|$.
We treat the cases $\amax=\alpha_*$ and $\amax=\alpha^*$ separately.

\begin{lemma}\label{lem_nul_*}
	Assume that $d,\lambda>0$ satisfy
	\begin{align}\label{eq_cor_frz_*}
		\Phi_{d,k,\lambda}(\alpha_*)>\Phi_{d,k,\lambda}(\alpha)\quad\mbox{for all $\alpha\in[0,1]\setminus\{\alpha_*\}$.}
	\end{align}
	Then $|\frz(\vA[\lambda])|=\alpha_*n+o(n)$ \whp
\end{lemma}
\begin{proof}\textcolor{red}{TOPROVE 22}\end{proof}

\begin{lemma}\label{lem_nul^*}
	Assume that $d,\lambda>0$ are such that
	\begin{align}\label{eq_cor_frz^*}
		\Phi_{d,k,\lambda}(\alpha^*)>\Phi_{d,k,\lambda}(\alpha)\quad\mbox{for all $\alpha\in[0,1]\setminus\{\alpha^*\}$.}
	\end{align}
	Then $|\frz(\vA[\lambda])|=\alpha^*n+o(n)$ \whp
\end{lemma}
\begin{proof}\textcolor{red}{TOPROVE 23}\end{proof}

\begin{proof}\textcolor{red}{TOPROVE 24}\end{proof}



\subsection{Proof of \Cor~\ref{cor_nll}}\label{sec_cor_nll}

There are four cases to consider separately.
Let $\eps>0$.
\begin{description}
	\item[Case 1: $d<\dmin$]
		As \Prop~\ref{prop_greg} (i) shows, in this case we have $\alpha_*=\alpha^*$ for all $\lambda>0$; thus, the function $\ph$ has only the single fixed point $\alpha_*$, which is stable.
		Furthermore, \Prop~\ref{prop_WP} shows that $||V_{\nll,\ell}(\FDC t)|-\alpha_*n|<\eps n/2$ for large enough $\ell$ \whp\
		Moreover, \Prop~\ref{cor_frz} yields $|\frz(\FDC t)|=\alpha_*n+o(n)$ \whp\
		Therefore, \Prop~\ref{prop_nlluzn} implies that $|\frz(\FDC t)\triangle V_{\nll,\ell}(\FDC t)|<\eps n$ \whp\ for large enough $\ell$.
		Since $|V_{\nll,\ell}(\FDC t)|\subset\frz(\FDC t)$ \whp\ and $|V_{\nll,\ell}(\FDC t)\triangle V_{\nll}(\FDC t)|<\eps n$ by~\eqref{eqprop_WP}, the assertion follows.
	\item[Case 2] $\dmin<d<\dsat$ and $\theta>\theta_*$: 
		A similar argument as under Case~1 applies.
		Indeed, \Prop~\ref{prop_greg} (ii) shows that $\alpha_*=\alpha^*$ is the unique and stable fixed point of $\ph$.
		Since $||V_{\nll,\ell}(\FDC t)|-\alpha_*n|<\eps n/2$ for large $\ell$ \whp\ by \Prop~\ref{prop_WP} and $|\frz(\FDC t)|=\alpha_*n+o(n)$ \whp\ by \Prop~\ref{cor_frz}, \Prop~\ref{prop_nlluzn} yields $|\frz(\FDC t)\triangle V_{\nll,\ell}(\FDC t)|<\eps n$ \whp\
		Therefore, \eqref{eqprop_WP} implies the assertion.
	\item[Case 3] $\dmin<d<\dsat$ and $\theta<\tcond$:
		\Prop~\ref{prop_greg} (ii) shows that $\alpha_*<\alpha^*$ in this case.
		Moreover, \Prop~\ref{prop_WP} yields $||V_{\nll,\ell}(\FDC t)|-\alpha_*n|<\eps n/2$ for large $\ell$ \whp, while \Prop~\ref{cor_frz} and \Prop~\ref{prop_greg} (iii) imply that $|\frz(\FDC t)|=\alpha_*n+o(n)$ \whp\
		Thus, the same steps as in Cases 1--2 complete the proof.
	\item[Case 4] $\dmin<d<\dsat$ and $\tcond<\theta<\theta_*$:
		Once again \Prop~\ref{prop_greg} (ii) shows that $\alpha_*<\alpha^*$, \Prop~\ref{prop_WP} yields $||V_{\nll,\ell}(\FDC t)|-\alpha_*n|<\eps n/2$ for large $\ell$ \whp, and \Prop~\ref{cor_frz} and \Prop~\ref{prop_greg} (iii) show that $|\frz(\FDC t)|=\alpha^*n+o(n)$ \whp\
		Since $|V_{\nll,\ell}(\FDC t)|\subset\frz(\FDC t)$ \whp, the assertion follows from \eqref{eqprop_WP} and the fact that $\alpha_*<\alpha^*$.
\end{description}

\subsection{Proof of \Cor~\ref{lem_fzn}}\label{sec_lem_fzn}

Assume first that $\theta<\tcond$.
Then \Cor~\ref{cor_nll} shows that $|\frz(\FDC t)\triangle V_{\nll}(\FDC t)|=o(n)$ for large enough $\ell$.
Since $V_{\nll}(\FDC t)\cap V_{\fzn}(\FDC t)=\emptyset$ by construction, the first assertion follows. 

Now suppose $\theta>\tcond$.
Then \Prop~\ref{prop_WP} yields $||V_{\fzn,\ell}(\FDC t)|-\alpha^*n|<\eps n/2$ for large $\ell$ \whp, while \Prop~\ref{cor_frz} and \Prop~\ref{prop_greg} (iii) show that $|\frz(\FDC t)|=\alpha^*n+o(n)$ \whp\
Additionally, \Prop~\ref{prop_WP} shows that $|V_{\uzn,\ell}(\FDC t)\cap \frz(\FDC t)|<\eps n$ for large $\ell$, which implies the assertion.
	
\subsection{Proofs of \Thm s~\ref{thm_recnonrec} and~\ref{thm_cond}}\label{sec_thm_recnonrec}
We begin with the following observation.

\begin{lemma}\label{prop_recon}
	Let $\SIGMA\in\ker(\FDC t)$ be uniformly random.
	For any $\ell>0$ \whp\ we have
	\begin{align}\label{eq_prop_recon_mu}
		\pr\brk{\SIGMA_{x_{t+1}}=0\mid\FDC t,\SIGMA_{\partial^{2\ell} x_{t+1}}}&=\frac12\bc{1+\vecone\{x_{t+1}\in V_{\fzn,\ell}(\FDC t)\cup V_{\nll,\ell}(\FDC t)\}},\\
		\pi_{\FDC{t}}=\pr\brk{\SIGMA_{x_{t+1}}=0\mid\FDC t}&=\frac12\bc{1+\vecone\{x_{t+1}\in \frz(\FDC t)\}}.\label{eq_prop_recon_pi}
	\end{align}
\end{lemma}
\begin{proof}\textcolor{red}{TOPROVE 25}\end{proof}


\begin{proof}\textcolor{red}{TOPROVE 26}\end{proof}


\begin{proof}\textcolor{red}{TOPROVE 27}\end{proof}

\section{Belief Propagation Guided Decimation}\label{sec_alg}

\noindent
In this section we prove \Thm~\ref{thm_bpgd}.
We begin by arguing that \BPGD\ is actually equivalent to the simple combinatorial Unit Clause Propagation algorithm.
Then we prove the `positive' part, i.e., the formula \eqref{eqthm_bpgd} for the success probability for $d<\dmin$.
Subsequently we prove the second part of the theorem concerning $\dmin<d<\dsat$.

\subsection{Unit Clause Propagation redux}\label{sec_UCP}

The simple-minded Unit Clause Propagation algorithm attempts to assign random values to as yet unassigned variables one after the other.
After each such random assignment the algorithm pursues the `obvious' implications of its decisions.
Specifically, the algorithm substitutes its chosen truth values for all occurrences of the already assigned variables.
If this leaves a clause with only a single unassigned variable, a so-called `unit clause', the algorithm assigns that variable so as to satisfy the unit clause.
If a conflict occurs because two unit clauses impose opposing values on a variable, the algorithm declares that a conflict has occurred, sets the variable to false and continues; of course, in the event of a conflict the algorithm will ultimately fail to produce a satisfying assignment.
The pseudocode for the algorithm is displayed in Algorithm~\ref{Alg_UCP}.


\begin{algorithm}[h!]
Let $U=\emptyset$ and let $\SIGUC:U\to\{0,1\}$ be the empty assignment\;
 \For{$t=0,\ldots,n-1$}{
	 \If{$x_{t+1}\not\in U$}{add $x_{t+1}$ to $U$\;
		 choose $\SIGUC(x_{t+1})\in\{0,1\}$ uniformly at random\;
			\While{$\PHI[\SIGUC]$ contains a unit clause $a$}{let $x$ be the variable in $a$\;
				let $s\in\{0,1\}$ be the truth value that $x$ needs to take to satisfy $a$\;
				\If{another unit clause $a'$ exists that requires $x$ be set to $1-s$}{output `conflict' and let $\SIGUC(x)=0$\;}\Else{add $x$ to $U$ and let $\SIGUC(x)=s$\;}
				}
			}
		}
	\Return $\SIGUC$\;
 \caption{The \UCP\ algorithm.}\label{alg_ucp}
 \label{Alg_UCP}
\end{algorithm}

Let $\FUC t$ denote the simplified formula obtained after the first $t$ iterations (in which the truth values chosen for $x_1,\ldots,x_t$ and any values implied by Unit Clauses have been substituted).
We notice that the values assigned during Steps~6--12 are deterministic consequences of the choices in Step~5.
In particular, the order in which unit clauses are processed Steps~6--12 does not affect the output of the algorithm.

\begin{proposition}\label{prop_UCP}
	We have
	\begin{align*}
		\pr\brk{\BPGD\mbox{ outputs a satisfying assignment of }\PHI}&=\pr\brk{\UCP\mbox{ outputs a satisfying assignment of }\PHI}.
	\end{align*}
\end{proposition}
\begin{proof}\textcolor{red}{TOPROVE 28}\end{proof}

In light of \Prop~\ref{prop_UCP} we are left to study the success probability of \UCP.
The following two subsections deal with this task for $d<\dmin$ and $d>\dmin$, respectively.


\subsection{The success probability of \UCP\ for $d<\dmin$}\label{sec_alg_pos}
We continue to denote by $\FUC{t}$ the sub-formula obtained after the first $t$ iterations of \UCP.
Let $V_n=\{x_1,\ldots,x_n\}$ be the set of variables of the \emph{XORSAT} instance \emph{F}. 
Also, let $\vV(t)\subset\{x_{t+1},\ldots,x_n\}$ be the set of variables of $\FUC{t}$.
Thus, $\vV(t)$ contains those variables among $x_{t+1},\ldots,x_n$ whose values are not implied by the assignment of $x_1,\ldots,x_t$ via unit clauses.
Also let $\vC(t)$ be the set of clauses of $\FUC{t}$; these clauses contain variables from $\vV(t)$ only, and each clause contains at least two variables.
Let $\bar\vV(t)=V_n\setminus\vV(t)$ be the set of assigned variables.
Thus, after its first $t$ iterations \UCP\ has constructed an assignment $\SIGUC:\bar\vV(t)\to\{0,1\}$.
Moreover, let $\vV'(t+1)=\vV(t)\setminus\vV(t+1)$ be the set of variables that receive values in the course of the iteration $t+1$ for $0\leq t<n$.
Additionally, let $\vC'(t+1)$ be the set of clauses of $\FUC t$ that consists of variables from $\vV'(t+1)$ only.
Finally, let $\FUC{t+1}'$ be the formula comprising the variables $\vV'(t+1)$ and the clauses $\vC'(t+1)$.

To characterise the distribution of $\FUC t$ let $\vn(t)=|\vV(t)|$ and let $\vm_\ell(t)$ be the number of clauses of length $\ell$, i.e., clauses that contain precisely $\ell$ variables from $\vV(t)$.
Observe that $\vm_1(t)=0$ because unit clauses get eliminated.
Let $\fF_t$ be the $\sigma$-algebra generated by $\vn(t)$ and $(\vm_\ell(t))_{2\leq\ell\leq k}$.

\begin{fact}\label{fact_deferred}
	The XORSAT formula $\FUC t$ is uniformly random given $\fF_t$.
	In other words, the variables that appear in each clause are uniformly random and independent, as are their signs.
\end{fact}
\begin{proof}\textcolor{red}{TOPROVE 29}\end{proof}

We proceed to estimate the random variables $\vn(t),\vm_\ell(t)$.
Let $\vec\alpha(t)=|\bar\vV(t)|/n$ so that $\vn(t)=n(1-\vec\alpha(t))$.
Recall, that $\bar\vV(t) =V_n\setminus\vV(t)$.
Let $\lambda=\lambda(\theta)=-\log(1-\theta)$ \lk{with $\theta \sim t/n$} and recall that $\alpha_*=\alpha_*(d,k,\lambda)$ denotes the smallest fixed point of $\phi_{d,k,\lambda}$.
The proof of the following proposition proof can be found in \Sec~\ref{sec_prop_uc_alpha}.

\begin{proposition}\label{prop_uc_alpha}
	Suppose that $d<\dmin(k)$.
	There exists a function $\delta=\delta(n)=o(1)$ such that for all $0\leq t<n$ and all $2\leq\ell\leq k$ we have
	\begin{align}\label{eq_prop_uc_alpha}
		\pr\brk{|\vec\alpha(t)-\alpha_*|>\delta}&=O(n^{-2}),&
		\pr\brk{\abs{\vm_\ell(t)-\frac{dn}k\binom k\ell(1-\alpha_*)^\ell\alpha_*^{k-\ell}}>\delta n}&=O(n^{-2}).
	\end{align}
\end{proposition}

\Prop~\ref{prop_uc_alpha} paves the way for the actual computation of the success probability of \UCP.
Let $\cR_t$ be the event that a conflict occurs in iteration $t$.
The following proposition gives us the correct value of $\pr\brk{\cR_t \mid \fF_t} $ \whp\, 
Since $\fF_t$ is a random variable the value for the probability $\pr\brk{\cR_t \mid \fF_t} $ is random as well. 


\begin{proposition}\label{prop_uc_error}
	Fix $\eps>0$, let $0\leq t<(1-\eps)n$ and define
	\begin{align}\label{eq_prop_uc_error}
		f_n(t)= d(k-1)(1-\alpha_*)  \alpha_*^{k-2}.
	\end{align}
	Then with probability $1-o(1/n)$ we have 
	\begin{align*}
		\pr\brk{\cR_t \mid \fF_t} = \frac{f_n(t)^2}{4(n-t)(1-f_n(t))^2}+o(1/n).
	\end{align*}
\end{proposition}

The proof of \Prop~\ref{prop_uc_error} can be found in \Sec~\ref{sec_prop_uc_error}.
Moreover, in \Sec~\ref{sec_prop_uc_pois} we prove the following.

\begin{proposition}\label{prop_uc_pois}
	Fix $\eps>0$ and $\ell\geq1$.
	For any $0\leq t_1<\cdots<t_\ell<(1-\eps)n$ we have
	\begin{align}\label{eq_prop_uc_pois}
		\pr\brk{\bigcap_{i=1}^\ell\cR_{t_i}}&\sim\prod_{i=1}^\ell \frac{f_n(t_i)^2}{4(n-t_i)(1-f_n(t_i))^2}.
	\end{align}
\end{proposition}

Finally, the following statement deals with the $\eps n$ final steps of the algorithm.

\begin{proposition}\label{prop_uc_endgame}
	For any $\delta>0$ there exists $\eps>0$ such that $\pr\brk{\bigcup_{(1-\eps)n<t<n}\cR_{t}}<\delta.$ 
\end{proposition}


Before we proceed we notice that \Prop s~\ref{prop_uc_error}--\ref{prop_uc_endgame} imply the first part of \Thm~\ref{thm_bpgd}.

\begin{proof}\textcolor{red}{TOPROVE 30}\end{proof}



\subsubsection{Proof of \Prop~\ref{prop_uc_alpha}}\label{sec_prop_uc_alpha}

The proof of \Prop~\ref{prop_uc_alpha} is based on the method of differential equations.
Specifically, based on Fact~\ref{fact_deferred} we derive a system of ODEs that track the random variables $\vec\alpha(t),\vm_2(t),\ldots,\vm_k(t)$.
We will then identify the unique solution to this system.
As a first step we work out the conditional expectations of $\vec\alpha(t+1),\vm_2(t+1),\ldots,\vm_k(t+1)$ given $\fF_t$.

\begin{lemma}\label{lem_condex}
	If $2\vm_2(t)/\vn(t)<1-\Omega(1)$ and $\vn(t)=\Omega(n)$, then 
	\begin{align}\label{eq_lem_condex_1}
		\ex\brk{\vn(t)-\vn(t+1)\mid\fF_t}&=\frac{\vn(t)^2}{(n-t)(\vn(t)-2\vm_2(t))}+o(1),\\
		\ex\brk{\vec m_\ell(t+1)-\vec m_\ell(t)\mid\fF_t}&=\frac{\vn(t)^2}{(n-t)(\vn(t)-2\vm_2(t))}\cdot\frac{(\ell+1)\vm_{\ell+1}(t)-\ell\vm_\ell(t)}{\vn(t)}+o(1)&&(2\leq\ell< k),\label{eq_lem_condex_2}\\
		\ex\brk{\vec m_k(t+1)\mid\fF_t}&=-\frac{\vn(t)^2}{(n-t)(\vn(t)-2\vm_2(t))}\cdot\frac{k\vm_k(t)}{\vn(t)}+o(1).\label{eq_lem_condex_3}
	\end{align}
\end{lemma}
\begin{proof}\textcolor{red}{TOPROVE 31}\end{proof}

\Lem~\ref{lem_condex} puts us in a position to derive a system of ODEs to track the random variables $\vn(t),\vm_2(t),\ldots,\vm_k(t)$.
Specifically, we obtain the following.

\begin{corollary}\label{cor_ODE}
	Let $\fn,\fm_2,\ldots,\fm_k:[0,1]\to\RR$ be continuously differentiable functions such that
	\begin{align}\label{eqODE1}
		\fn(0)&=1,&\fm_k(0)&=\frac dk,\\
		\frac{\partial\fn}{\partial\theta}&=-\frac{\fn^2}{(1-\theta)(\fn-2\fm_2)},\label{eqODE2}\\
		\frac{\partial\fm_\ell}{\partial\theta}&=\frac{\fn((\ell+1)\fm_{\ell+1}-\ell\fm_\ell)}{(1-\theta)(\fn-2\fm_2)}\quad(2\leq\ell<k),&
		\frac{\partial\fm_k}{\partial\theta}&=-\frac{k\fn\fm_k}{(1-\theta)(\fn-2\fm_2)}.\label{eqODE3}
	\end{align}
	Assume, furthermore, that
	\begin{align}\label{eqODE4}
		\sup_{\theta\in[0,1]}2\fm_2(\theta)/\fn(\theta)&<1.
	\end{align}
	Then with probability $1-o(n^{-2})$ for all $0\leq t\leq n$ we have
	\begin{align*}
		\vn(t)/n&=\fn(t/n)+o(1),& \vm_\ell(t)/n&=\fm_\ell(t/n)+o(1)\quad(2\leq\ell\leq k).
	\end{align*}
\end{corollary}
\begin{proof}\textcolor{red}{TOPROVE 32}\end{proof}

As a next step we construct an explicit solution to the system~\eqref{eqODE1}--\eqref{eqODE3}.

\begin{lemma}\label{lem_ODE}
	If $d<\dmin$, then the functions
	\begin{align}\label{eq_lem_ODE}
		\fn^*(\theta)&=1-\alpha_*(\lambda(\theta)),&
		\fm^*_\ell(\theta)&=\frac dk\binom{k}\ell(1-\alpha_*(\lambda(\theta)))^\ell\alpha_*(\lambda(\theta))^{k-\ell}
	\end{align}
	satisfy \eqref{eqODE1}--\eqref{eqODE4}.
\end{lemma}
\begin{proof}\textcolor{red}{TOPROVE 33}\end{proof}



\begin{proof}\textcolor{red}{TOPROVE 34}\end{proof}

\subsubsection{Proof of \Prop~\ref{prop_uc_error}}\label{sec_prop_uc_error}
$\FUC{t+1}'$ is the XORSAT formula that contains the variables $\vV'(t+1)$ that get assigned during iteration $t+1$ and the clauses $\vC'(t+1)$ of $\FUC{t}$ that contain variables from $\vV'(t+1)$ only.
Also recall that $G(\FUC{t+1}')$ signifies the graph representation of this XORSAT formula.
Unless $\vV'(t+1)=\emptyset$, the graph $G(\FUC{t+1}')$ is connected.

\begin{lemma}\label{lemma_arnab}
	Fix $\eps>0$ and let $0\leq t\leq(1-\eps)n$.
	With probability $1-o(1/n)$ the graph $G(\FUC{t+1}')$ satisfies 
			$$|E(G(\FUC{t+1}'))|\leq|V(G(\FUC{t+1}'))|.$$
\end{lemma}
\begin{proof}\textcolor{red}{TOPROVE 35}\end{proof}

Thus, with probability $1-o(1/n)$ the graph $G(\FUC{t+1}')$ contains at most one cycle.
While it is easy to check that no conflict occurs in iteration $t+1$ if $G(\FUC{t+1}')$ is acyclic, in the case that $G(\FUC{t+1}')$ contains a single cycle there is a chance of a conflict.
The following definition describes the type of cycle that poses an obstacle.

\begin{definition}
	For a XORSAT formula $F$ we call a sequence of variables and clauses $\toxl=(v_1, c_1, \dots, v_\ell, c_\ell, v_\ell+1=v_1)$ a \emph{toxic cycle} of length $\ell$ if 
	\begin{description}
		\item[TOX1] $c_i$ contains the variables $x_i, x_{i+1}$ only, and
		\item[TOX2] the total number of negations in $c_1, \dots c_\ell$ is odd iff $\ell$ is even.
	\end{description}
\end{definition}

\begin{lemma}\label{lem_tox}
	\begin{enumerate}[(i)]
		\item If $\FUC{t+1}'$ contains a toxic cycle, then a conflict occurs in iteration $t+1$.
		\item If $\FUC{t+1}'$ contains no toxic cycle and $|E(G(\FUC{t+1}'))|\leq|V(G(\FUC{t+1}'))|$, then no conflict occurs in iteration $t+1$.
	\end{enumerate}
\end{lemma}
\begin{proof}\textcolor{red}{TOPROVE 36}\end{proof}
	
\begin{corollary}\label{lem_tox_error_t}
	Fix $\eps>0$ and let $0\leq t\leq(1-\eps)n$.
	Then
	\begin{align*}
		\pr\brk{\cR_{t+1}}&=\pr\brk{\mbox{$\FUC{t+1}'$ contains a toxic cycle}}+o(1/n).
	\end{align*}
\end{corollary}
\begin{proof}\textcolor{red}{TOPROVE 37}\end{proof}

	Thus, we are left to calculate the probability that $\FUC{t+1}'$ contains a toxic cycle.
	To this end, we estimate the number of toxic cycles in the `big' formula $\FUC t$.
	Let $\vT_{t}(\ell)$ be the number of toxic cycles of length $\ell$ in $\FUC t$.

	\begin{lemma}\label{lem_toxlt}
	Fix $\eps>0$ and let $1\leq t\leq(1-\eps)n$.
	\begin{enumerate}[(i)]
		\item For any fixed $\ell$, with probability $1-O(n^{-2})$ we have
		\begin{align*}
			\ex\brk{\vT_t\bc\ell\mid\fF_t}&=\beta_\ell+o(1),&&\mbox{where }\beta_\ell=\frac{1}{4\ell} \bc{ d(k-1) (1-\alpha_*)\alpha_*^{k-2} }^\ell = \frac 1 {4\ell} \bc{f_n(t)}^\ell.
		\end{align*}
	\item For any $1\leq\ell\leq n$, with probability $1-O(n^{-2})$ we have $\ex\brk{\vT_t\bc{\ell}\mid\fF_t}\leq\beta_\ell\exp(\eps\ell).$ \end{enumerate}
	\end{lemma}
	\begin{proof}\textcolor{red}{TOPROVE 38}\end{proof}
	
\begin{proof}\textcolor{red}{TOPROVE 39}\end{proof}





\subsubsection{Proof of \Prop~\ref{prop_uc_pois}}\label{sec_prop_uc_pois}
	We combine Fact~\ref{fact_deferred} with the tower rule.
	Specifically, let $0\leq t_1<\cdots<t_h<(1-\eps)n$ be distinct time indices.
	Then repeated application of the tower rule gives
	\begin{align}\nonumber
		\Pr\brk{\bigcap_{i=1}^h \everrti}&= \Erw\brk{\prod_{i=1}^h \vecone\cbc{\everrti}} =  \Erw\brk{ \Erw\brk{\prod_{i=1}^h \vecone \cbc{\everrti} \mid \pastim } }\\
										 &= \Erw\brk{ \bc{\prod_{i=1}^{h-1} \vecone\cbc{\everrti}}  \pr\brk{\everrth \mid \pasthm } } = \cdots=\Erw\brk{\prod_{i=1}^{h} \pr\brk{\everrti \mid \pastim }  }.\label{eq_prop_uc_pois_1}
	\end{align}
	Furthermore, \Prop~\ref{prop_uc_error} shows that with probability $1-o(1/n)$,
	\begin{align}\label{eq_prop_uc_pois_2}
		\pr\brk{\everrti \mid \pastim }&= \frac{f_n(t_i)^2}{4(n-t_i)(1-f_n(t_i))^2}+o(1/n)&&\mbox{for all }1\leq i\leq h.
	\end{align}
Combining~\eqref{eq_prop_uc_pois_1}--\eqref{eq_prop_uc_pois_2} completes the proof.

	
	


\subsubsection{Proof of \Prop~\ref{prop_uc_endgame}}\label{sec_prop_uc_endgame}
Given $\delta>0$ pick $\eps>0$ small enough and let $t=\lceil(1-\eps)n\rceil$.
We are going to show that the graph $G(\FUC t)$ is acyclic with probability at least $1-\delta$.
Since all clauses of $\FUC t$ contain at least two variables, \UCP\ will find a satisfying assignment if $G(\FUC t)$ is acyclic.

To show that $G(\FUC t)$ is acyclic, we observe that $\alpha_*\geq t/n$.
Hence, $\alpha_*$ approaches one as $t/n\to1$.
Further, Fact~\ref{fact_deferred} shows that $G(\FUC t)$ is uniformly random given the degree distribution \eqref{eq_prop_uc_alpha} of the clause nodes.
Indeed, the expression \eqref{eq_prop_uc_alpha} shows that with probability $1-O(n^{-2})$ the expected size of the second neighbourhood of a given variable node is asymptotically equal to
	\begin{align*}
		\gamma=\gamma(\eps)&=\frac1{(1-\alpha_*)n}\cdot\frac{dn}{k}\sum_{\ell=2}^k\ell\binom k\ell(1-\alpha_*)^\ell\alpha_*^{k-\ell}=d(1-\alpha_*^{k-1}).
	\end{align*}
Hence, as $\lim_{\eps\to 0}\gamma=0$, the average degree of the random graph $G(\FUC t)$ tends to zero as $\eps\to0$.
Therefore, for small enough $\eps>0$ the random graph $\G(\FUC t)$ is acyclic with probability greater than $1-\delta$.



\subsection{Failure of \UCP\ for $\dmin<d<\dsat$}\label{sec_failure}
In this section we assume that $\dmin<d<\dsat$.
As in \Sec~\ref{sec_alg_pos} we are going to trace \UCP\ via the method of differential equations.
In particular, we keep the notation from \Sec~\ref{sec_alg_pos}.
Thus, $\vn(t)$ signifies the number of unassigned variables after $t$ iterations, and $\vm_\ell(t)$ denotes the number of clauses that contain precisely $2\leq\ell\leq k$ unassigned variables.
Moreover, $\FUC t$ is the formula comprising these variables and clauses.
The following statement is the analogue of \Prop~\ref{prop_uc_alpha} for $\dmin<d<\dsat$.
Its proof relies on similar arguments as the proof of \Prop~\ref{prop_uc_alpha}.

\begin{proposition}\label{prop_bpgd_cond}
	Suppose that $\dmin(k)<d<\dsat(k)$, fix $\eps,\delta>0$ and let $0<t<(1-\eps)\theta_*n$.
	Then \eqref{eq_prop_uc_alpha} holds with probability $1-O(n^{-2})$.
\end{proposition}
\begin{proof}\textcolor{red}{TOPROVE 40}\end{proof}

\begin{proof}\textcolor{red}{TOPROVE 41}\end{proof}

\section*{Acknowledgement}

\thanks{Amin Coja-Oghlan is supported by DFG CO 646/3, DFG CO 646/5 and DFG CO 646/6.}
\thanks{Lena Krieg is supported by DFG CO 646/3.}
\thanks{Maurice Rolvien is supported by DFG Research Group ADYN (FOR 2975) under grant DFG 411362735.}
\thanks{This research was funded in part by the Austrian Science Fund (FWF) [10.55776/I6502]. For open access purposes, the authors have applied a CC BY public copyright license to any author accepted manuscript version arising from this submission.}


\bibliography{jdec}

\end{document}
