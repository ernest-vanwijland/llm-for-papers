
\documentclass[a4paper,UKenglish,cleveref, autoref, thm-restate, pdfa]{lipics-v2021}


\pdfoutput=1 \hideLIPIcs  



\bibliographystyle{plainurl}

\usepackage{algorithm}
\usepackage[noend]{algpseudocode}

\renewcommand{\mod}{\mathrm{\ mod\ }}
\renewcommand{\S}{\mathcal{S}}
\newcommand{\polylog}{\mathrm{polylog}}
\newcommand{\dm}{\mathrm{dm}}
\newcommand{\ap}{\mathit{ap}}
\newcommand{\whp}{1 - (n+t)^{-\Omega(1)}}
\newcommand{\eps}{\varepsilon}
\newcommand{\warn}{\textcolor{red}}

\newcommand{\I}{{\mathcal{I}}}
\renewcommand{\leq}{\leqslant}
\renewcommand{\geq}{\geqslant}
\newtheorem{fact}[theorem]{\bfseries{Fact}}


\title{Weakly Approximating Knapsack in Subquadratic Time} 



\author{Lin Chen}{Zhejiang University, Hangzhou, China
\and \url{https://person.zju.edu.cn/en/linchen}}{chenlin198662@zju.edu.cn}{https://orcid.org/0000-0003-3909-4916}{}



\author{Jiayi Lian}{Zhejiang University, Hangzhou, China
\and \url{https://joeylian.github.io/}}{jiayilian@zju.edu.cn}{https://orcid.org/0009-0006-4408-9627}{}

\author{Yuchen Mao}{Zhejiang University, Hangzhou, China
\and \url{https://person.zju.edu.cn/en/maoyc}}{maoyc@zju.edu.cn}{https://orcid.org/0000-0002-1075-344X}{Supported by National Natural Science Foundation of China [Project No. 62402436].}

\author{Guochuan Zhang}{Zhejiang University, Hangzhou, China
\and \url{https://person.zju.edu.cn/en/0096209}}{zgc@zju.edu.cn}{https://orcid.org/0000-0003-1947-7872}{Supported by National Natural Science Foundation of China [Project No. 12271477].}



\authorrunning{L. Chen, J. Lian, Y. Mao and G. Zhang} 

\Copyright{Lin Chen, Jiayi Lian, Yuchen Mao and Guochuan Zhang} 

\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10003752.10003809.10003636</concept_id>
       <concept_desc>Theory of computation~Approximation algorithms analysis</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Theory of computation~Approximation algorithms analysis}

\keywords{Knapsack, FPTAS} 



\relatedversion{} 







\nolinenumbers 



\EventEditors{Keren Censor-Hillel, Fabrizio Grandoni, Joel Ouaknine, and Gabriele Puppis}
\EventNoEds{4}
\EventLongTitle{52nd International Colloquium on Automata, Languages, and Programming (ICALP 2025)}
\EventShortTitle{ICALP 2025}
\EventAcronym{ICALP}
\EventYear{2025}
\EventDate{July 8--11, 2025}
\EventLocation{Aarhus, Denmark}
\EventLogo{}
\SeriesVolume{334}
\ArticleNo{117}


\begin{document}

\maketitle

\begin{abstract}
We consider the classic Knapsack problem.  Let $t$ and $\mathrm{OPT}$ be the capacity and the optimal value, respectively. If one seeks a solution with total profit at least $\mathrm{OPT}/(1 + \eps)$ and total weight at most $t$, then Knapsack can be solved in $\tilde{O}(n + (\frac{1}{\eps})^2)$ time [Chen, Lian, Mao, and Zhang '24][Mao '24]. This running time is the best possible (up to a logarithmic factor), assuming that $(\min,+)$-convolution cannot be solved in truly subquadratic time [Künnemann, Paturi, and Schneider '17][Cygan, Mucha, Węgrzycki, and Włodarczyk '19]. The same upper and lower bounds hold if one seeks a solution with total profit at least $\mathrm{OPT}$ and total weight at most $(1 + \eps)t$. Therefore, it is natural to ask the following question. 
    \begin{quote}
        If one seeks a solution with total profit at least $\mathrm{OPT}/(1+\eps)$ and total weight at most $(1 + \eps)t$, can Knsapck be solved in $\tilde{O}(n + (\frac{1}{\eps})^{2-\delta})$ time for some constant $\delta > 0$?
    \end{quote}
    We answer this open question affirmatively by proposing an $\tilde{O}(n + (\frac{1}{\eps})^{7/4})$-time algorithm.
\end{abstract}

\section{Introduction}
In the Knapsack problem, one is given a knapsack of capacity $t$ and a set of $n$ items. Each item $i$ has a weight $w_i$ and a profit $p_i$. The goal is to select a subset of items such that their total weight does not exceed $t$, and their total profit is maximized. 

Knapsack is one of Karp's 21 NP-complete problems~\cite{Kar72}, which motivates the study of approximation algorithms. In particular, it is well known that Knapsack admits fully polynomial-time approximation schemes (FPTASes), which can return a feasible solution with a total profit of at least $\mathrm{OPT}/(1 + \eps)$ in $\mathrm{poly}(n, \frac{1}{\eps})$ time. (We use $\mathrm{OPT}$ to denote the maximum total profit one can achieve using capacity $t$.) There has been a long line of research on designing faster FPTASes for Knapsack~\cite{IK75,Law79,KP04,Rhe15,Chan18,Jin19,DJM23,CLMZ24bSTOCKnapsack,
Mao24}. Very recently, Chen, Lian, Mao and Zhang~\cite{CLMZ24bSTOCKnapsack} and Mao~\cite{Mao24} indenpendently obtained $\tilde{O}(n + (\frac{1}{\eps})^2)$-time FPTASes\footnote{Thourghout, we use an $\tilde{O}(\cdot)$ notation to hide polylogarithmic factors in $n$ and $\frac{1}{\eps}$.}.
On the negative side, there is no $O((n + \frac{1}{\eps})^{2-\delta})$-time FPTAS for any constant $\delta > 0$ assuming $(\min, +)$-convolution cannot be solved in subquadratic time~\cite{CMWW19, KPS17}. Hence, further improvement on FPTASs for Knapsack seems hopeless unless the $(\min, +)$-convolution conjecture is overturned. Due to the symmetry of weight and profit, the same upper and lower bounds also hold if we seek a solution of total profit at least $\mathrm{OPT}$, but relax the capacity to $(1 + \eps)t$.
In this paper, we consider {\it weak approximation schemes}, which relax both the total profit and the capacity. More precisely, a weak approximation scheme seeks a subset of items with total weight at most $(1+\eps)t$ and total profit at least $\mathrm{OPT}/(1+ \eps)$.

Weak approximation algorithms are commonly studied in the context of resource augmentation (see e.g.,~\cite{FGJS05, IZ10}). One often resorts to resource augmentation when there is a barrier to further improvement on standard (i.e., non-weak) approximation algorithms. The two most relevant examples are Subset Sum and Unbounded Knapsack. Both problems admit an $\tilde{O}(n + (\frac{1}{\eps})^2)$-time FPTAS~\cite{KMPS03,BN21b,JK18}, while an $O((n+\frac{1}{\eps})^{2-\delta})$-time FPTAS has been ruled out assuming the $(\min,+)$-convolution conjecture~\cite{KPS17,CMWW19,MWW19,BN21b}. Their weak approximation, however, can be done in truly subquadratic time. For Subset Sum, Mucha, W{\k{e}}grzycki and W{\l}odarczyk~\cite{MWW19} showed a $O(n+(\frac{1}{\eps})^{5/3})$-time weak approximation scheme, and very recently, Chen et al.~\cite{CLMZ24cSTOCPartition} further improved the running time to $\tilde{O}(n+\frac{1}{\eps})$-time. 
For Unbounded Knapsack, Bringmann and Cassis~\cite{BC22} established an $\tilde{O}(n+(\frac{1}{\eps})^{3/2})$-time weak approximation scheme. Given that the standard approximation of Knapsack has been in a similar situation, it is natural to ask whether the weak approximation of Knapsack can be done in truly subquadratic time.

It is worth mentioning that weak approximation schemes for Subset Sum and Unbounded Knapsack are not straightforward extensions of the existing standard (non-weak) approximation schemes. The first truly subquadratic-time weak approximation scheme for Subset Sum~\cite{MWW19} as well as its recent improvement~\cite{CLMZ24cSTOCPartition} both rely on a novel application of additive combinatorics, and the first truly subquadratic-time weak approximation scheme for Unbounded Knapsack~\cite{BC22} relies on a breakthrough on bounded monotone $(\min, +)$-convolution~\cite{CL15,CDXZ22}. Unfortunately, techniques developed in these prior papers are not sufficient to obtain a truly subquadratic-time weak approximation scheme for Knapsack. 

\paragraph*{Our contribution.}
We propose the first truly subquadratic-time weak approximation scheme for Knapsack. 

\begin{theorem}
	Knapsack admits an $\tilde{O}({n+(\frac{1}{\eps})^{7/4}})$-time weak approximation scheme, which is randomized and succeeds with high probability.\footnote{Throughout, ``with high probability'' stands for ``with probability at least $1 - (n + \frac{1}{\eps})^{-\Omega(1)}$''.}
\end{theorem}

Our algorithm can be easily generalized to Bounded Knapsack, in which, every item $i$ can be selected up to $u_i$ times. When $u_i=O(1)$ for all $i$, Bounded Knapsack is equivalent to Knapsack (up to a constant factor), and therefore can be handled by our algorithm. For the general case, there is a standard approach of reducing Bounded Knapsack to instances where $u_i=O(1)$  (see e.g., ~\cite[Lemma 4.1]{MWW19}\cite[Lemma 2.2]{KX19}). Note that such a reduction will blow up item weights and profits by a factor $O(u_i)$, but this does not make much difference in (weak) approximation schemes due to scaling.


\subsection{Technical Overview}
We give a brief overview of the techniques that are used by our algorithm.

\paragraph*{Bounded Monotone (min,+)-Convolution} 
$(\min,+)$-convolution (more precisely, its counterpart, $(\max,+)$-convolution) has been used in almost all the recent progress on Knapsack~\cite{Chan18,Jin19,CLMZ24aSODA,Jin24,Bri24,CLMZ24bSTOCKnapsack,Mao24}. It allows one to cut the input instance into sub-instances, deal with each sub-instance independently, and then merge the results using $(\min,+)$-convolution. In general, it takes $O(n^2)$ time to compute the $(\min,+)$-convolution of two length-$n$ sequences\footnote{A slightly faster $O(n^2/2^{\Omega(\sqrt{\log n})})$-time algorithm is known by Bremner et.al.'s reduction to $(\min,+)$-matrix multiplication~\cite{BCD+14} and William's algorithm for the latter problem~\cite{Wil14} (which was derandomized by Chan and Williams~\cite{CW16}).}, and it is conjectured that there is no algorithm with time $O(n^{2 - \delta})$ for any constant $\delta > 0$. Nevertheless, some special cases of $(\min,+)$-convolution can be computed in subquadratic time (See~\cite{AKM+87,AT19,PRW21,CL15,CDXZ22} for example), and among them is bounded monotone $(\min,+)$-convolution where the two sequences are monotone and their entries are integers bounded by $O(n)$. In a celebrated work, Chan and Lewenstein~\cite{CL15} showed that bounded monotone $(\min,+)$-convolution can be computed in $O(n^{1.859})$ time. This was later improved to $\tilde{O}(n^{1.5})$ by Chi, Duan, Xie, and Zhang~\cite{CDXZ22}, and further generalized to rectangular monotone $(\min,+)$-convolution by Bringmann, D{\"{u}}rr and Polak~\cite{DBLP:conf/esa/BringmannD024}.

Bounded monotone $(\min,+)$-convolution is closely related to Unbounded Knapsack. Indeed, Bringmann and Cassis presented a $\tilde{O}(n+(\frac{1}{\eps})^{3/2})$-time weak approximation scheme~\cite{BC22} that builds upon bounded monotone $(\min,+)$-convolution, and they also showed an equivalence result between the two problems in the sense that a truly subquadratic-time weak approximation scheme for Unbounded Knapsack implies a truly subquadratic algorithm for bounded monotone $(\min,+)$-convolution, and vice versa.



For our problem, we follow the general framework mentioned above: we cut the input instance into sub-instances (called the reduced problem), deal with each sub-instance independently, and then merge the results using bounded monotone $(\min,+)$-convolution. Note that following this framework, for each sub-instance we need to compute the near-optimal objective value for all (approximate) knapsack capacities.

One might anticipate that a subquadratic algorithm for bounded monotone $(\min,+)$-convolution leads to a subquadratic weak approximation scheme for Knapsack straightforwardly, however, this is not the case. Bounded monotone $(\min,+)$-convolution is only used to merge results; the main challenge lies in dealing with each group, where we need the following two techniques.





\paragraph*{Knapsack with Items of Similar Efficiencies}
When all items have the same efficiency, Knapsack becomes Subset Sum, and it can be weakly approximated in $\tilde{O}(n + \frac{1}{\eps})$ time~\cite{CLMZ24cSTOCPartition}.  We extend the result to the case where all the items have their efficiencies (that is, profit-to-weight ratio) in the range $[\rho, \rho + \Delta]$ for constant $\rho$, and show that in such a case, Knapsack can be weakly approximated in $\tilde{O}(n + \frac{1}{\eps} + (\frac{1}{\eps})^2\Delta)$ time. This result may be of independent interest. The following is a rough idea. By scaling, we can assume that there are only $O(\frac{\Delta}{\eps})$ distinct item efficiencies. Then we divide the items into $O(\frac{\Delta}{\eps})$ groups so that the items in the same group have the same efficiency, and therefore, each group can be treated as a Subset Sum problem. For each group, we solve it by invoking Chen et al.'s $\tilde{O}(n + \frac{1}{\eps})$-time weak approximation scheme for Subset Sum~\cite{CLMZ24cSTOCPartition}. After solving all groups, we merge their results using 2D-FFT and divide-and-conquer, which takes $\tilde{O}((\frac{1}{\eps})^2 \Delta)$ time. 

We note that the general idea of using Subset Sum to solve Knapsack when items have similar efficiencies was used by Jin~\cite{Jin19}, and the idea of merging Subset Sum results using 2D-FFT and divide-and-conquer was used by Deng, Jin and Mao~\cite{DJM23}.

\paragraph*{Proximity Bounds}
Proximity is another tool that is crucial to the recent progress of Knapsack~\cite{PRW21,CLMZ24aSODA,Jin24,Bri24,CLMZ24bSTOCKnapsack}. Proximity bounds capture the intuition that the optimal solution should contain a lot of high-efficiency items and only a few low-efficiency items. (The efficiency of an item is defined to be its profit-to-weight ratio.) It was pioneered by Eisenbrand and Weismantel~\cite{EW19}. Their proximity bound works for a more general problem of integer programming and is based on Steinitz's lemma. The proof can be greatly simplified if one only considers Knapsack~\cite{PRW21}.  This bound was subsequently improved using tools from additive combinatorics~\cite{CLMZ24aSODA,Bri24,Jin24}. These bounds depend on the maximum item weight $w_{\max}$ or the maximum item profits $p_{\max}$, and leads to $\tilde{O}(n + w^2_{\max})$-time and $\tilde{O}(n + p^2_{\max})$-time algorithm for Knapsack~\cite{Jin24,Bri24}. (We remark that all the proximity bounds of this type assume that the item weights and profits are integers.) These results, however, seem not helpful in breaking the quadratic barrier: even if we can scale the $w_{\max}$ and $p_{\max}$ to integers bounded by $O(\frac{1}{\eps})$, it still takes  $\tilde{O}(n + (\frac{1}{\eps})^2)$ time to solve the problem.

We shall use another type of proximity bounds, which depends on the item efficiencies. Such a proximity bound was used to design FPTASes for Knapsack~\cite{Jin19,DJM23} before. This proximity states that the optimal solution selects almost all the high-efficiency items, and exchanges only a few of them for the low-efficiency items. Moreover, the number of exchanged items is inversely proportional to the efficiency gap between high- and low-efficiency items. Fewer exchanged items allow a larger additive error per exchanged item, and therefore, a faster approximation.


\paragraph*{Our Main Technical Contribution}
While each of the above-mentioned techniques appeared in previous papers before, it seems that none of them alone can break the quadratic barrier for weakly approximating Knapsack. Our main contribution is to modify and then combine these techniques to obtain a truly subquadratic-time algorithm. 

Note that if the item efficiencies differ significantly, then the efficiency-based proximity bound becomes sharp; If item efficiencies stay similar, then the Subset Sum based approach becomes efficient. To combine both ideas, the main difficulty is that we need to approximate the optimal objective value for all (approximate) capacities, and the efficiency-based proximity bound may be sharp for certain capacities, and weak for other capacities. Towards this, the main idea is to use different rounding precision for different items. We roughly explain it below. For simplicity, when we say we approximately solve a group of items, we mean to approximately compute the optimal objective value for all knapsack capacities when taking items from this group. We partition all items into multiple groups of fixed size. If the items in a group have similar efficiencies, then we approximately solve this group with high precision using the Subset Sum based approach; Otherwise, we approximately solve this group with low precision. Low precision means a larger error per item, but if proximity guarantees that only a few items from this group will contribute to the optimal solution (or only a few items will not contribute), then the total error may still be bounded. So the challenge is to prove that a sharp proximity holds for such a group regardless of the knapsack capacity. 


We remark that a simpler implementation of the above idea leads to an algorithm with running time $\tilde{O}(n + (\frac{1}{\eps})^{11/6})$. By grouping the items in a more sophisticated way, the running time can be improved to  $\tilde{O}(n + (\frac{1}{\eps})^{7/4})$.





\subsection{Further Related Work}
\paragraph*{FPTASes with Running Time in Multiplicative Form} So far, we have discussed FPTASes for Knapsack whose running times are $n+f(\frac{1}{\eps})$. There is also a line of research focusing on FPTASes with a running time of the form $n^{O(1)}\cdot f(\frac{1}{\eps})$ (see, e.g.,~\cite{Law79,KP99,Chan18}). The best known running time of this form is $\tilde{O}(\frac{n^{3/2}}{\eps})$ due to Chan~\cite{Chan18}. It remains open whether there exists an FPTAS of running time $O(\frac{n}{\eps})$.  It is not even clear whether Knapsack admits a weak approximation scheme in $O(\frac{n}{\eps})$ time.

\paragraph*{Pseudo-polynomial Time Algorithm}
Recent years have witnessed important progress in the study of pseudo-polynomial time algorithms for Knapsack and Unbounded Knapsack~\cite{BC22,BC23,CLMZ24aSODA,Bri24,Jin24}. In particular, the best-known algorithm for Knapsack that is parametrized by the largest weight $w_{\max}$ (or the largest profit $p_{\max}$) runs in $\tilde{O}(n+w_{\max}^2)$-time (or $\tilde{O}(n+p_{\max}^2)$-time)~\cite{Bri24,Jin24}, and is the best possible assuming the $(\min, +)$-convolution conjecture~\cite{CMWW19, KPS17}. It is, however, a major open problem whether the quadratic barrier can be broken by combining both parameters, that is, whether an $\tilde{O}(n+{(w_{\max}+p_{\max})}^{2-\delta})$-time algorithm exists for Knapsack. Notably, for Unbounded Knapsack, one can find an algorithm that runs in $\tilde{O}(n+{(w_{\max}+p_{\max})}^{3/2})$ time~\cite{BC22}.

\subsection{Paper Outline}
In Section~\ref{sec:pre}, we introduce all the necessary notation and terminology. In Section~\ref{sec:reduce}, we reduce the problem so that it suffices to consider instances with nice properties. In Section~\ref{sec:tools}, we present all the ingredients that are needed by our algorithm. In Section~\ref{sec:weak}, we present an $\tilde{O}(n + (\frac{1}{\eps})^{11/6})$-time algorithm to illusrate our main idea. In Section~\ref{sec:strong}, we show how to improve the running time to $\tilde{O}(n + (\frac{1}{\eps})^{7/4})$. All the missing proofs can be found in the appendix.

\section{Preliminaries}\label{sec:pre}
We assume that $\eps \leq 1$, since otherwise we can approximate using a linear-time 2-approximation algorithm for Knapsack~\cite[Section 2.5]{KPD04}. We denote the set of real numbers between $a$ and $b$ as $[a, b]$. All logarithms are based $2$.

To ease the presentation of our algorithm, we interpret Knapsack from the perspective of tuples. An item can be regarded as a tuple $(w,p)$, where $w$ and $p$ represent the weight and profit, respectively. Throughout, we do not distinguish an item and a tuple, and always refer to the first entry as weight and the second entry as profit.  The efficiency of a tuple $(w, p)$ is defined to be its profit-to-weight ratio $\frac{p}{w}$.  The efficiency of $(0,0)$ is defined to be $0$.

Let $I$ be a multiset of tuples. We use $w(I)$ to denote the total weight of tuples in $I$. That is, $w(I) = \sum_{(w,p)\in I}w$. Similarly, we define $p(I) = \sum_{(w,p)\in I}p$. Define the set of all (2-dimensional) subset sums of $I$ as follows:
\[
    \mathcal{S}(I) = \{(w(I'), p(I')) : I' \subseteq I\}.
\]
Every tuple $(w, p) \in \mathcal{S}(I)$ corresponds to a Knapsack solution with total weight $w$ and total profit $p$. We say a tuple $(w, p)$ dominates another tuple $(w', p')$ if $w \leq w'$,  $p \geq p'$, and at least one of the two inequalities holds strictly. Let ${\cal S}^+(I)$ be the set obtained from ${\cal S}(I)$ by removing all dominated tuples.
Every tuple in ${\cal S}^+(I)$ corresponds to a Pareto optimal solution for Knapsack, and vice versa. Let $t$ be the given capacity. Knapsack can be formulated as follows:
\[
    \max\{\text{$p$ : $(w, p) \in \mathcal{S}^+(I)$ and $w \leq t$}\}.
\]

We consider the more general problem of (weakly) approximating $\mathcal{S}^+(I)$.
\begin{definition}\label{def:approx-set}
    Let $S$ and $S'$ be two sets of tuples. 
    \begin{romanenumerate}
        \item $S'$ approximates $S$ with factor $1 + \eps$ if for any $(w,p) \in S$, there exists $(w', p') \in S'$ such that $w' \leq (1 + \eps)w$ and $p' \geq p/(1 + \eps)$;

        \item  $S'$ approximates $S$ with additive error $(\delta_w, \delta_p)$ if for any $(w,p) \in S$, there exists $(w' , p') \in S'$ such that $w' \leq w + \delta_w$ and $p' \geq p - \delta_p$;

        \item  $S$ dominates $S'$ if for any $(w', p') \in S'$, there exists $(w, p) \in S$ such that $w \leq w'$ and $p \geq p'$.
    \end{romanenumerate}
\end{definition}
When $S'$ approximates a set $S$ that contains only one tuple $(w,p)$, we also say that $S'$ approximates the tuple $(w,p)$.

Let $(I,t)$ be a Knapsack instance. Let $\mathrm{OPT}$ be the optimal value. Our goal is to compute a set $S'$ such that 
\begin{itemize}
    \item $S'$ approximates $\mathcal{S}^+(I) \cap ([0, t]\times [0, OPT])$ with additive error $(\eps t, \eps \cdot \mathrm{OPT})$ and 

    \item $S'$ is dominated by $\mathcal{S}(I)$.
\end{itemize}
The former ensures that $S'$ provides a good (weak) approximation of the optimal solution, and the latter ensures that every tuple in $S'$ is not better than a true solution. Our definition is essentially the same as that by Bringmann and Cassis~\cite{BC22}, albeit their definition is for sequences.

Let $I_1 \cup I_2$ be a partition of $I$. It is easy to observe the following.
\begin{itemize}
\item \(
    {\cal S}(I) = {\cal S}(I_1) + {\cal S}(I_2)
\), where the sumset $X + Y$ is defined by the following formula.
\[
    X + Y = \{\text{$(w_1 + w_2, p_1 + p_2)$ : $(w_1, p_1) \in X$ and $(w_2, p_2) \in Y$}\};
\]
\item \(
    {\cal S}^+(I) = {\cal S}^+(I_1) \oplus {\cal S}^+(I_2),
\)
where $\oplus$ represents the $(\max, +)$-convolution defined by the following formula.
\[
    X \oplus Y = \{\text{$(w, p) \in X + Y$ : $(w, p)$ is not dominated in $X+Y$}\}.
\]
\end{itemize}

Using Chi et al.'s $\tilde{O}(n^{3/2})$-time algorithm for bounded monotone $(\min,+)$-convolution~\cite{CDXZ22} and Bringmann and Cassis's algorithm for approximating the $(\max,+)$-convolution~\cite[Lemma 28 in the full version]{BC22}, we can approximate the $(\max,+)$-convolution of two sets in $\tilde{O}((\frac{1}{\eps})^{3/2})$ time. Using the divide-and-conquer approach from~\cite{Chan18}, we can generalize it to multiple sets. We defer the proof of the following lemma to Appendix~\ref{app:conv}.

\begin{restatable}{lemma}{lemmerging}\label{lem:merging}
    Let $S_1, S_2, \ldots, S_m \subseteq (\{0\} \cup [A, B]) \times (\{0\} \cup [C, D])$ be sets of tuples with total size $\ell$. In $\tilde{O}(\ell + (\frac{1}{\eps})^{3/2}m)$ time\footnote{Here, the $\tilde{O}(\cdot)$ notation hides polylogarithmic factor not only in $n$ and $\frac{1}{\eps}$, but also in other parameters such as $\frac{B}{A}$ and $\frac{D}{C}$. These parameters will eventually be set to be polynomial in $n$ and $\frac{1}{\eps}$.} and with high probability, we can compute a set $\tilde{S}$ of size $\tilde{O}(\frac{1}{\eps})$ that approximates $S_1 \oplus S_2 \oplus \cdots \oplus S_m$ with factor $1 + O(\eps)$.  Moreover, $\tilde{S}$ is dominated by $S_1 + S_2 + \cdots + S_m$.
\end{restatable}

With Lemma~\ref{lem:merging}, we can approximate $\mathcal{S}^+(I) \cap ([0, t]\times [0, OPT])$ as follows. Partition $I$ into several groups, approximate each group independently, and merge their results using Lemma~\ref{lem:merging}.  As long as there are not too many groups, the merging time will be subquadratic. The following lemmas explain how the multiplicative factor and additive error accumulate during the computation. Their proofs are deferred to Appendix~\ref{app:err}.

\begin{restatable}{lemma}{lemapproxerra}
    \label{lem:approx-err-1}
    Let $S$, $S_1$, and $S_2$ be sets of tuples. 
    \begin{romanenumerate}
        \item If $S_1$ approximates $S$ with factor $1 + \eps_1$ and $S_2$ approximates $S_1$ with factor $1 + \eps_2$, then $S_2$ approximates $S$ with factor $(1 + \eps_1)(1 + \eps_2)$.

        \item If $S_1$ approximates $S$ with additive error $(\delta_{w1}, \delta_{p1})$ and $S_2$ approximates $S_1$ with factor $(\delta_{w2}, \delta_{p2})$, then $S_2$ approximates $S$ with additive error $(\delta_{w1} + \delta_{w2}, \delta_{p1}+ \delta_{p2})$.
    \end{romanenumerate}
\end{restatable}


\begin{restatable}{lemma}{lemapproxerrb}\label{lem:approx-err-2}
    Let $S_1$, $S_2$, $S'_1$, $S'_2$ be sets of tuples.
    \begin{romanenumerate}
        \item If $S'_1$ and $S'_2$ approximates $S_1$ and $S_2$ respectively with factor $1 + \eps$, then 
        $S'_1 \oplus S'_2$ and $S'_1 + S'_2$ approximate $S_1 \oplus S_2$ and $S_1 + S_2$ respectively with factor $1 + \eps$.

        \item If $S'_1$ and $S'_2$ approximates $S_1$ and $S_2$ with additive error $(\delta_{w1}, \delta_{p1})$ and $(\delta_{w2}, \delta_{p2})$, respectively, then 
        $S'_1 \oplus S'_2$ and $S'_1 + S'_2$ approximate $S_1 \oplus S_2$ and $S_1 + S_2$ respectively with additive error $(\delta_{w1} + \delta_{w2}, \delta_{p1}+ \delta_{p2})$.
    \end{romanenumerate}
\end{restatable}


In the rest of the paper, we may switch between the multiplicative factor and the additive error, although our goal is to bound the latter. It is helpful to keep the following observation in mind.
\begin{observation}
    If $S'$ approximates $S$ with factor $1 + \eps$, then $S'$ approximates each tuple $(w, p) \in S$ with additive error $(\eps w, \eps p)$.
\end{observation}

\section{Reducing the Problem}\label{sec:reduce}
With Lemma~\ref{lem:merging}, we can reduce the problem of weakly approximating Knapsack to the following problem. 
\begin{quote}
The reduced problem $\mathrm{RP}(\alpha)$: given a set $I$ of $n$ items from $[1,2] \times [1,2]$ and a real number $\alpha \in [\frac{1}{2}, \frac{1}{4\eps}]$, compute a set $S$ of size $\tilde{O}(\frac{1}{\eps})$ that approximates ${\cal S}^+(I) \cap ([0,\frac{1}{\alpha\eps}] \times [0, \frac{1}{\alpha\eps}])$ with additive error $(O(\frac{1}{\alpha}), O(\frac{1}{\alpha}))$. Moreover, $S$ should be dominated by $\mathcal{S}(I)$.
\end{quote}

\begin{restatable}{lemma}{lemreduce}
\label{lem:reduce}
    An $\tilde{O}(n + T(\frac{1}{\eps}))$-time algorithm for the reduced problem $\mathrm{RP}(\alpha)$ implies an $\tilde{O}(n + (\frac{1}{\eps})^{3/2} + T(\frac{1}{\eps}))$-time weak approximation scheme for Knapsack.
\end{restatable}

The approach to reducing the problem is similar to that in~\cite{CLMZ24bSTOCKnapsack}. The details of the proof are deferred to Appendix~\ref{app:reducing}.

Throughout, we write ${\cal S}^+(I) \cap ([0,\frac{1}{\alpha\eps}] \times [0, \frac{1}{\alpha\eps}])$ as $\mathcal{S}^+(I; \frac{1}{\alpha\eps})$ for short, and when the weight and profit both have an additive error of $\delta$, we simply say that additive error is $\delta$.

We also remark that when presenting algorithms for the reduced problem $\mathrm{RP}(\alpha)$, we will omit the requirement that the resulting set $S$ should be dominated by $\mathcal{S}(I)$ since it is guaranteed by the algorithms in a quite straightforward way. More specifically, in those algorithms (including Lemma~\ref{lem:merging}), rounding is the only place that may incur approximation errors, and whenever we round a tuple $(w,p)$, we always round $w$ up and round $p$ down. As a consequence, a set after rounding is always dominated by the original set. 

\section{Algorithmic Ingredients}\label{sec:tools}
We present a few results that will be used by our algorithm as subroutines.
\subsection{An Algorithm for Small Solution Size}
Consider the reduced problem $\mathrm{RP}(\alpha)$. Since items are from $[1,2] \times [1,2]$, it is easy to see that $|I'| \leq \frac{1}{\alpha\eps}$ for any $I' \subseteq I$ with $(w(I'), p(I')) \in {\cal S}^+(I; \frac{1}{\alpha\eps})$. When $\alpha$ is large, $|I'|$ is small, and such a case can be tackled using Bringmann's two-layer color-coding technique~\cite{Bri17}. Roughly speaking, we can divide $I$ into $k = \tilde{O}(\frac{1}{\alpha\eps})$ subsets $I_1, \ldots, I_k$ so that with high probability, $|I_j \cap I'| \leq 1$ for every $j$. Then it suffices to (approximately) compute $I_1 \oplus \cdots \oplus I_k$, and the resulting set will provide a good approximation for the tuple $(w(I'), p(I'))$.  Since this technique becomes quite standard nowadays, we defer the proof to Appendix~\ref{app:color-coding}.

\begin{restatable}{lemma}{lemalgforlargealpha}\label{lem:alg-for-large-alpha}
    Let $I$ be a set of items from $[1, 2]\times [1,2]$.  In $\tilde{O}(n + (\frac{1}{\eps})^{5/2}\frac{1}{\alpha})$ time and with high probability, we can compute a set $S$ of size $\tilde{O}(\frac{1}{\eps})$ that approximates ${\cal S}^+(I; \frac{1}{\alpha\eps})$ with factor $1 + O(\eps)$.
\end{restatable}

\subsection{An Efficiency-Based Proximity Bound}
A proximity bound basically states that the optimal solution of Knapsack must contain lots of high-efficiency items and very few low-efficiency items. Let $I = \{(w_1, p_1), \ldots, (w_n, p_n)\}$ be a set of items. Assume that the items are labeled in a non-increasing order of efficiencies. That is, $\frac{p_1}{w_1} \geq \cdots \geq \frac{p_n}{w_n}$. (The labeling may not be unique. We fix an arbitrary one in that case.) Given a capacity $w$, let $b$ be the minimum integer such that $w_1 + \cdots + w_b > w$. We say that the item $(w_b, p_b)$ is the breaking item with respect to $w$. (When no such $b$ exists, we simply let $b = n+1$ and define the breaking item to be $(+\infty, 0)$.) 

The following proximity bound is essentially the same as the one used in~\cite{Jin19,DJM23}. For completeness, we provide a proof in Appendix~\ref{app:proximity}.

\begin{restatable}{lemma}{lemproximity}\label{lem:proximity}
    Let $I$ be a set of items from $[1,2]\times [1,2]$. Let $(w^*, p^*) \in {\cal S}^+(I)$.   Let $I^* \subseteq I$ be a subset of items with $w(I^*) = w^*$ and $p(I^*) = p^*$.  Let $\rho_b$ be the efficiency of the breaking item with respect to $w^*$.  For any $I' \subseteq I$, 
    \begin{romanenumerate}
        \item if every item in $I'$ has efficiency at most $\rho_b - \Delta$ for some $\Delta > 0$, then 
        \[
            w(I' \cap I^*) \leq \frac{2}{\Delta};
        \]

        \item if every item in $I'$ has efficiency at least $\rho_b + \Delta$ for some $\Delta > 0$, then 
        \[
            w(I' \setminus  I^*) \leq \frac{2}{\Delta}.
        \]
    \end{romanenumerate}
\end{restatable}

\subsection{An Algorithm for Items with Similar Efficiencies}
When all the items have the same efficiency, Knapsack becomes Subset Sum, and there is an $\tilde{O}(n + \frac{1}{\eps})$-time weak approximation scheme for Subset Sum~\cite{CLMZ24cSTOCPartition}. The following lemma extends this result to the case where the item efficiencies differ by only a small amount. 


\begin{lemma}\label{lem:alg-for-similar-eff}
    Let $I$ be a set of $n$ items from $[1,2]\times [0,\infty]$ whose efficiencies are in the range $[\rho, \rho + \Delta]$. In $\tilde{O}(n + \frac{1}{\eps} + (\frac{1}{\eps})^2 \frac{\Delta}{\rho})$ time and with high probability, we can compute a set of size $\tilde{O}(\frac{1}{\eps})$ that approximates ${\cal S}^+(I)$ with factor $1 + O(\eps)$.
\end{lemma}

We first consider the $(\max, +)$-convolution of two sets of tuples.
\begin{lemma}\label{lem:2d-fft}
    Let $S_1, S_2$ be two sets of tuples from $(\{0\}\cup[A, B]) \times [0, +\infty])$ with total size $\ell$. Suppose that the efficiencies of the tuples in $S_1 \cup S_2$ are in the range $\{0\} \cup [\rho, \rho + \Delta]$. In $\tilde{O}(\ell + \frac{1}{\eps} + (\frac{1}{\eps})^2\frac{\Delta}{\rho})$ time\footnote{Here, the $\tilde{O}(\cdot)$ notation hides polylogarithmic factor not only in $n$ and $\frac{1}{\eps}$, but also in $\frac{B}{A}$. These parameters will eventually be set to be polynomial in $n$ and $\frac{1}{\eps}$.}, we can compute a set $S$ of size $\tilde{O}(\frac{1}{\eps})$ that approximates $S_1 \oplus S_2$ with factor $1 + O(\eps)$. Moreover, $S$ is dominated by $S_1 + S_2$.
\end{lemma}
\begin{proof}\textcolor{red}{TOPROVE 0}\end{proof}

Now we are ready to prove Lemma~\ref{lem:alg-for-similar-eff}. \begin{proof}\textcolor{red}{TOPROVE 1}\end{proof}

The Lemma~\ref{lem:alg-for-similar-eff} immediately implies the following.
\begin{corollary}\label{coro:approx-add}
    Let $I$ be a set of $n$ items from $[1, 2] \times [0, +\infty]$ whose efficiencies are in the range $[\rho, \rho + \Delta]$. In $\tilde{O}(n + \frac{1}{\eps} + (\frac{1}{\eps})^2 \frac{\Delta}{\rho})$ time and with high probability, we can compute a set of size $\tilde{O}(\frac{1}{\eps})$ that approximates each tuple $(w,p) \in {\cal S}^+(I)$ with additive error $(\eps w, \eps p)$. 
\end{corollary}

In the above corollary, the additive error is proportional to the value of the entry of the tuple: a large entry means a large additive error. We can also achieve the contrary and approximate in a way so that a large entry means a small additive error. Basically, this can be done by (approximately) solving a problem symmetric to Knapsack: instead of determining which items should be selected, we determine which items should not be selected. 

\begin{corollary}\label{coro:approx-del}
    Let $I$ be a set of $n$ items from $[1, 2] \times [0, +\infty]$ whose efficiencies are in the range $[\rho, \rho + \Delta]$. In $\tilde{O}(n + \frac{1}{\eps} + (\frac{1}{\eps})^2 \frac{\Delta}{\rho})$ time and with high probability, we can compute a set of size $\tilde{O}(\frac{1}{\eps})$ that approximates ${\cal S}^+(I)$ with additive error $(\eps(w(I) - w), \eps(p(I) - p))$.
\end{corollary}
\begin{proof}\textcolor{red}{TOPROVE 2}\end{proof}



\section{An Algorithm with Exponent 11/6}\label{sec:weak}
To illustrate our main idea, we first present a simpler algorithm that runs in $\tilde{O}(n + (\frac{1}{\eps})^{11/6})$ time.  When $\alpha \geq (\frac{1}{\eps})^{2/3}$, the algorithm in Lemma~\ref{lem:alg-for-large-alpha} already runs in $\tilde{O}(n + (\frac{1}{\eps})^{11/6})$ time. It remains to consider the case where $\frac{1}{2}\leq \alpha \leq (\frac{1}{\eps})^{2/3}$. In the rest of this section, we assume that $I = \{(w_1, p_1), \ldots, (w_n, p_n)\}$ and that $\frac{p_1}{w_1} \geq \cdots \geq \frac{p_n}{w_n}$.

Let $\tau$ be a parameter that will be specified later.  We first partition $I$ into groups as follows. For the items $(w_1, p_1), \ldots, (w_n, p_n)$ in $I$, we bundle every $\tau$ items as a group until we obtain $\lceil\frac{1}{\alpha\eps\tau}\rceil + 1$ groups. For the remaining items, say $\{(w_i, p_i), \ldots, (w_n, p_n)\}$, we further divide them into two groups $\{(w_i, p_i), \ldots, (w_{i'}, p_{i'})\}$ and $\{(w_{i'+ 1}, p_{i'+1}), \ldots, (w_n, p_n)\}$, where $i'$ is the maximum index such that $\frac{p_i}{w_i} - \frac{p_{i'}}{w_{i'}} \leq \frac{1}{\tau}$. At the end, we obtain $m = \lceil\frac{1}{\alpha\eps\tau}\rceil + 3$ groups $I_1, I_2, \ldots, I_m$. For simplicity, we assume that none of these groups is empty. This assumption is without loss of generality, since empty groups only make things easier.

For each group $I_j$, we define $\Delta_j$ to be the maximum difference between the efficiencies of the items in $I_j$. That is,
\[
    \Delta_j = \max\left\{\frac{p}{w} - \frac{p'}{w'} : (w, p), (w', p') \in I_j\right\}. 
\]
By the way we construct the groups, we have that $\sum_j \Delta_j \leq \frac{3}{2}$.  

We say a group $I_j$ is good if $\Delta_j \leq \frac{1}{\tau}$, and bad otherwise. 

Note that the efficiency of any item in $I$ is within the range $[\frac{1}{2}, 2]$. For each good group $I_j$, its item efficiencies are in the range $[\rho, \rho + \frac{1}{\tau}]$ for some constant $\rho \in [\frac{1}{2},2]$, so we can compute a set $S_j$ of $\tilde{O}(\frac{1}{\eps})$ that approximates each tuple $(w, p) \in \mathcal{S}^+(I_j)$ with additive error $(\eps w, \eps p)$ via Corollary~\ref{coro:approx-add} in $O(|I_j| + \frac{1}{\eps} + (\frac{1}{\eps})^2\frac{1}{\tau})$ time. Since the items are from $[1,2]\times [1,2]$, for any tuple in $\mathcal{S}^+(I_j)$, its weight and profit are of the same magnitude, so are the corresponding additive errors. We can say that $S_j$ approximates each tuple $(w, p) \in \mathcal{S}^+(I_j)$ with additive error $O(\eps w)$.

For bad groups, we shall approximate them less accurately: we shall use $\frac{1}{\alpha\tau}$ instead of $\eps$ as the accuracy parameter. (Later we will show that we can still obtain a good bound on the total additive error using the proximity bound in Lemma~\ref{lem:proximity}.)  We compute a set $S'_j$ of size $\tilde{O}(\alpha\tau)$ that approximates each tuple $(w, p) \in \mathcal{S}^+(I_j)$ with additive error $(\frac{w}{\alpha \tau}, \frac{p}{\alpha \tau})$ via Corollary~\ref{coro:approx-add} in $O(|I_j| + \alpha\tau + \alpha^2\tau^2\Delta_j)$ time. We also compute a set $S''_j$ of size $\tilde{O}(\alpha\tau)$ that approximates each $(w, p) \in \mathcal{S}^+(I_j)$ with additive error $(\frac{w(I)-w}{\alpha \tau}, \frac{p(I) - p}{\alpha \tau})$ via Corollary~\ref{coro:approx-del} in $O(|I_j| + \alpha\tau + \alpha^2\tau^2\Delta_j)$ time. Again, since the weight and the profit are of the same magnitude, we can say that $S'_j$ approximates each tuple $(w, p) \in \mathcal{S}^+(I_j)$ with additive error $O(\frac{1}{\alpha \tau}\cdot w)$, and that $S''_j$ approximates each tuple $(w, p) \in \mathcal{S}^+(I_j)$ with additive error $O(\frac{1}{\alpha \tau}\cdot(w(I) - w))$. Now consider the set $S_j = S'_j \cup S''_j$. One can verify that $S_j$ approximates each tuple $(w, p) \in \mathcal{S}^+(I_j)$ with additive error $O(\frac{1}{\alpha \tau}\cdot \min\{w, (w(I) - w)\})$.

The last step is to compute a set $S$ of size $\tilde{O}(\frac{1}{\eps})$ that approximates $S_1 \oplus \cdots \oplus S_m$ with factor $1+O(\eps)$ via Lemma~\ref{lem:merging}. Note that each set $S_j$ is of size $\tilde{O}(\frac{1}{\eps})$ or $\tilde{O}(\alpha\tau)$. This step takes time $\tilde{O}(\frac{m}{\eps} + m\alpha\tau + (\frac{1}{\eps})^{3/2}m)$.

\paragraph*{Bounding Total Time Cost}
Partitioning the item into $I_1, \ldots, I_m$ can be done by sorting and scanning, and therefore, it takes only $O(n\log n + m)$ time. Each good group costs $O(|I_j| + \frac{1}{\eps} + (\frac{1}{\eps})^2\frac{1}{\tau})$ time. Since there are at most $m$ good groups, the good groups cost  total time
\[
    \tilde{O}\left(n + \frac{m}{\eps} + (\frac{1}{\eps})^2\frac{m}{\tau}\right).
\]
Each bad group $I_j$ costs $O(|I_j| + \alpha\tau + \alpha^2\tau^2\Delta_j)$ time, so the total time cost for bad groups is
\[
    \tilde{O}(n + m\alpha\tau + \alpha^2\tau^2\sum_j\Delta_j) \leq \tilde{O}(n + m\alpha\tau + \alpha^2\tau^2).
\]
The inequality is due to that $\sum_j\Delta_j \leq \frac{3}{2}$. Merging all the sets $S_j$ via Lemma~\ref{lem:merging} costs $\tilde{O}(\frac{m}{\eps} + m\alpha\tau + (\frac{1}{\eps})^{3/2}m)$ time. Taking the sum of all these time costs, we have that the running time of the algorithm is
\[
    \tilde{O}\left(n  + \frac{m}{\eps} + m\alpha\tau + (\frac{1}{\eps})^2\frac{m}{\tau} + \alpha^2\tau^2 + (\frac{1}{\eps})^{3/2}m\right).
\]
Recall that $m = \lceil \frac{1}{\alpha\eps\tau} \rceil$ and that $\frac{1}{2} \leq \alpha \leq (\frac{1}{\eps})^{2/3}$. Set $\tau = \lceil(\frac{1}{\eps})^\frac{11}{12}\frac{1}{\alpha}\rceil$. One can verify that the running time is $\tilde{O}(n + (\frac{1}{\eps})^{11/6})$.

\paragraph*{Bounding Additive Error}
To prove the correctness of the algorithm, it suffices to show that the set $S$ returned by the algorithm approximates every tuple in ${\cal S}^+(I; \frac{1}{\alpha\eps})$ with additive error $O(\frac{1}{\alpha})$. 

Let $(w^*, p^*)$ be an arbitrary tuple in ${\cal S}^+(I; \frac{1}{\alpha\eps})$.  Let $(w_b, p_b)$ be the breaking item with respect to $w^*$. Let $\rho_b = \frac{p_b}{w_b}$ be the efficiency of the breaking item. We first show that the proximity bound can be applied to all but four bad groups.

\begin{lemma}\label{lem:bound-median}
    At most four bad groups may contain an item whose efficiency is within the range $[\rho_b - \frac{1}{\tau}, \rho_b + \frac{1}{\tau}]$. Moreover, each of these groups is of size $\tau$.
\end{lemma}
\begin{proof}\textcolor{red}{TOPROVE 3}\end{proof}

Now we are ready to bound the total additive error.
\begin{lemma}\label{lem:total-err}
    $S_1 \oplus S_2 \oplus \cdots \oplus S_m$ approximates $(w^*, p^*)$ with additive error $O(\frac{1}{\alpha})$.
\end{lemma}
\begin{proof}\textcolor{red}{TOPROVE 4}\end{proof}

The above lemma immediately implies the following since $S$ approximates $S_0 \oplus S_1 \oplus \cdots \oplus S_m$ with factor $1 + O(\eps)$.
\begin{corollary}
    $S$ approximates $(w^*, p^*)$ with additive error $O(\frac{1}{\alpha})$.
\end{corollary}

We summarize this section by the following lemma.
\begin{lemma}
   There is an $\tilde{O}({n+(\frac{1}{\eps})^{11/6}})$-time algorithm for the reduced problem, which is randomized and succeeds with high probability.
\end{lemma}


\section{An Improved Algorithm with Exponent 7/4}\label{sec:strong}
Due to the space limit, we present only the most crucial part of the algorithm and the analysis. A full version of this section can be found in Appendix~\ref{app:strong}.

In our $\tilde{O}(n + (\frac{1}{\eps})^{11/6})$-time algorithm, only the bad groups benefit from the proximity bound, which allows us to approximate with larger factor $1 + \frac{1}{\alpha\tau}$, while for the good groups, we simply approximate with factor $1 + \eps$. To further improve the running time, we shall partition the items in a way that all the groups can benefit from the proximity bound.

We assume that $\frac{1}{2}\leq \alpha \leq (\frac{1}{\eps})^{3/4}$ since the algorithm in Lemma~\ref{lem:alg-for-large-alpha} already runs in $\tilde{O}(n + (\frac{1}{\eps})^{7/4})$ time for all $\alpha \geq (\frac{1}{\eps})^{3/4}$. We assume that $I = \{(w_1, p_1), \ldots, (w_n, p_n)\}$ and that $\frac{p_1}{w_1} \geq \cdots \geq \frac{p_n}{w_n}$.

We first partition $I$ into the head group $I_{\mathrm{head}}$ and the tail group $I_{\mathrm{tail}}$ where $I_{\mathrm{head}}$ contains the first $\lceil \frac{1}{\alpha\eps}\rceil + 1 $ items and $I_{\mathrm{tail}}$ contains the rest of the items. 
Below we only give an algorithm for $I_{\mathrm{head}}$, and analyze its additive error. The remaining analysis and the algorithm for $I_{\mathrm{tail}}$ can be found in the full version (Appendix~\ref{app:strong}).

\subsection{Approximating Head Group}\label{sec:strong-head-incomp}
Let $n' = \lceil \frac{1}{\alpha\eps}\rceil + 1$.  We shall show that in $\tilde{O}((\frac{1}{\eps})^{7/4})$ time, we can compute a set $S$ of size $\tilde{O}(\frac{1}{\eps})$ that approximate $\mathcal{S}^+(I_{\mathrm{head}})$ with additive error $O(\frac{1}{\alpha})$.

Let $\tau$ be a parameter that will be specified later. Roughly speaking, we will further partition $I_{\mathrm{head}}$ into good groups and bad groups. The bad groups are the same as before: a bad group is a group of $\tau$ items whose efficiencies differ by at least $\frac{1}{\tau}$. For good groups, we will make them large than before: when we obtain a good group $I'$ of size $\tau$, we will keep adding items to $I'$ until the difference between the maximum and the minimum item efficiencies exceeds $\frac{1}{|I'|}$ with the next item. A more precise description of the partition process is given below. 

We partition $I_{\mathrm{head}}$ into $I_1, \ldots, I_m$ as follows. Initially, $j = 1$ and $k = 1$, and the remaining items are $\{(w_k, p_k), \ldots, (w_{n'}, p_{n'})\}$. Let $k'$ be the minimum integer such that
\[
    (k' - k) \cdot (\frac{p_k}{w_k} - \frac{p_{k'}}{w_{k'}}) > 1.
\]
If such $k'$ does not exist, we set $I_j = \{(w_k, p_k), \ldots, (w_{n'}, p_{n'})\}$, set $\Delta_j = \Delta'_j = \frac{p_k}{w_k} - \frac{p_{n'}}{w_{n'}}$, set $m: = j$, and finish. Assume that $k'$ exists. 
\begin{itemize}
    \item If $k' - k \geq \tau$, we set $I_j = \{(w_k, p_k), \ldots, (w_{k' - 1}, p_{k'-1})\}$, set $\Delta_j = \frac{p_k}{w_k} - \frac{p_{k'-1}}{w_{k'-1}}$, set $\Delta'_j = \frac{p_k}{w_k} - \frac{p_{k'}}{w_{k'}}$, and proceed with $j := j + 1$ and $k: = k'$. In this case, we say $I_j$ is a good group. 

    \item Otherwise, we let $I_j = \{(w_k, p_k), \ldots, (w_{k + \tau - 1}, p_{k + \tau - 1})\}$, set $\Delta_j = \frac{p_k}{w_k} - \frac{p_{k + \tau - 1}}{w_{k+\tau - 1}}$, set $\Delta'_j = \frac{p_k}{w_k} - \frac{p_{k + \tau}}{w_{k+\tau}}$, and proceed with $j: = j + 1$ and $k:= k + \tau$. In this case, we say that $I_j$ is a bad group. 
\end{itemize}
(We remark that $\Delta_j$ and $\Delta'_j$ are not required by the algorithm. They are maintained only for the purpose of analysis. $\Delta_j$ is the actual difference between the maximum and minimum item efficiencies in $I_j$. For technical reasons, we also need $\Delta'_j$. Basically, compared to $\Delta_j$, the gap $\Delta'_j$ also includes the efficiency gap between the minimum efficiency in $I_j$ and the maximum efficiency in $I_{j+1}$. We will use $\Delta_j$ to bound the time cost of $I_j$, and use $\Delta'_j$ to create an efficiency gap for other groups.)

We have the following observations.
\begin{observation}\label{obs:group-property-incomp}
The groups $I_1, \ldots, I_m$ satisfy the following properties.
\begin{romanenumerate}
    \item $m \leq \frac{1}{\alpha\eps\tau} + 1$

    \item $\sum_{j=1}^{m}|I_j| = |I_{\mathrm{head}}| = \frac{1}{\alpha\eps} + 1$ and $\sum_{j=1}^{m}\Delta_j \leq \sum_{j=1}^{m}\Delta'_j \leq \frac{3}{2}$.

    \item For every group $I_j$, the efficiencies of the items in $I_j$ differ by at most $\Delta_j$.

    \item For every good group $I_j$, we have $|I_j|\Delta_j \leq 1$ and $|I_j|\Delta'_j \geq \frac{1}{2}$.

    \item For every bad group $I_j$, we have $|I_j|\Delta'_j \geq |I_j|\Delta_j > 1$.

    \item For the last group $I_m$, we have $|I_m|\Delta_m \leq 1$.
\end{romanenumerate}
\end{observation}


For each group, we shall approximate it in exactly the same way as we did for the bad groups in the $\tilde{O}(n + (\frac{1}{\eps})^{11/6})$-time algorithm, except that we shall use $\frac{1}{\alpha|I_j|}$ as the accuracy parameter. More specifically, we use Corollary~\ref{coro:approx-add} and Corollary~\ref{coro:approx-del} to compute a set $S_j$ of size $\tilde{O}(\alpha|I_j|)$ that approximate each tuple $(w, p) \in \mathcal{S}^+(I_j)$ with additive error $\frac{1}{\alpha|I_j|} \cdot \min\{w, w(I) - w\}$. The time cost for $I_j$ is $\tilde{O}(|I_j| + \alpha|I_j| + \alpha^2|I_j|^2\Delta_j)$.

Then we compute a set $S_{\mathrm{head}}$ of size $\tilde{O}(\frac{1}{\eps})$ that approximates $S_1 \oplus \cdots \oplus S_m$ with factor $1+O(\eps)$ via Lemma~\ref{lem:merging}. 

\paragraph*{Bound Additive Error}
We shall show that $S_{\mathrm{head}}$ approximates every tuple $(w^*, p^*) \in \mathcal{S}^+(I_{\mathrm{head}})$ with additive error $\tilde{O}(\frac{1}{\alpha})$. 

Let $(w^*, p^*)$ be an arbitrary tuple in $\mathcal{S}^+(I_{\mathrm{head}})$. Let $(w_b, p_b)$ be the breaking item with respect to $w^*$. Let $\rho_b = \frac{p_b}{w_b}$ be the efficiency of the breaking item. 

Let $I_{j'}$ be the group containing the breaking item $(w_b, p_b)$. To apply the proximity bound, we will show that the groups can be divided into  $O(\log \frac{1}{\eps})$ collections so that for each collection $\{I_{j_{k} + 1}, I_{j_{k} + 2}, \ldots, I_{j_{k+1}-1} \}$, the efficiency gap between $\rho_b$ and the groups in this collection is at least the maximum $\Delta'_j$ of these groups. To do this,
we need the following auxiliary lemma. Its proof is deferred to the full version.

\begin{lemma}\label{lem:cluster-incomp}
    Let $\Delta_1, \ldots, \Delta_n$ be a sequence of positive real numbers. Let $\Delta_{\min}$ and $\Delta_{\max}$ be the minimum and maximum numbers in the sequence. There exists $h = O(\log \frac{\Delta_{\max}}{\Delta_{\min}})$ indices $1 = j_1 < j_2 < \cdots < j_h = n$ such that for any $k \in \{1, \ldots, h-1\}$, we have that
    \begin{equation}\label{eq:cluster-original-incomp}
            \max\{\Delta_j : j_k < j < j_{k+1}\} \leq \sum\{\Delta_j : j_{k+1} \leq j \leq n\},
    \end{equation}
    where the maximum of an empty set is defined to be $0$.
\end{lemma}


\begin{lemma}
    $S_1 \oplus \cdots \oplus S_m$ approximates $(w^*, p^*)$ with additive error $\tilde{O}(\frac{1}{\alpha})$.
\end{lemma}
\begin{proof}\textcolor{red}{TOPROVE 5}\end{proof}

The above lemma immediately implies the following since $S_{\mathrm{head}}$ approximates $ S_1 \oplus \cdots \oplus S_m$ with factor $1 + O(\eps)$.
\begin{corollary}
    $S_{\mathrm{head}}$ approximates $(w^*, p^*)$ with additive error $\tilde{O}(\frac{1}{\alpha})$.
\end{corollary}

The remaining analysis and the algorithm for $I_{\mathrm{tail}}$ can be found in the full version (Appendix~\ref{app:strong}). We summarize this with the following lemma.
\begin{lemma}
   There is an $\tilde{O}({n+(\frac{1}{\eps})^{7/4}})$-time algorithm for the reduced problem, which is randomized and succeeds with high probability.
\end{lemma}

\bibliography{lipics-v2021-sample-article}

\clearpage
\appendix

\section{Details of Approximating Convolution}\label{app:conv}
We shall prove the following lemma.
\lemmerging*


We need Chi et al.'s algorithm for bounded monotone $(\min,+)$-convolution~\cite{CDXZ22}. We say a set $S$ of tuples is monotone if for any two tuple $(w, p)$, $(w', p')$ in $S$, $w' \geq w$ implies $p' \geq p$. We say $S$ is bounded by $O(\ell)$ if the entries of the tuples in $S$ are non-negative integers bounded by $O(\ell)$. 

\begin{lemma}\label{lem:bm-conv-accurate}{\normalfont \cite{CDXZ22}}
    Let $S_1$ and $S_2$ be two sets of tuples. Suppose that both $S_1$ and $S_2$ are monotone and bounded by $O(\ell)$. In $\tilde{O}(\ell^{3/2})$ time and with high probability\footnote{The original algorithm in~\cite{CDXZ22} runs in $\tilde{O}(\ell^{3/2})$ expected time. It is easy to convert it into an algorithm that succeeds with high probability. Such an algorithm is sufficient for our purpose since other parts of our algorithm only succeed with high probability.}, we can compute $S_1 \oplus S_2$.
\end{lemma}

With this algorithm, we can approximately compute the $(\max,+)$-convolution of two sets of tuples in $\tilde{O}((\frac{1}{\eps})^{3/2})$ time. The following lemma is essentially the same as that in~\cite[Lemma 28 in the full version]{BC22}, and its proof follows the idea of~\cite[Lemma 1]{Chan18}.

\begin{restatable}{lemma}{lembmconvapprox}
\label{lem:bm-conv-approx}
    Let $S_1, S_2 \subseteq (\{0\} \cup [A, B]) \times (\{0\} \cup [C, D])$ be two sets of tuples with total size $\ell$. In $\tilde{O}(\ell + (\frac{1}{\eps})^{3/2})$ time and with high probability, we can compute a set $S$ of size $\tilde{O}(\frac{1}{\eps})$ that approximates $S_1 \oplus S_2$ with factor $1 + \eps$.  Moreover, $S$ is dominated by $S_1 + S_2$.
\end{restatable}
    
\begin{proof}\textcolor{red}{TOPROVE 6}\end{proof}

Now we are ready to present an approximation algorithm for computing the $(\max,+)$-convolution of multiple sets. It follows the idea of~\cite[Lemma 2(i)]{Chan18}.
\begin{proof}\textcolor{red}{TOPROVE 7}\end{proof}


\section{Details of Approximation Errors}\label{app:err}
We prove the following two lemmas.
\lemapproxerra*
\begin{proof}\textcolor{red}{TOPROVE 8}\end{proof}

\lemapproxerrb*
\begin{proof}\textcolor{red}{TOPROVE 9}\end{proof}

\section{Details of Problem Reduction}\label{app:reducing}
Recall that the reduced problem is defined as follows.
\begin{quote}
The reduced problem $\mathrm{RP}(\alpha)$: given a set $I$ of $n$ items from $[1,2] \times [1,2]$ and a real number $\alpha \in [\frac{1}{2}, \frac{1}{4\eps}]$, compute a set $S$ that approximates ${\cal S}^+(I) \cap ([0,\frac{1}{\alpha\eps}] \times [0, \frac{1}{\alpha\eps}])$ with additive error $(\tilde{O}(\frac{1}{\alpha}), \tilde{O}(\frac{1}{\alpha}))$. Moreover, $S$ should be dominated by $\mathcal{S}(I)$.
\end{quote}

We shall prove the following lemma.
\lemreduce*

Let $(I, t)$ be a Knapsack instance. Let $\mathrm{OPT}$ be the optimal value. Recall that to obtain a weak approximation, it suffices to compute a set that approximates $\mathcal{S}^+(I) \cap ([0, t] \times [0, OPT])$ with additive error $(\eps t, \eps\cdot \mathrm{OPT})$. (Strictly speaking, the set should also be dominated by $\mathcal{S}(I)$. One can verify that this property follows quite straightforwardly. So we omit this property.) We can slightly relax the permissible additive error to $(\tilde{O}(\eps t), \tilde{O}(\eps\cdot \mathrm{OPT}))$ because an additive error of $(\tilde{O}(\eps t), \tilde{O}(\eps\cdot \mathrm{OPT}))$ can be reduced to $(\eps t, \eps\cdot \mathrm{OPT})$ by adjusting $\eps$ by a polylogarithmic factor and this increases the running time by only a logarithmic factor.

\subsection{Preprocessing}
We first preprocess the instance so that the following properties hold after the preprocessing.
\begin{itemize}
    \item $(w, p) \in [1, \frac{1}{\eps}] \times [1, \frac{1}{\eps}]$ for every item $(w, p) \in I$;

    \item $\frac{1}{2\eps} \leq \mathrm{OPT} \leq \frac{1}{\eps}$ and $t = \frac{1}{\eps}$.
\end{itemize}
The preprocessing takes $O(n \log n)$ time.

\subsubsection{Preprocessing Profits}
Knapsack has an $O(n)$-time greedy algorithm that achieves an approximation ratio of $2$~\cite[Section 2.5]{KPD04}. Let $y$ be the value of this greedy solution. Then $y \leq \mathrm{OPT} \leq 2y$. We say an item $(w, p)$ is cheap if $p \leq 2\eps y$. We shall merge cheap items into meta-items with large profits.  Let $I^c$ be the set of cheap items. We partition $I^c$ into $I^c_1, \ldots, I^c_\ell$ so that
\begin{romanenumerate}
    \item For every integer $j \in [1, \ell-1]$, the efficiency of an item in $I^c_j$ is no smaller than that of any item in $I^c_{j+1}$. That is, $\frac{p}{w} \geq \frac{p'}{w'}$ for any $(w, p) \in I^c_j$ and any $(w', p') \in I^c_{j+1}$.

    \item $2\eps y < p(I^c_j) \leq 4\eps y$ for every integer $j \in [1, \ell - 1]$ and $p(I^c_\ell) \leq 4\eps y$. 
\end{romanenumerate}
The partition can be done in $O(n \log n)$ time by scanning the items in $I^c$ in decreasing order of efficiency. The items in $I^c_\ell$ can be safely discarded. This decreases the optimal value by at most $4\eps y$.  For every remaining group, $2\eps y < p(I^c_j) \leq 4\eps y$, and we replace it with a single meta-item $(p(I^c_j), w(I^c_j))$. We claim that replacing all $I^c_j$'s with meta-items decreases the optimal value by at most $4\eps y$. To see this, consider any subset $S$ of cheap items. By selecting meta-items in decreasing order of efficiency, we can always obtain a subset $S'$ of meta-items such that $p(S') \geq p(S) - 4\eps y$ and $w(S') \leq w(S)$.

Now every item has $p_i > 2\eps y$. Without loss of generality, we assume that $p_i \leq 2y$ (since $OPT \leq 2y$). Then we scale all the profit by $2\eps y$. That is, we set $p_i:=  \frac{p_i}{2\eps y} $ for every $p_i$.  After scaling, every item has $p_i \in (1, \frac{1}{\eps}]$, and $\frac{1}{2\eps} \leq \mathrm{OPT} \leq \frac{1}{\eps}$.

\subsubsection{Preprocessing Weights}
Preprocessing weights is similar to preprocessing profits. We say an item is small if $w_i \leq \eps t$. Let $I^s$ be the set of small items.  We partition $I^s$ into $I^s_1, \ldots, I^s_\ell$ so that
\begin{romanenumerate}
    \item For every integer $j \in [1, \ell-1]$, the efficiency of an item in $I^s_j$ is no smaller than that of any item in $I^s_{j+1}$. That is, $\frac{p}{w} \geq \frac{p'}{w'}$ for any $(w, p) \in I^s_j$ and any $(w', p') \in I^s_{j+1}$.

    \item $\eps t < p(I^s_j) \leq 2\eps t$ for every integer $j \in [1, \ell - 1]$ and $p(I^s_\ell) \leq 2\eps t$.
\end{romanenumerate}
It can be done in $O(n \log n)$ time by scanning the items in $I^s$ in decreasing order of efficiency.  The items in $I^s_\ell$ can be ignored, since we can always select these items using an additional capacity of at most $2\eps t$. For every remaining group, $\eps t < p(I^s_j) \leq 2\eps t$, and we replace it with a single meta-item $(p(I^s_j), w(I^s_j))$. We claim that replacing all $I^s_j$'s with meta-items increases the given capacity by at most $2\eps t$. To see this, consider any subset $S$ of small items. By selecting meta-items in decreasing order of efficiency, we can always obtain a subset $S'$ of meta-items such that $p(S') \geq p(S)$ and $w(S') \leq w(S) + 2\eps t$.

Now every item has $w_i > \eps t$. Without loss of generality, we assume that $w_i \leq t$. Then we scale all the weights by $\eps t$. That is, we set $w_i:=  \frac{w_i}{\eps t} $ for every $w_i$.  After scaling, every item has $w_i \in (1, \frac{1}{\eps}]$, and $t = \Theta(\frac{1}{\eps})$.


\subsection{Partitioning Items into Groups}
After preprocessing, $\frac{1}{2\eps} \leq OPT \leq \frac{1}{\eps}$ and $t = \frac{1}{\eps}$. Our goal becomes computing a set that approximates ${\cal S}^+(I) \cap ([0, \frac{1}{\eps}]\times [0, \frac{1}{\eps}])$ with additive error $(\tilde{O}(1), \tilde{O}(1))$.  We shall compute such a set by the following three steps.
\begin{romanenumerate} 
    \item Recall that, after preprocessing, all the tuples in $I$ are in $[1, \frac{1}{\eps}] \times [1, \frac{1}{\eps}]$. Partition $I$ into $O(\log^2 \frac{1}{\eps})$ groups so that within each group, all item weights differ by a factor of at most $2$, and all item profits differ by a factor of at most $2$.

    \item for each group $I'$, compute a set $S'$ that approximates ${\cal S}^+(I') \cap ([0, \frac{1}{\eps}]\times [0, \frac{1}{\eps}])$ with additive error $(\tilde{O}(1), \tilde{O}(1))$. 

    \item merging the resulting sets via Lemma~\ref{lem:merging}.
\end{romanenumerate}
Step (i) costs $\tilde{O}(n)$ time. Since there are only $O(\log^2 \frac{1}{\eps})$ groups, Step (iii) costs $\tilde{O}((\frac{1}{\eps})^{3/2})$ time and increases the additive error by polylogarithmic factors.

It remains to consider Step (ii). Let $I'$ be an arbitrary group.  The tuples in $I'$ are from $[a, 2a]\times [b, 2b]$ for some $a,b \in [1, \frac{1}{2\eps}]$. Let $\alpha = \frac{\max\{a, b\}}{2}$. Note that $\alpha \in [\frac{1}{2},\frac{1}{4\eps}]$. By scaling, we can assume that every $(w, p) \in [1, 2] \times [1,2]$. After the scaling, the permissible additive error becomes $(\tilde{O}(1/a), \tilde{O}(1/b)) \geq (\tilde{O}(1/\alpha), \tilde{O}(1/\alpha))$. The part of $S^+(I')$ that needs to be approximated becomes $S^+(I') \cap ([0, \frac{1}{a\eps}]\times [0, \frac{1}{b\eps}])$.  Since the tuples in $I'$ are from $[1, 2] \times [1,2]$, we have $\frac{1}{2} \leq \frac{p}{w} \leq 2$ for every $(w, p) \in {\cal S}^+(I')$.  Therefore, 
\[
    (S^+(I') \cap ([0, \frac{1}{a\eps}]\times [0, \frac{1}{b\eps}])) \subseteq 
    S^+(I') \cap ([0, \frac{1}{\alpha\eps}]\times [0, \frac{1}{\alpha\eps}]).
\]
In conclusion, all we need to do is to approximate $S^+(I') \cap ([0, \frac{1}{\alpha\eps}]\times [0, \frac{1}{\alpha\eps}])$ with additive error $(\tilde{O}(\frac{1}{\alpha}), \tilde{O}(\frac{1}{\alpha}))$.


\section{Details of the Algorithm for Bounded Solution Size}\label{app:color-coding}
We shall prove the following lemma. 

\lemalgforlargealpha*



Since the items in $I$ are from $[1,2]\times [1,2]$, we have that  $|X| \leq \frac{1}{\alpha\eps}$ for any $X \subseteq I$ with $(w(X), p(X)) \in {\cal S}^+(I;\frac{1}{\alpha\eps})$. To deal with this case where the solution size is small, we use color coding. To reduce the running time, Bringmann~\cite{Bri17} proposed a two-layer color-coding technique, leading to an $\tilde{O}(n + t)$-time algorithm for Subset Sum. This technique was also used in later works for Subset Sum~\cite{CLMZ24cSTOCPartition,CLMZ24FOCS}.

Let $X$ be a subset of $I$ with $(w(X), p(X)) \in {\cal S}^+(I;\frac{1}{\alpha\eps})$.  Roughly speaking, if we randomly partition $I$ into $\tilde{O}(\frac{1}{\alpha\eps})$ subsets, then with high probability, each subset contains at most one element from $X$. After adding $(0,0)$ to each subset, and merging them via Lemma~\ref{lem:merging}, we can obtain a set that (approximately) contains $(w(X), p(X))$ in $\tilde{O}((\frac{1}{\eps})^{5/2}\frac{1}{\alpha})$ time. 


The standard color-coding is based on balls-and-bins analysis, and can be summarized by the following fact.
\begin{fact}\label{fact:colorcoding}
    Let $I$ be a set of items. Let $k>0$ be an integer. Let $I_1 \cup \cdots \cup I_{k^2}$ be a random partition of $I$. For any $X\subseteq I$ and $|X|\leq k$, with probability at least $1/4$, we have that
    \(
        | X \cap I_i| \leq 1
    \)
    for all $i \in \{1, \ldots, k^2\}$.
\end{fact}

\begin{lemma}\label{lem:secend-layer-color-coding-algorithm}
    Let $I$ be a set of items. Let $k>0$ be an integer. In $\tilde{O}(n + (\frac{1}{\eps})^{3/2}k^2)$ time, we can compute a set $\tilde{S}$ of size $\tilde{O}(\frac{1}{\eps})$ that for any $X \subseteq I$ with $|X| \leq k$ and $(w(X),p(X))\in \mathcal{S}^+(I)$, with high probability, $\tilde{S}$ approximates $(w(X), p(X))$ with factor $1 + O(\eps)$.
\end{lemma}
\begin{algorithm}
\caption{$\mathtt{ColorCoding}(I, k, q)$~\cite{Bri17}}
\label{alg:color-coding}
    \begin{algorithmic}[1]
    \Statex \textbf{Input:} A set of $n$ tuples $I$, a positive integer $k$, and a error probability $\delta$
    \Statex \textbf{Output:} a set of $S$
        \For{$j =1,\ldots,\lceil \log \frac{1}{q} \rceil$}
           \State Randomly partition $I$ into $k^2$ subsets $I^j_1, \ldots, I^j_{k^2}$\;
           \State Compute $\tilde{S}_j$ that approximates $(I^j_1\cup\{(0,0)\}) \oplus \cdots \oplus (I^j_{k^2}\cup\{(0,0)\})$ with factor $1+O(\eps)$ by Lemma~\ref{lem:merging}.
        \EndFor
        \State Compute $\tilde{S} = \cup_j \tilde{S}_j$
  \State \Return $\tilde{S}$
  \end{algorithmic}
\end{algorithm}
\begin{proof}\textcolor{red}{TOPROVE 10}\end{proof}

However, the running time of Algorithm~\ref{alg:color-coding} depends on $k^2$, which is too large for our purpose. So we use the two-layer color-coding technique proposed by Bringmann~\cite{Bri17}, which reduces the running time by a factor of $k$.

\begin{lemma}[\cite{Bri17}]\label{lem:first-layer-color-coding}
    Let $I$ be a set of items. Let $k>0$ be an integer, and let $q\in (0,1)$. Let $I_1,\ldots,I_{k}$ be a random partition of $I$. For any $X\subseteq I$ with $|X|\leq k$, with probability at least $1-q$, we have that
    \(
        |X\cap I_j| \leq 6\log\frac{k}{q}
    \)
    for all $j \in \{1,\ldots, k\}$.
\end{lemma}

\begin{lemma}\label{lem:first-layer-color-coding-algorithm}
    Let $I$ be a set of items. Let $k>0$ be an integer. In $\tilde{O}(n + (\frac{1}{\eps})^{3/2}k)$ time, we can compute a set $\tilde{S}$ of size $\tilde{O}(\frac{1}{\eps})$ that for any $X \subseteq I$ with $|X| \leq k$ and $(w(X),p(X))\in \mathcal{S}^+(I)$, with high probability, $\tilde{S}$ approximates $(w(X), p(X))$ with factor $1 + O(\eps)$.
\end{lemma}
\begin{proof}\textcolor{red}{TOPROVE 11}\end{proof}

Since for any $(w(X),p(X))\in {\cal S}^+(I;\frac{1}{\alpha\eps})$, $|X|\leq \frac{1}{\alpha\eps}$, we can prove Lemma~\ref{lem:alg-for-large-alpha}.

\begin{proof}\textcolor{red}{TOPROVE 12}\end{proof}


\section{Details of the Proximity Bound}\label{app:proximity}
We shall prove the following lemma.
\lemproximity*
    
\begin{proof}\textcolor{red}{TOPROVE 13}\end{proof}




\section{An Improved Algorithm with Exponent 7/4 (Full Version)}\label{app:strong}
In our $\tilde{O}(n + (\frac{1}{\eps})^{11/6})$-time algorithm, only the bad groups benefit from the proximity bound, which allows us to approximate with larger factor $1 + \frac{1}{\alpha\tau}$, while for the good groups, we simply approximate with factor $1 + \eps$. To further improve the running time, we shall partition the items in a way that all the groups can benefit from the proximity bound.

We assume that $\frac{1}{2}\leq \alpha \leq (\frac{1}{\eps})^{3/4}$ since the algorithm in Lemma~\ref{lem:alg-for-large-alpha} already runs in $\tilde{O}(n + (\frac{1}{\eps})^{7/4})$ time for all $\alpha \geq (\frac{1}{\eps})^{3/4}$. We assume that $I = \{(w_1, p_1), \ldots, (w_n, p_n)\}$ and that $\frac{p_1}{w_1} \geq \cdots \geq \frac{p_n}{w_n}$.

We first partition $I$ into the head group $I_{\mathrm{head}}$ and the tail group $I_{\mathrm{tail}}$ where $I_{\mathrm{head}}$ contains the first $\lceil \frac{1}{\alpha\eps}\rceil + 1 $ items and $I_{\mathrm{tail}}$ contains the rest of the items.

\subsection{Approximating Head Group}\label{sec:strong-head}
Let $n' = \lceil \frac{1}{\alpha\eps}\rceil + 1$.  We shall show that in $\tilde{O}((\frac{1}{\eps})^{7/4})$ time, we can compute a set $S$ of size $\tilde{O}(\frac{1}{\eps})$ that approximate $\mathcal{S}^+(I_{\mathrm{head}})$ with additive error $O(\frac{1}{\alpha})$.

Let $\tau$ be a parameter that will be specified later. Roughly speaking, we will further partition $I_{\mathrm{head}}$ into good groups and bad groups. The bad groups are the same as before: a bad group is a group of $\tau$ items whose efficiencies differ by at least $\frac{1}{\tau}$. For good groups, we will make them larger than before: when we obtain a good group $I'$ of size $\tau$, we will keep adding items to $I'$ until the difference between the maximum and the minimum item efficiencies exceeds $\frac{1}{|I'|}$ with the next item. A more precise description of the partition process is given below. 

We partition $I_{\mathrm{head}}$ into $I_1, \ldots, I_m$ as follows. Initially, $j = 1$ and $k = 1$, and the remaining items are $\{(w_k, p_k), \ldots, (w_{n'}, p_{n'})\}$. Let $k'$ be the minimum integer such that
\[
    (k' - k) \cdot (\frac{p_k}{w_k} - \frac{p_{k'}}{w_{k'}}) > 1.
\]
If such $k'$ does not exists, we set $I_j = \{(w_k, p_k), \ldots, (w_{n'}, p_{n'})\}$, set $\Delta_j = \Delta'_j = \frac{p_k}{w_k} - \frac{p_{n'}}{w_{n'}}$, set $m: = j$, and finish. Assume that $k'$ exists. 
\begin{itemize}
    \item If $k' - k \geq \tau$, we set $I_j = \{(w_k, p_k), \ldots, (w_{k' - 1}, p_{k'-1})\}$, set $\Delta_j = \frac{p_k}{w_k} - \frac{p_{k'-1}}{w_{k'-1}}$, set $\Delta'_j = \frac{p_k}{w_k} - \frac{p_{k'}}{w_{k'}}$, and proceed with $j := j + 1$ and $k: = k'$. In this case, we say $I_j$ is a good group. 

    \item Otherwise, we let $I_j = \{(w_k, p_k), \ldots, (w_{k + \tau - 1}, p_{k + \tau - 1})\}$, set $\Delta_j = \frac{p_k}{w_k} - \frac{p_{k + \tau - 1}}{w_{k+\tau - 1}}$, set $\Delta'_j = \frac{p_k}{w_k} - \frac{p_{k + \tau}}{w_{k+\tau}}$, and proceed with $j: = j + 1$ and $k:= k + \tau$. In this case, we say that $I_j$ is a bad group. 
\end{itemize}
(We remark that $\Delta_j$ and $\Delta'_j$ are not required by the algorithm. They are maintained only for the purpose of analysis. $\Delta_j$ is the actual difference between the maximum and minimum item efficiencies in $I_j$. For technical reasons, we also need $\Delta'_j$. Basically, in addition to $\Delta_j$, the gap $\Delta'_j$ also includes the efficiency gap between the minimum efficiency in $I_j$ and the maximum efficiency in $I_{j+1}$. We will use $\Delta_j$ to bound the time cost of $I_j$, and use $\Delta'_j$ to create an efficiency gap for other groups.)

We have the following observations.
\begin{observation}\label{obs:group-property}
The groups $I_1, \ldots, I_m$ satisfy the following properties.
\begin{romanenumerate}
    \item $m \leq \frac{1}{\alpha\eps\tau} + 1$

    \item $\sum_{j=1}^{m}|I_j| = |I_{\mathrm{head}}| = \frac{1}{\alpha\eps} + 1$ and $\sum_{j=1}^{m}\Delta_j \leq \sum_{j=1}^{m}\Delta'_j \leq \frac{3}{2}$.

    \item For every group $I_j$, the efficiencies of the items in $I_j$ differ by at most $\Delta_j$.

    \item For every good group $I_j$, we have $|I_j|\Delta_j \leq 1$ and $|I_j|\Delta'_j \geq \frac{1}{2}$.

    \item For every bad group $I_j$, we have $|I_j|\Delta'_j \geq |I_j|\Delta_j > 1$.

    \item For the last group $I_m$, we have $|I_m|\Delta_m \leq 1$.
\end{romanenumerate}
\end{observation}

For each group, we shall approximate it in exactly the same way as we did for the bad groups in the $\tilde{O}(n + (\frac{1}{\eps})^{11/6})$-time algorithm, except that we shall use $\frac{1}{\alpha|I_j|}$ as the accuracy parameter. More specifically, we use Corollary~\ref{coro:approx-add} and Corollary~\ref{coro:approx-del} to compute a set $S_j$ of size $\tilde{O}(\alpha|I_j|)$ that approximate each tuple $(w, p) \in \mathcal{S}^+(I_j)$ with additive error $\frac{1}{\alpha|I_j|} \cdot \min\{w, w(I) - w\}$. The time cost for $I_j$ is $\tilde{O}(|I_j| + \alpha|I_j| + \alpha^2|I_j|^2\Delta_j)$.

Then we compute a set $S_{\mathrm{head}}$ of size $\tilde{O}(\frac{1}{\eps})$ that approximates $S_1 \oplus \cdots \oplus S_m$ with factor $1+O(\eps)$ via Lemma~\ref{lem:merging}. 

\paragraph*{Bounding Time Cost}
Let $J_{\mathrm{good}}$ be the set of indices of good groups, and let $J_{\mathrm{bad}}$ be the set of indices of bad groups.
 
For the good groups and the last group, the total time cost is
\[
    \sum_{j \in J_{\mathrm{good}} \cup \{m\}} \tilde{O}(|I_j| + \alpha|I_j| + \alpha^2|I_j|^2\Delta_j) \leq \sum_{j \in J_{\mathrm{good}}\cup\{m\}}\tilde{O}( \alpha^2|I_j|)  \leq \tilde{O}( \frac{\alpha}{\eps}).
\]
The first inequality is due to that $|I_j|\Delta_j \leq 1$ (Observation~\ref{obs:group-property}(iv) and (vi)), and the last inequality is due to that $\sum_{j}|I_j| = \frac{1}{\alpha\eps} + 1$ (Observation~\ref{obs:group-property}(ii)).

For the bad groups, the total time cost is
\[
    \sum_{j \in J_{\mathrm{bad}}} \tilde{O}(|I_j| + \alpha|I_j| + \alpha^2|I_j|^2\Delta_j) \leq \sum_{j \in J_{\mathrm{bad}}} \tilde{O}(\alpha\tau + \alpha^2\tau^2\Delta_j) \leq \tilde{O}(m\alpha\tau + \alpha^2\tau^2).
\] 
The first inequality is due to that every bad group has size exactly $\tau$, and the second inequality is due to that $\sum_{j}\Delta_j \leq 3/2$ (Observation~\ref{obs:group-property}(ii)).

For the merging step, note that each set $S_j$ is of size $\tilde{O}(\alpha|I_j|)$ except for that $S_m$ is of size $\tilde{O}(\frac{1}{\eps})$. (Recall that $\sum_{j}|I_j| = \frac{1}{\alpha\eps} + 1$.) According to Lemma~\ref{lem:merging}, the time cost  is
\[
    \tilde{O}(\frac{1}{\eps} + \alpha\sum_j|I_j| + m\alpha\tau + (\frac{1}{\eps})^{3/2}m) \leq \tilde{O}(\frac{1}{\eps} + m\alpha\tau + (\frac{1}{\eps})^{3/2}m).
\]

So the total time cost to approximate $\mathcal{S}^+(I_{\mathrm{head}})$ is 
\[
    \tilde{O}(\frac{\alpha}{\eps}) + \tilde{O}(m\alpha\tau + \alpha^2\tau^2) +  \tilde{O}(\frac{1}{\eps} + m\alpha\tau + (\frac{1}{\eps})^{3/2}m)\leq \tilde{O}(\frac{\alpha}{\eps}+m\alpha\tau+\alpha^2\tau^2+(\frac{1}{\eps})^{3/2}m).
\]
Recall our assumption that $\frac{1}{2}\leq \alpha \leq (\frac{1}{\eps})^{3/4}$ and Observation~\ref{obs:group-property}(i) that $m \leq \frac{1}{\alpha\tau\eps} + 1$. By setting $\tau = \lceil (\frac{1}{\eps})^{5/6}\frac{1}{\alpha}\rceil$, one can verify that all the above time costs are bounded by $\tilde{O}((\frac{1}{\eps})^{7/4})$.

\paragraph*{Bound Additive Error}
We shall show that $S_{\mathrm{head}}$ approximates every tuple $(w^*, p^*) \in \mathcal{S}^+(I_{\mathrm{head}})$ with additive error $\tilde{O}(\frac{1}{\alpha})$. 

Let $(w^*, p^*)$ be an arbitrary tuple in $\mathcal{S}^+(I_{\mathrm{head}})$. Let $(w_b, p_b)$ be the breaking item with respect to $w^*$. Let $\rho_b = \frac{p_b}{w_b}$ be the efficiency of the breaking item. 

To apply the proximity bound, let $I_{j'}$ be the group containing the breaking item $(w_b, p_b)$. To apply the proximity bound, we will show that the groups can be divided into  $O(\log \frac{1}{\eps})$ collections so that for each collection $\{I_{j_{k} + 1}, I_{j_{k} + 2}, \ldots, I_{j_{k+1}-1} \}$,  the efficiency gap between $\rho_b$ and the groups in this collection is at least the maximum $\Delta'_j$ of these groups. To do this, we need the following auxiliary lemma.

\begin{lemma}\label{lem:cluster}
    Let $\Delta_1, \ldots, \Delta_n$ be a sequence of positive real numbers. Let $\Delta_{\min}$ and $\Delta_{\max}$ be the minimum and maximum numbers in the sequence. There exists $h = O(\log \frac{\Delta_{\max}}{\Delta_{\min}})$ indices $1 = j_1 < j_2 < \cdots < j_h = n$ such that for any $k \in \{1, \ldots, h-1\}$, we have that
    \begin{equation}\label{eq:cluster-original}
            \max\{\Delta_j : j_k < j < j_{k+1}\} \leq \sum\{\Delta_j : j_{k+1} \leq j \leq n\},
    \end{equation}
    where the maximum of an empty set is defined to be $0$.
\end{lemma}
\begin{proof}\textcolor{red}{TOPROVE 14}\end{proof}


\begin{lemma}
    $S_1 \oplus \cdots \oplus S_m$ approximates $(w^*, p^*)$ with additive error $\tilde{O}(\frac{1}{\alpha})$.
\end{lemma}
\begin{proof}\textcolor{red}{TOPROVE 15}\end{proof}

The above lemma immediately implies the following since $S_{\mathrm{head}}$ approximates $ S_1 \oplus \cdots \oplus S_m$ with factor $1 + O(\eps)$.
\begin{corollary}
    $S_{\mathrm{head}}$ approximates $(w^*, p^*)$ with additive error $\tilde{O}(\frac{1}{\alpha})$.
\end{corollary}


We summarize this subsection by the following lemma.
\begin{lemma}\label{lem:approx-head}
    In $\tilde{O}((\frac{1}{\eps})^{7/4})$ time and with high probability, we can compute a set of size $\tilde{O}(\frac{1}{\eps})$ that approximates $\mathcal{S}^+(I_{\mathrm{head}})$ with additive error $\tilde{O}(\frac{1}{\alpha})$.
\end{lemma}

\subsection{Approximating the Tail Group}
Recall that the tail group $I_{\mathrm{tail}}$ contains $\{(w_{\lceil \frac{1}{\alpha\eps}\rceil + 2}, p_{\lceil \frac{1}{\alpha\eps}\rceil + 2}), \ldots, (w_n, p_n)\}$. We partition $I_{\mathrm{tail}}$ into $m = O(\log \frac{1}{\eps})$ groups $I_0, \ldots, I_m$ so that $I_j$ is a maximal group of items whose efficiencies differ by at most $2^{j} \alpha\eps$. See Algorithm~\ref{alg:tail-group} for details.

\begin{algorithm}
\caption{$\mathtt{PartitionTailGroup}(I_{\mathrm{tail}})$}
\label{alg:tail-group}
    \begin{algorithmic}[1]
        \State $i = \lceil \frac{1}{\alpha\eps}\rceil + 2$, $j = 0$ and $I_j = \emptyset$
        \While{$i \leq n$}
            \If{the item efficiencies of $I_j\cup \{(w_i, p_i)\}$ differ by at most $2^{j-1}\alpha\eps$}
                \State $I_j := I_j \cup \{(w_i, p_i)\}$ and $i:= i+1$
            \Else
                \State $j:= j + 1$ and $I_j := \emptyset$ 
            \EndIf  
        \EndWhile
        \State m := j
        \State \Return $I_1, \ldots, I_{m}$
  \end{algorithmic}
\end{algorithm}

For each group $I_j$, we approximate it with accuracy parameter $2^j\eps$.  More precisely, we compute a set $S_j$ of size $\tilde{O}(\frac{1}{2^j\eps})$ that approximates each tuple $(w, p) \in \mathcal{S}^+(I_j)$ with additive error $2^j\eps w$ using Corollary~\ref{coro:approx-add} in time $\tilde{O}(|I_j| + \frac{1}{2^j\eps} + (\frac{1}{2^j\eps})^2 \cdot 2^j \alpha\eps)$.

Then we compute a set $S_{\mathrm{tail}}$ that approximates $S_0 \oplus \cdots \oplus S_m$ with factor $1 + O(\eps)$ via Lemma~\ref{lem:merging}. Note that the total size of $S_1, \ldots, S_m$ is $\tilde{O}(\frac{1}{\eps})$.  Since $m = O(\log \frac{1}{\eps})$, the time cost of Lemma~\ref{lem:merging} is $\tilde{O}((\frac{1}{\eps})^{3/2})$.

\paragraph*{Bounding Time Cost}
It is easy to see that the total time cost is 
\[
    \tilde{O}((\frac{1}{\eps})^{3/2}) + \sum_{j} \tilde{O}(|I_j| + \frac{1}{2^j\eps} + (\frac{1}{2^j\eps})^2 \cdot 2^j \alpha\eps) = \tilde{O}(n + (\frac{1}{\eps})^{3/2} + \frac{\alpha}{\eps}).
\]
Recall that $\frac{1}{2}\leq \alpha\leq (\frac{1}{\eps})^{3/4}$. The total time cost is $\tilde{O}(n + (\frac{1}{\eps})^{7/4})$.

\paragraph*{Bounding Additive Error}
Let $(w^*, p^*)$ be an arbitrary tuple in $\mathcal{S}^+(I; \frac{1}{\alpha\eps})$. Let $(w_b, p_b)$ be the breaking item with respect to $w^*$. Let $\rho_b = \frac{p_b}{w_b}$ be the efficiency of the breaking item. Note that $b \leq \frac{1}{\alpha\eps} + 1$, so $b \in I_{\mathrm{head}}$. Recall that $S_{\mathrm{head}}$ is the set we computed by Lemma~\ref{lem:approx-head}.

\begin{lemma}
    $S_{\mathrm{head}} \oplus S_0 \oplus S_1 \oplus \cdots \oplus S_m$ approximates $(w^*, p^*)$ with additive error $\tilde{O}(\frac{1}{\alpha})$.
\end{lemma}
\begin{proof}\textcolor{red}{TOPROVE 16}\end{proof}

The above lemma immediately implies the following since $S_{\mathrm{tail}}$ approximates $S_0 \oplus S_1 \oplus \cdots \oplus S_m$ with factor $1 + O(\eps)$.
\begin{corollary}
    $S_{\mathrm{head}} \oplus S_{\mathrm{tail}}$ approximates $(w^*, p^*)$ with additive error $\tilde{O}(\frac{1}{\alpha})$.
\end{corollary}

\subsection{Putting Things Together}
We summarize this section with the following lemma.
\begin{lemma}
   There is an $\tilde{O}({n+(\frac{1}{\eps})^{7/4}})$-time algorithm for the reduced problem, which is randomized and succeeds with high probability.
\end{lemma}


\end{document}
