\documentclass[11pt]{article}

\usepackage{graphicx} 

\usepackage{amsmath, amsfonts, amssymb, amsthm}

\usepackage{CJKutf8}
\usepackage{fullpage}

\usepackage{graphicx}
\graphicspath{{figs/}}
\usepackage{amsmath,amssymb,amsfonts, amsthm}
\usepackage{xcolor}
\usepackage{xspace}
\usepackage{thm-restate}
\usepackage{mathtools}
\usepackage{algorithmicx} 

\usepackage[colorlinks,linkcolor=blue,filecolor=blue,citecolor=blue,urlcolor=blue,pagebackref]{hyperref}

\usepackage{soul}

\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{bbold}

\algtext*{EndWhile}
\algtext*{EndIf}
\algtext*{EndFor}
\algrenewcommand\algorithmicrequire{\textbf{Input:}}
\algrenewcommand\algorithmicensure{\textbf{Output:}}

\usepackage[colorinlistoftodos,textsize=tiny,textwidth=2cm,color=red!25!white,obeyFinal]{todonotes}
\newcommand{\snote}[1]{\todo[color=blue!25!white]{SL: #1}\xspace}
 \newcommand{\snoteinline}[1]{\todo[inline,color=blue!25!white]{SL: #1}\xspace}
\newcommand{\lnote}[1]{\todo[color=orange!25!white]{LV: #1}\xspace}


\usepackage{enumitem}
\setlist{leftmargin=*, topsep=3pt, itemsep=3pt}

\newcommand{\R}{\mathbb{R}}
\newcommand{\cost}{\mathrm{cost}}
\newcommand{\obj}{\mathrm{obj}}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{coro}[theorem]{Corollary}

\newtheorem{remark}{Remark}

\newcommand{\ceil}[1]{{\left\lceil#1\right\rceil}}
\newcommand{\floor}[1]{{\left\lfloor#1\right\rfloor}}


\DeclareMathOperator*{\E}{{\mathbb{E}}}
\DeclareMathOperator*{\union}{\bigcup}

\newcommand{\calA}{{\mathcal{A}}}
\newcommand{\calC}{{\mathcal{C}}}
\newcommand{\calK}{{\mathcal{K}}}
\newcommand{\calP}{{\mathcal{P}}}
\newcommand{\calQ}{{\mathcal{Q}}}

\newcommand{\heavy}{{\mathrm{heavy}}}
\newcommand{\light}{{\mathrm{light}}}
\newcommand{\clique}{{\mathrm{clique}}}
\newcommand{\poly}{{\mathrm{poly}}}
\newcommand{\alp}{\text{Admissible-LP}}

\newcommand{\tG}{\tilde{G}}

\newcommand{\pedge}{{$+$edge}\xspace}
\newcommand{\medge}{{$-$edge}\xspace}
\newcommand{\pedges}{{$+$edges}\xspace}
\newcommand{\medges}{{$-$edges}\xspace}

\newcommand{\prepinstance}{(\calK, E_{\adm})}

\newcommand{\ppp}{$+\!+\!+$\xspace}
\newcommand{\ppm}{$+\!+\!-$\xspace}
\newcommand{\pmm}{$+\!-\!-$\xspace}
\newcommand{\mmm}{$-\!-\!-$\xspace}
\newcommand{\mmp}{$-\!-\!+$\xspace}

\newcommand{\adm}{\mathrm{adm}}
\newcommand{\err}{\mathrm{err}}

\newcommand{\opt}{{\mathrm{opt}}}
\newcommand{\optsdp}{{\mathrm{OPT}_{\mathrm{SDP}}}}

\newcommand{\da}{d^{adm}}

\newcommand{\epsrt}{\varepsilon_{\mathrm{rt}}}
\newcommand{\cc}{{Correlation Clustering}\xspace}
\newcommand{\hardnessratio}{24/23}





\newcommand{\cupdot}{\uplus}

\newcommand{\costr}{\mathrm{cost}^r}
\newcommand{\costi}{\mathrm{cost}^i}
\newcommand{\costs}{\mathrm{cost}^s}
\newcommand{\costf}{\mathrm{cost}^f}
\newcommand{\ball}{\mathrm{Ball}}
\newcommand{\profit}{\mathrm{profit}}
\newcommand{\onefive}{\frac{1}{5}}
\newcommand{\twofive}{\frac{2}{5}}
\newcommand{\threefive}{\frac{3}{5}}
\newcommand{\fourfive}{\frac{4}{5}}
\newcommand{\topk}{\mathrm{top}_k}
\newcommand{\order}{\mathrm{order}}

\newcommand{\lp}{\Delta}
\newcommand{\lpr}{\Delta^r}
\newcommand{\lpi}{\Delta^i}
\newcommand{\lps}{\Delta^s}
\newcommand{\lpf}{\Delta^f}

\newcommand{\chbudget}{1.49}
\newcommand{\sdpbudget}{1.437}






\newcommand{\eps}{\varepsilon}

\newcommand{\epsone}{\eps_1}

\newcommand{\epstwo}{\eps_2}


\newcommand{\epsfour}{\eps}

\newcommand{\epsthree}{\eps_3}

\newcommand{\epsfive}{\eps_5}

\newcommand{\epsq}{\eps_q}

\newcommand{\epsa}{\eps_a}

\newcommand{\diff}{d}





\newcommand{\bpb}{1.49}  

\newcommand{\bb}{1.447}  

\newcommand{\piv}{\mathrm{piv}}   

\newcommand{\Kp}{K^+} 

\newcommand{\Km}{K^-} 

\newcommand{\nairen}[1]{\noindent{\bf {\color{orange!80!black}{\sc Nairen:}  
#1}}}

\newcommand{\shi}[1]{\noindent{\bf {\color{blue!80!black}{\sc Shi:}  
#1}}}


\newenvironment{cproof}
{\begin{proof}\textcolor{red}{TOPROVE 0}\end{proof}}


\allowdisplaybreaks


\newcommand{\calT}{{\mathcal{T}}}

\newcommand{\OPT}{\mathrm{OPT}}

 
\begin{document}\begin{CJK*}{UTF8}{gbsn} 


\title{Simultaneously Approximating All Norms for Massively Parallel Correlation Clustering}

\author{ 
{Nairen Cao \footnote{The work of NC was supported by NSF grant CCF-2008422.}} \\ Department of Computer Science \\ Boston College \\ Chestnut Hill, MA, United States \\ 
\href{mailto:nc1827@nyu.edu}{nc1827@nyu.edu} 
\and Shi Li \footnote{The work of SL and JY was supported by the State Key Laboratory for Novel Software Technology, and the New Cornerstone Science Laboratory.} \\ School of Computer Science, \\ Nanjing University, \\ Nanjing, Jiangsu Province, China \\ \href{mailto:shili@nju.edu.cn}{shili@nju.edu.cn} \and Jia Ye \footnotemark[\value{footnote}] \\ School of Computer Science, \\ Nanjing University, \\ Nanjing, Jiangsu Province, China \\\href{mailto:jiaye@smail.nju.edu.cn}{jiaye@smail.nju.edu.cn}}
\date{}


\maketitle

\begin{abstract}
We revisit the simultaneous approximation model for the correlation clustering problem introduced by Davies, Moseley, and Newman~\cite{davies2023one}. The objective is to find a clustering that minimizes given norms of the disagreement vector over all vertices. 

We present an efficient algorithm that produces a clustering that is simultaneously a $63.3$-approximation for all monotone symmetric norms. This significantly improves upon the previous approximation ratio of $6348$ due to Davies, Moseley, and Newman~\cite{davies2023one}, which works only for $\ell_p$-norms. 

To achieve this result, we first reduce the problem to approximating all top-$k$ norms simultaneously, using the connection between monotone symmetric norms and top-$k$ norms established by Chakrabarty and Swamy \cite{chakrabarty2019approximation}. Then we develop a novel procedure that constructs a $12.66$-approximate fractional clustering for all top-$k$ norms. Our $63.3$-approximation ratio is obtained by combining this with the $5$-approximate rounding algorithm by Kalhan, Makarychev, and Zhou~\cite{kalhan2019correlation}.

We then demonstrate that with a loss of $\epsilon$ in the approximation ratio, the algorithm can be adapted to run in nearly linear time and in the MPC (massively parallel computation) model with poly-logarithmic number of rounds. 

By allowing a further trade-off in the approximation ratio to $(359+\epsilon)$, the number of MPC rounds can be reduced to a constant. \end{abstract}



\thispagestyle{empty} 
\clearpage

\tableofcontents
\clearpage

\section{Introduction}
Clustering is a classic problem in unsupervised machine learning. It aims to classify a given set of data elements based on their similarities, with the goal of maximizing the similarity between elements within the same class and minimizing the similarity between elements in different classes. Among the various graph clustering problems, correlation clustering stands out as a classic model. Initially proposed by Bansal, Blum, and Chawla \cite{BBC04}, the model has numerous practical applications, including automated labelling \cite{chakrabarti2008graph,agrawal2009generating}, community detection and mining \cite{chen2012clustering,DBLP:conf/www/VeldtGW18,shi2021scalable}, and disambiguation task \cite{kalashnikov2008web}, among others.

The input of the standard correlation clustering problem is a complete graph over a set $V$ of $n$ vertices, where edges are partitioned into  the set $E^+$  of $+$edges and the set $E^-$ of $-$edges.  The output of the problem is a clustering  (or a partition) $\calC$ of $V$ that minimizes the number of edges in disagreement: an edge $uv \in {V \choose 2}$ is in disagreement if $uv \in E^+$ but $u$ and $v$ are in different clusters in $\calC$, or $uv \in E^-$ but $u$ and $v$ are in a same cluster in $\calC$.  Throughout the paper, we shall use a graph $G = (V, E)$ to denote a correlation clustering instance, with $E$ being $E^+$ and ${V \choose 2} \setminus E$ being $E^-$. 

This problem is known to be APX-Hard \cite{CGW05}. There has been a long stream of $O(1)$-approximation algorithms for the problem \cite{BBC04,CGW05,ACN08,CMSY15,CLN22,CLLN23, cao2024understanding}, with the current best approximation ratio being $1.437$~\cite{cao2024understanding}. In the same paper, the authors presented an improved hardness of 24/23 for the problem, which also made the constant explicit. 


Besides the standard setting, other objectives have been studied recently, with the goal of minimizing some norm of the \textit{disagreement vector} of the clustering $\calC$ over vertices. For a clustering $\mathcal{C}$ of $V$, the disagreement vector of $\calC$ is defined as $\cost_{\calC} \in \mathbb{Z}_{\ge 0}^{n}$, where $\cost_{\calC}(u)$ for every $u \in V$ is the number of edges incident to $u$ that are in disagreement with respect to $\mathcal{C}$. Given some norm $f:\R_{\geq 0}^n \to \R_{\geq 0}$ \footnote{This means $f$ satisfies $f(\alpha x) = \alpha f(x)$ for every real $\alpha \geq 0$ and $x \in \R_{\geq 0}^n$, and $f(x + y) \leq f(x) + f(y)$ for every $x, y \in \R_{\geq 0}^n$}, the goal of the problem is to minimize $f(\cost_{\calC})$. Notice that the standard correlation clustering problem corresponds to the case where $f$ is the $\ell_1$ norm.


Puleo and Milenkovic~\cite{puleo2016correlation} initiated the study of correlation clustering with the goal of minimizing the $\ell_p$ norm of the disagreement vector, where $p \in [1, \infty]$. They proved that the problem is NP-hard for the $\ell_\infty$-norm objective. Given a fixed $p \in [1, \infty]$, for the $\ell_p$-norms objective, they gave a $48$-approximation algorithm. The approximation ratio was subsequently improved by Charikar, Gupta, and Schwartz \cite{charikar2017local} to $7$ for the $\ell_\infty$-norm, and by Kalhan, Makarychev and Zhou \cite{kalhan2019correlation} to $5$ for the $\ell_p$-norm with any fixed $p \in [1, \infty]$. Very recently, Heidrich, Irmai, and Andres~\cite{heidrich20244} improved the approximate ratio to 4 for the $\ell_\infty$-norm.

Davies, Moseley and Newman \cite{davies2023one} introduced the concept of simultaneous approximation for all $\ell_p$-norms. They developed an efficient algorithm that outputs a single clustering $\calC$, which is simultaneously an $O(1)$-approximation for the $\ell_p$ norm for all $p \in [1, \infty]$. This is rather surprising, as it was not known a priori whether such a clustering $\calC$ even exists. To achieve the goal, they first construct a fractional clustering $x$ that is simultaneously an $O(1)$-approximation for all $\ell_p$ norms and then use the $5$-approximate rounding algorithm of Kalhan, Makarychev, and Zhou~\cite{kalhan2019correlation} to round $x$ into an integral clustering $\calC$.  Crucially, the algorithm of \cite{kalhan2019correlation}  guarantees a per-vertex $5$-approximation, meaning that $\cost_\calC(u)$ is at most $5$ times the fractional number of edges in disagreement incident to $u$, for every $u \in V$.  This strong property is necessary to obtain the final simultaneous $O(1)$-approximation in \cite{davies2023one}.




In light of the growing networks, it is imperative to develop efficient parallel algorithms. This urgency is particularly pronounced in machine learning and data mining applications, where timely and efficient processing is essential for extracting meaningful insights from vast datasets.
Many works in the literature aim to design efficient parallel algorithms \cite{blelloch2012greedy, chierichetti2014correlation, DBLP:conf/nips/PanPORRJ15, fischer2019tight, DBLP:conf/wdag/CambusCMU21, cohen2021correlation, DBLP:conf/innovations/Assadi022, cambus20243+, cao2024breaking}. 
The MPC model, as a theoretical abstraction of several real-world parallel models such as MapReduce \cite{dean2008mapreduce}, is a prevalent methodology employed in these works.








\subsection{Our results}
In this paper, we revisit and generalize the simultaneous approximation model for the correlation clustering that was introduced by \cite{davies2023one}. Instead of considering only $\ell_p$ norms, we consider all \emph{monotone symmetric norms}. We say a norm $f:\R_{\geq 0}^n \to \R_{\geq 0}$ is monotone if for every $x, y \in \R_{\geq 0}^n$ with $x \leq y$, we have $f(x) \leq f(y)$. We say $f$ is symmetric if $f(x) = f(x')$ for every $x, x' \in \R_{\geq 0}^n$ such that $x'$ is a permutation of $x$. Such norms were considered in \cite{chakrabarty2019approximation} in the context of load balancing and clustering.  Our first result is that there exists simultaneous $O(1)$-approximation for all monotone symmetric norms for correlation clustering and it can be constructed in polynomial time.
\begin{definition}
    Given a correlation clustering instance $G = (V, E)$ and $\alpha \geq 1$, we say a clustering $\calC$ over $V$ is simultaneously $\alpha$-approximate, or a simultaneous $\alpha$-approximation, for a family $F$ of norms, if we have $f(\cost_\calC) \leq \alpha \cdot f(\cost_{\OPT_f})$ for every $f \in F$, where $\OPT_f$ is the optimum clustering for $G$ under norm $f$.
\end{definition}

\begin{theorem}
\label{thm:sequentialAlgorithm}
    Given a correlation clustering instance $G = (V, E)$, in polynomial time we can construct a simultaneous $63.3$-approximate clustering $\calC$ for the family of monotone symmetric norms.
\end{theorem}

Next, we are concerned with the running time of the algorithm and its implementation under the MPC model. To state the result, we need a formal description of the MPC model. 

\paragraph{The MPC model.} In the MPC model, data is distributed across a set of machines, and computation proceeds in synchronous rounds. During each round, each machine first receives messages from other machines, then performs computations based on this information and its own allocated memory, and finally sends messages to other machines to be received at the start of the next round. Each machine has limited local memory, restricting the total number of messages it can receive or send in a round. The efficiency of the algorithm is measured by the number of rounds, the memory used by each machine, the total memory used by all machines, and the running time over all machines, also known as the total work.

In this paper, we consider the MPC model in the \emph{strictly sublinear regime}: Each machine has $O(n^\delta)$ local memory, where $n$ is the input size and $\delta > 0$ is a constant that can be made arbitrarily small. Under this model, we assume the input received by each machine has size $O(n^\delta)$. 


We then describe the correlation clustering problem under the MPC model in the strictly sublinear regime. We use $n = |V|$ and $m = |E|$ to denote the number of vertices and edges respectively in the input graph $G = (V, E)$. The edges $E$ are distributed across the machines, where each machine has $O(n^\delta)$ memory for a constant $\delta > 0$ which can be made arbitrarily small. At the end of the algorithm, each machine needs to store in its local memory the IDs of the clusters for all the vertices incident to its assigned edges.

Our main result regarding MPC algorithm is given as follows,



\begin{theorem}
\label{thm:mainthmlp}
    Let $\epsilon \in (0, 1)$. 
There exists a randomized MPC algorithm in the strictly sublinear regime that, given a correlation clustering instance $G = (V, E)$, in $O(\log^3 n)$ rounds outputs a simultaneous $(63.3 + O(\epsilon))$ clustering for $G$ for all monotone symmetric norms. This algorithm succeeds with high probability. It uses $\tilde{O}(m / \epsilon^6)$ total memory and $\tilde{O}(m / \epsilon^6)$ total work.\footnote{As usual, we use $\tilde O(\cdot)$ to hide a poly-logarithmic factor in the input size.}
\end{theorem}



In particular, the algorithm can be converted into a nearly linear time algorithm that with high probability outputs a $(63.3+O(\epsilon))$-simultaneous approximation for all monotone symmetric norms. \smallskip

Along the way, we develop an MPC rounding algorithm with a per-vertex $(5 + 55\epsilon)$ approximation guarantee, based on the sequential algorithm due to \cite{kalhan2019correlation}. Given its potential independent interest, we state it here for future references.
\begin{restatable}{theorem}{thmroundingmain}
\label{thm:roundingmaintheorem}
Let $\epsilon \in (0,1)$ be a constant. Given a graph $G = (V, E)$ and a set of LP value $( x_{uv} )_{u,v \in V}$ satisfying the approximate triangle inequality, that is, for any $u,v,w \in V$, we have $x_{uv} + x_{uw} + \epsilon \geq x_{vw}$. Let $y_u = \sum_{uv \in E} x_{uv} + \sum_{uv \in {V \choose 2} \setminus E}(1 - x_{uv})$ be the LP disagreement for node $u$. There exists an MPC algorithm that computes a clustering $\calC$ such that for any node $u$, we have 
\begin{align*}
    \cost_{\calC}(u) \leq (5 + 55\epsilon) y_u.
\end{align*}

This algorithm always succeeds but terminates in $O(\log^3 n / \epsilon)$ rounds with high probability and requires $O(n^\delta)$ memory per machine. Moreover, let $K = E \cup \{ uv \in {V \choose 2} \setminus E \mid x_{uv} < 1 \}$ be the set of $+$edges and $-$edges whose LP value is less than 1. The algorithm uses a total memory of $O(|K|\log n)$ and a total work of $O(|K|\log^3 n /\epsilon)$.
\end{restatable}



The $O(\log^3 n)$ round in the above theorem might not be desirable for many applications. Our next result shows that we can reduce the number of rounds to $O(1)$, albeit with a worse $O(1)$ approximation ratio:
\begin{restatable}{theorem}{thmconstantMPCAlgorithmForCC} \label{thm:constantMPCAlgorithmForCC}
Let $\epsilon \in (0, 1)$ be a constant.
There exists a randomized MPC algorithm in the strictly sublinear regime that, given a correlation clustering instance $G = (V, E)$, in $\mathbf{O(1)}$ rounds outputs a clustering that is simultaneously a $(359 + \epsilon)$-approximation, for all monotone symmetric norms. This algorithm succeeds with high probability, and uses a total memory of $\tilde{O}(m / \epsilon^2)$ and a total work of $\tilde{O}(m / \epsilon^2)$.
\end{restatable}



Overall, relative to \cite{davies2023one}, our algorithms demonstrate the following improvements.
\begin{enumerate}[leftmargin=*]
    \item We generalize the family of norms for the simultaneous approximation from $\ell_p$ norms to all monotone symmetric norms.
    \item We obtain a simpler construction, which leads to a much smaller approximation ratio. Using a result from \cite{chakrabarty2019approximation}, to simultaneously approximate all monotone symmetric norms, it suffices to approximate all top-$k$ norms: the top-$k$ norm of a non-negative vector is the sum of its largest $k$ coordinates. Though being more general mathematically, the top-$k$ norms are more convenient to deal with compared to $\ell_p$ norms. 
    \item We can make our algorithm run in nearly linear time. This is the first nearly-linear time simultaneous $O(1)$-approximation algorithm for the problem, even when we restrict to $\ell_p$ norms. In contrast, the algorithm of \cite{davies2023one} runs in nearly linear time only when the graph $G$ has $O(1)$ maximum degree.
    \item We can make our algorithm run in the MPC model with $O(1)$ rounds. Our work is the first to consider the problem in the MPC model. 
\end{enumerate}

\subsection{Overview of Techniques} We then discuss our techniques for each of our main results. 
\paragraph{Polynomial Time Construction of Simultaneous $O(1)$-Approximation for All Symmetric Norms} By \cite{chakrabarty2019approximation}, we can reduce the problem of approximating all monotone symmetric norms to approximating all top-$k$ norms.  We then construct a fractional solution $x$, which is a metric over $V$ with range $[0, 1]$, such that the fractional disagreement vector for $x$ has top-$k$ norm at most $12.66 \cdot \opt_k$ for any $k \in [n]$, where $\opt_k$ is the cost of the optimum clustering under the top-$k$ norm. Then, we can use the 5-approximate rounding algorithm of KMZ \cite{kalhan2019correlation}, to obtain a simultaneous $63.3$-approximation for all top-$k$ norms.  The KMZ rounding algorithm has two crucial properties that we need: it does not depend on $k$ and it achieves a per-vertex guarantee. 

We elaborate more on how to construct the metric $x: {V \choose 2} \to [0, 1]$. 
A natural idea to assign the LP values, that was used by~\cite{davies2023one}, is to set $x_{uv}$ based on the intersection of the neighborhood between $u$ and $v$. Intuitively, the more common neighbors two nodes share, the closer they should be.  A straightforward approach to implementing this idea is to set $x_{uv} = 1 - \frac{|N(u) \cap N(v)|}{\max(d(u), d(v))}$, where $N(u)$ denotes the neighboring nodes of $u$ in $G$ and $d(u) = |N(u)|$ denotes the degree of $u$; it is convenient to assume $u \in N(u)$. This approach works for the top-$1$ norm (i.e., the $\ell_\infty$ norm) as discussed in \cite{davies2023fast}, but fails for the top-$n$ norm (i.e., the $\ell_1$ norm). Consider a star graph, where the optimal clustering under the top-$n$ norm has a cost of $n - 2$. This approach will assign $x_{uv} = 1 - \frac{1}{2} = 1/2$ for all $-$edges, leading to an LP cost of $\Theta(n^2)$ and a gap of $\Omega(n)$. \cite{davies2023one} addressed the issue by rounding up LP values to $1$ for $-$edge, if for a given node, its total $-$edges LP disagreement is larger than the number of its $+$edges. After the transformation, the triangle inequalities are only satisfied approximately, but this can be handled with $O(1)$ loss in the approximation ratio. 

We address this issue using a different approach, that is 
inspired by the pre-clustering technique in \cite{cohen2021correlation}. We first preprocess the graph $G$ by removing edges $uv \in E$ for which $|N(u) \cap N(v)|$ is small compared to $\max\{d(u), d(v)\}$. Let the resulting graph be $H$. We then set our LP values as $x_{uv} = 1 - \frac{|N_H(u) \cap N_H(v)|}{\max\{d(u), d(v)\}}$ if $u \neq v$, where $N_H(u)$ is the set of neighbors of $u$ in $H$.  We show that this solution is a $12.66$-approximation for all top-$k$ norms simultaneously. When compared to \cite{davies2023one}, in addition to the improved approximation ratio, we obtain a considerably simpler analysis.

\paragraph{Implementation of Algorithm in Nearly-Linear Time and in MPC Model} We then proceed to discuss our techniques to improve the running time of the algorithm to nearly-linear. The algorithm contains two parts: the construction of the fractional solution $x$ and the rounding procedure. We discuss the two procedures separately. 

Constructing $x$ in nearly linear time poses several challenges. First, the construction of the subgraph $H$ requires us to identify edges $uv \in E$ with small $|N(u) \cap N(v)|$. Second, we can not explicitly assign $x$ values to all $-$edges. Finally, to compute $x_{uv}$, we need to compute $|N_H(u) \cap N_H(v)|$. 

The first and third challenges can be addressed through sampling, with an $O(\log n)$ factor loss in the running time. To avoid considering too many $-$edges, we only consider $-$edges with length at most $1-\epsilon$. Consequently, we only need to consider $-$edges whose other endpoints share at least an $\epsilon$ fraction of neighbors with $u$. Given that each neighbor of $u$ in $H$ has degree similar to $u$, we demonstrate that there will be at most $O(d(u) / \epsilon)$ $-$edges to consider for each node $u$. Overall, there will be $\tilde O(m \cdot \poly(1/\epsilon))$ $-$edges for which we need to explicitly assign $x$ values.  Moreover, the nearly-linear time algorithm for the construction of $x$ can be naturally implemented in the MPC model, with $O(1)$ number of rounds.  \smallskip

Then we proceed to the rounding algorithm for $x$. We are explicitly given the $x$ values for $+$edges, and for nearly-linear number of $-$edges. For other $-$edges, their $x$ values are $1$. 
The KMZ algorithm works as follows: in each round, the algorithm selects a node $u$ as the cluster center and then includes a ball with some radius, meaning the algorithm includes all nodes $v$ such that $x_{uv} \leq \textmd{radius}$ into the cluster, removes the clustered nodes, and repeats the process on the remaining nodes. The rounding algorithm can be easily implemented in nearly-linear time using a priority queue structure.  This leads to a nearly-linear time simultaneous $O(1)$-approximation for correlation clustering for all monotone symmetric norms. 

The challenge to implement the algorithm in MPC model is the sequential nature of the algorithm. \cite{kalhan2019correlation} observes that in each round, if we select the nodes that maximize $L(u) = \sum_{x_{uv} \leq r}(r - x_{uv})$ as cluster center, we can effectively bound each node's algorithmic cost, where $r$ is the final ratio. However, choosing a node that maximizes some target inherently makes the process sequential. Our key observation is that, instead of selecting the node that maximizes $L(u)$, we can allow some approximation. This strategy still permits achieving a reasonable approximate ratio with an additional $1 + \epsilon$ overhead while allowing the selection of multiple nodes as cluster centers, thereby parallelizing the rounding process. In each round, there might be several candidate cluster centers with conflicts. To resolve these conflicts, we employ the classical Luby's algorithm~\cite{luby1985simple, chierichetti2014correlation} to find a maximal independent set, ensuring that none of the cluster centers have conflicts.











 
\paragraph{Organization} We give some 
preliminary remarks in Section~\ref{sec:prelim}. In Section~\ref{sec:top-k}, we describe our simultaneous $O(1)$-approximation algorithm for correlation clustering for all top-$k$ norms. The reduction from any monotone symmetric norm to top-$k$ norms is deferred to Appendix~\ref{sec:all-norm}. Combining the results leads to a simultaneous $O(1)$-approximation algorithm for all monotone symmetric norms. Then in Section~\ref{sec:nearly-linear} and \ref{sec:MPC-solve-rounding}, we show how we can run the algorithm in the MPC model with nearly linear work. In particular, Section~\ref{sec:nearly-linear} and \ref{sec:MPC-solve-rounding} discuss how to solve the LP and round the LP solution in the MPC model, respectively.  The constant round MPC algorithm is described in Section~\ref{sec:MPC-solve-LP}. Theorem \ref{thm:sequentialAlgorithm}, \ref{thm:mainthmlp}, \ref{thm:roundingmaintheorem} and \ref{thm:constantMPCAlgorithmForCC} are proved in Section \ref{sec:top-k}, \ref{sec:MPC-solve-rounding}, \ref{sec:MPC-solve-rounding} and \ref{sec:MPC-solve-LP} respectively.


 

\section{Preliminaries}
\label{sec:prelim}
The input to correlation clustering is a complete graph whose edges are partitioned into $+$edges and $-$edges. We shall use the graph $G = (V, E)$ of $+$edges to denote an instance. Let $n = |V|$ and $m = |E|$. For simplicity, we assume $E$ contains all the $n$ self-loops $uu, u \in V$.
So, $E$ is the set of $+$edges, and ${V \choose 2} \setminus E$ is the set of $-$edges.  The graph $G$ is fixed in most part of the paper. 


For any graph $H = (V_H, E_H)$, and any vertex $v \in V_H$, let $N_H(u) = \{v \in V_H \mid uv \in E_H\}$. For any vertex $u \in V_H$ and any subset $S \subseteq V_H$, we define $d_H(u, S) = \sum_{v \in S} \mathbb{1}(uv \in E_H)$ as the number of edges between $u$ and $S$. We simply use $d_H(u)$ for $d_H(u, V_H)$. When the graph $H$ is the input graph $G$, we omit the subscript. So we use $N(u)$ for $N_G(u)$ and $d(u)$ for $d_G(u)$. Notice that $u \in N(u)$ and $d(u) = |N(u)| \geq 1$ for every $u \in U$.  For the input graph $G = (V, E)$ and any two vertex $u, v \in V$, we define $M_{uv} = \max\{d(u), d(v)\}$ as the maximum degree of $u$ and $v$ for simplicity, as this notion will be frequently used. For any two sets $X$ and $Y$, we denote their symmetric difference by $X \Delta Y$. Algorithms are parameterized by constants $ \beta(0<\beta<1), \lambda(0<\lambda<1)$ that will be determined later. \medskip


A norm on $n$-dimensional non-negative vectors is a function $f:\R_{\geq 0}^n \to \R_{\geq 0}$ satisfying $f(\alpha x) = \alpha f(x)$ for every real $\alpha \geq 0$ and $x \in \R_{\geq 0}^n$, and $f(x + y) \leq f(x) + f(y)$ for every $x, y \in \R_{\geq 0}^n$. We say a norm $f:\R_{\geq 0}^n \to \R_{\geq 0}$ is monotone if for every $x, y \in \R_{\geq 0}^n$ with $x \leq y$, we have $f(x) \leq f(y)$. We say $f$ is symmetric if $f(x) = f(x')$ for every $x, x' \in \R_{\geq 0}^n$ such that $x'$ is a permutation of $x$.
We say $f$ is the top-$k$ norm for an integer $k \in [n]$ if $f(x)$ is equal to the sum of the $k$ largest coordinates of $x$.
 Chakrabarty and Swamy \cite{chakrabarty2019approximation} showed that any monotone and symmetric norm can be written as the maximum of many ordered norms.  This leads to the following lemma which reduces the monotone-symmetric norms to top-$k$ norms. For completeness, we defer its proof to Appendix~\ref{sec:all-norm}. 
\begin{restatable}{lemma}{lemmatopktolpnorm}
\label{lem:Topk2all}
    For any integer $k\in [n]$, if an algorithm returns a single clustering $\mathcal{C_{\text{ALG}}}$ that is simultaneously a $\rho$-approximation for all top-$k$ norm objectives, then $\mathcal{C_{\text{ALG}}}$ is a $\rho$-approximation for any monotone and symmetric norm $f:\mathbb{R}^n_{\geq 0} \rightarrow \mathbb{R}_+$.
\end{restatable}

For a fixed clustering $\mathcal{C}$, we already defined the disagreement vector of $\mathcal{C}$ as $\cost_\mathcal{C} \in \mathbb{Z}_{\ge 0}^n$, with $\cost_\mathcal{C}(u)$ for every $u \in V$ being the number of edges incident to $u$ that are in disagreement w.r.t $\mathcal{C}$.
Given an integer $k$, and a clustering $\mathcal{C}$, we denote the top-$k$ value by $\cost^k_\calC = \max_{T \subseteq V, |T| = k} \sum_{u \in T} \cost_\calC(u)$. Similarly, for any fractional vector $(x_{uv})_{u, v \in V}$,  we denote $\cost_{x}(u) = \sum_{uv \in E} x_{uv} + \sum_{uv \in {V \choose 2} \setminus E}(1 - x_{uv})$ as the disagreement for $u$ with respect to $x$. The top-$k$ value of $x$ is defined as $\cost^k_x  = \max_{T \subseteq V, |T| = k} \sum_{u \in T} \cost_x(u)$.
\medskip

We will use the following theorem from \cite{kalhan2019correlation}:
\begin{theorem}
    \label{thm:KMZ}
    Let $G = (V, E)$ be a correlation clustering instance, and $x \in [0, 1]^{{V \choose 2}}$ be a metric over $V$ with range $[0, 1]$.  There is a polynomial time algorithm that, given $G$ and $x$, outputs a clustering $\calC$ of $V$ such that $\cost_{\calC}(u) \leq 5\cdot \cost_x(u)$ for every $u \in V$. 
\end{theorem}



We will use the following well-known concentration inequalities.
\begin{theorem}[Chernoff Bound]
\label{thm:chernoff}
Let $X_1, X_2, ..., X_k$ be independent random variables taking values in $\{0, 1 \}$. Let $X = \sum_{i} X_i$ be the sum of these $k$ random variables. Then the following inequalities hold:
\begin{enumerate}[label=(\ref{thm:chernoff}\alph*)]
    \item For any $\epsilon \in (0, 1)$, if $E[X] \leq U$, then $\Pr[X \geq (1+ \epsilon) U] \leq \mathrm{exp}(-\epsilon^2U/3)$. \label{thm:chernoffgeq}
    \item For any $\epsilon \in (0, 1)$, if $E[X] \geq U$, then $\Pr[X \leq (1 - \epsilon) U] \leq  \mathrm{exp}(-\epsilon^2U/2)$. \label{thm:chernoffleq}
\end{enumerate}
\end{theorem}


 






\section{Simultaneous $O(1)$-Approximation for Top-$k$ Norms}
\label{sec:top-k}
In this section, we describe our simultaneous $63.3$-approximation for correlation clustering for all top-$k$ norms. The algorithm described in this section runs in polynomial time. It first constructs an LP solution to the top-$k$ linear program using a combinatorial procedure. Crucially, the construction does not depend on the value of $k$. We show that the solution has cost $12.66$ times the optimum cost under the top-$k$ norm, for any integer $k \geq 1$. Then we use the rounding algorithm of \cite{kalhan2019correlation} to round the LP solution to an integral one. As it gives a vertex-by-vertex 5-approximation guarantee, this leads to a $63.3$-approximation for the top-$k$ norm for any $k$. 



The LP for minimizing the top-$k$ norm of the clustering is given in LP~\eqref{lp:metric-lp}.
\begin{equation}\label{lp:metric-lp}
    \min \qquad \cost^k_x \qquad \text{s.t.}
\end{equation}\vspace*{-30pt}

\noindent
\begin{minipage}[t]{0.4\textwidth}
    \begin{align}
        x_{uv}+x_{uw} \ge x_{vw},\  \forall u,v,w\in V \label{LPC:triangle}\end{align}
\end{minipage}\hfill
\begin{minipage}[t]{0.3\textwidth}
    \begin{align}
        x_{uv} \in [0,1],\  \forall u,v \in V \label{LPC:non-negative}
    \end{align}
\end{minipage}\hfill
\begin{minipage}[t]{0.25\textwidth}
    \begin{align}
        x_{uu} = 0, \ \forall u \in V \label{LPC:self}
    \end{align}
\end{minipage} \bigskip


In the correspondent integer program, $x_{uv}$ for every $u, v \in V$ indicates if $uv$ is separated or not. We view $uv$ as an unordered pair and thus $x_{uv}$ and $x_{vu}$ are the same variable. So $(x_{uv})_{u, v \in V}$ is a metric with distances in $\{0, 1\}$, which is relaxed to $[0, 1]$ in the linear program. This is captured by constraints \eqref{LPC:triangle}, \eqref{LPC:non-negative} and \eqref{LPC:self}.
Notice that $\cost_x(u)$ for any $u \in V$ is a linear function of the $x$ variables.  The top-$k$ norm of the fractional clustering is defined by $\cost^k_x = \max_{S \subseteq V: |S| = k} \sum_{u \in S} \cost_x(u)$. This could be captured by introducing a variable $z$ and constraints $z \geq \sum_{u \in S}\cost_x(u)$ for any $S \subseteq V$ of size $k$, and setting $z$ to be the objective to minimize. For simplicity, we use the form as described. Despite having an exponential number of constraints, the LP can be solved efficiently as there is a simple separation oracle.  Moreover, we use a combinatorial algorithm to construct a solution $x$, and thus the algorithm does not solve the LP. 

\subsection{Algorithm}
The algorithm for constructing the LP solution $x$ is given in Algorithm~\ref{alg:pre-clusteringlp}. It depends on the parameter $\beta \in (0, 1)$, whose value will be specified later. During the process, we construct a subgraph $H$ by removing any edge $uv \in E$ where $u$ and $v$ have significantly different neighbors. We then set $x_{uv}$ as $1 - \frac{|N_{H}(u) \cap N_{H}(v)|}{M_{uv}}$ if $u\neq v$ and $x_{uv} = 0$ otherwise. Recall that $M_{uv} = \max\{d(u), d(v)\}$ is the maximum degree for any nodes $u$ and $v$ in graph $G$. Intuitively, we treat $+$edges as indicators of whether two nodes belong to the same cluster. The first step is to remove edges that should not be in the same cluster. The second step ensures that the more common neighbors two nodes have, the closer their distance should be. 

\begin{algorithm}[t]
    \caption{Construction of norm-oblivious solution $x$ to metric LP. \\
     \textbf{Input}: Graph $G = (V, E)$\\
\textbf{Output}: $(x_{uv})_{u, v \in V} $ }
\label{alg:pre-clusteringlp}
    \begin{algorithmic}[1]
    \Function {\textsc{AllNormCC}}{$G = (V, E)$}
    \State let $E_H = \{uv \in E: |N(u) \Delta N(v)| \leq \beta \cdot M_{uv}\}$ and $H = (V, E_H)$
 \label{alg:pre-clusteringlpfirst}
    \State let $x_{uv} \leftarrow 1 - \frac{|N_{H}(u) \cap N_{H}(v)|}{\mathrm{\max}(d(u), d(v))}$ for every $uv \in {V \choose 2}$ and $x_{uu} = 0$ for every $u \in V$
    \EndFunction
\end{algorithmic}    
\end{algorithm}

In the remaining part of this section, we will show 
\begin{lemma}
\label{lemma:boundratiomain}
Let $k$ be any integer in $[n]$. 
Algorithm \ref{alg:pre-clusteringlp} outputs a feasible solution $(x_{uv})_{u, v \in V}$ for ~\eqref{lp:metric-lp} such that for any $k$, we have
\begin{align*}
    \cost^k_x \leq 12.66 \cdot \opt^k, 
\end{align*}
where $\opt^k$ is the cost of the optimum solution under the top-$k$ norm. 
\end{lemma}

\begin{proof}\textcolor{red}{TOPROVE 1}\end{proof}

We will first show that our $x$ is feasible in Section \ref{sec:xismetric}, then we will bound the approximate ratio in Section \ref{sec:boundtopknormcost}.


\subsection{The validity of $x$ to LP~\eqref{lp:metric-lp}}
\label{sec:xismetric}
To show that $x$ is a valid solution to LP~\eqref{lp:metric-lp}, it suffices to prove that it is a metric over $V$ with range $[0, 1]$. Moreover, \eqref{LPC:non-negative} and \eqref{LPC:self} hold trivially. Therefore, it remains to show that the triangle inequality (i.e., constraint \eqref{LPC:triangle}) is satisfied: 
\begin{lemma}
\label{lemma:metric}
For any $u,v,w \in V$, we have 
    $x_{uv} + x_{uw} \geq x_{vw}$.
\end{lemma}
\begin{proof}\textcolor{red}{TOPROVE 2}\end{proof}



\subsection{Bounding the Top-$k$ Norm Cost of $x$}
\label{sec:boundtopknormcost}
In this section, we compare the top-$k$ norm cost of $x$ to $\opt^k$. 

\paragraph{Notations} 
We fix the integer $k \in [n]$. Let $\mathcal{C}$ be the clustering that minimizes the top-$k$ norm of disagreement vector, but our analysis works for any clustering.  For every $v \in V$, let $C(v)$ be the cluster in $\calC$ that contains $v$. 

For every $u \in V$, let $\cost^+_\calC(u), \cost^-_\calC(u)$ and $\cost_\calC(u)$ respectively be the number of $+$edges, $-$edges and edges incident to $u$ that are in disagreement in the clustering $\calC$. Recall $\cost^{k}_\calC = \max_{S \subseteq V:|S| = k} \sum_{u \in S}\cost_\calC(u)$ is the top-$k$ norm cost of the clustering $\calC$, thus we have $\opt^k = \cost^k_\calC$.

Let $U$ be the set of $k$ vertices $u$ with the largest $\cost_x(u)$ values. So, $\cost^k_x = \sum_{u \in U} \cost_x(u)$. In order to provide a clear demonstration, we divide all of the edges in $G$ into five parts. First, we separate out the parts that are easily constrained by $\cost^k_\calC$. Let $\varphi^+_1$ be the set of $+$edges that are cut in $\calC$, and $\varphi^-_1$ be the set of $-$edges that are not cut in $\calC$. For the remaining $+$edges in $E$ that are not cut in $\calC$, it is necessary to utilize the properties of $+$edges in $E_H$. To this end, let $\varphi^+_2$ be the set of $+$edges in $E\setminus E_H$ that are not cut in $\calC$, and $\varphi^+_3$ be the set of $+$edges in $E_H$ that are not cut in $\calC$. Finally, we define $\varphi^-_2$ as the set of $-$edges that are cut in $\calC$. Formally, we set 
\begin{align*}
    \varphi^+_1 &:= \{uv \mid uv \in E, C(u) \not= C(v) \},\\
    \varphi^+_2 &:= \{uv \mid uv \in E \setminus E_H, C(u) = C(v) \},   \\
    \varphi^+_3 &:= \{uv \mid uv \in E_H, C(u) = C(v) \}, \\
    \varphi^-_1 &:= \{uv \mid uv \in {V \choose 2} \setminus E, C(u) = C(v)\},\\
    \text{and} \quad \varphi^-_2 &:= \{uv \mid uv \in {V \choose 2} \setminus E, C(u) \not= C(v)\}.
\end{align*} 

 Notice that $\varphi^+_3$ contains all the self-loops. For every $(i,j) \in \{(1,+),(2,+),(3,+),(1,-),(2,-)\}$ and $u \in V$, we let $\varphi^j_i(u)$ be the set of pairs in $\varphi^j_i$ incident to $u$. We use $\phi^j_i(u) = \{v: uv \in \varphi^j_i(u)\}$ to denote the end-vertices of the edges in $\varphi^j_i(u)$ other than $u$; so $|\phi^j_i(u)| = |\varphi^j_i(u)|$.  We let $y^j_i(u)$ denote the cost of edges in $\varphi^j_i(u)$ in the solution $x$.  For every $(i,j) \in \{(1,+),(2,+),(3,+),(1,-),(2,-)\}$,  we define $f^j_i = \sum_{u \in U}y^j_i(u)$. Therefore, the top-$k$ norm cost of $x$ is $f^+_1 + f^+_2 + f^+_3 + f^-_1 + f^-_2$. 
\medskip

With the notations defined, we can proceed to the analysis. Prior to this, several propositions are presented, which will prove useful in the following analysis. We start with the property of edges in $E \setminus E_H$.

\begin{lemma}
    \label{lemma:varphi1}
    For every $uv \in \varphi^+_2$, we have $\cost_\calC(u) + \cost_\calC(v) \geq \beta \cdot M_{uv}$.
\end{lemma}
\begin{proof}\textcolor{red}{TOPROVE 3}\end{proof}

Then, we show that edges in $E_H$ have similar degrees.

\begin{lemma}
    \label{lemma:Hdegreebound}
    For every $uv \in E_H$, we have $(1-\beta)\cdot d(v) \le d(u) \le \frac{1}{1-\beta}\cdot d(v)$.
\end{lemma}
\begin{proof}\textcolor{red}{TOPROVE 4}\end{proof}

As we are about to analyze the top-$k$ norm objective, we can bound the cost in the solution $x$ using coefficients of $\cost_\calC$. This key observation can be formally demonstrated as follows:

\begin{claim}
    \label{claim:using-coefficients}
    Let $c \in \R_{\geq 0}^{n}$ be a vector and $\alpha > 0$ satisfying that $|c|_\infty \leq \alpha$ and $|c|_1 \leq \alpha k$.  Then we have 
    \begin{align*}
        \sum_{r \in V}c(r) \cdot \cost_\calC(r) \leq \alpha \cdot \cost^k_\calC.
    \end{align*}
\end{claim}
\begin{proof}\textcolor{red}{TOPROVE 5}\end{proof}

From the definitions of $f^+_1$ and $f^-_1$, we can see that it can be constrained by $\cost^k_\calC$.

 \begin{claim}
    $f^+_1 + f^-_1 \leq \cost^k_\calC$. 
 \end{claim}
 \begin{proof}\textcolor{red}{TOPROVE 6}\end{proof}

To bound the cost of the remaining $+$edges, we separately analyze the cost coefficients of vertices in $f^+_2$ and $f^+_3$. 

\begin{lemma}
    \label{lemma:f+2}
    There exists a vector $c^+_2 \in \R_{\geq 0}^{n}$ with the following properties:
    \begin{enumerate}[label=(\ref{lemma:f+2}\alph*)]
        \item \label{property:f+2-cost} $f^+_2 \leq \sum_{r \in V}c^+_2(r)\cdot \cost_\calC(r)$.
        \item \label{property:f+2-c+2-infty} $c^+_2(r) \leq \frac{2}{\beta} \cdot \frac{|\varphi^+_2(r)|}{d(r)}$, for every $r \in V$.
        \item \label{property:f+2-c+2-1} $|c^+_2|_1 \leq \frac2\beta\sum_{u \in U}\frac{|\varphi^+_2(u)|}{d(u)}$.
    \end{enumerate}
\end{lemma}
\begin{proof}\textcolor{red}{TOPROVE 7}\end{proof}

Following the analysis of the cost coefficients $c^+_2$ of $f^+_2$, we analyze the cost coefficients of $f^+_3$ in edge set $E_H$.

\begin{lemma}
    \label{lemma:f+3}
    There exists a vector $c^+_3 \in \R_{\geq 0}^{n}$ with the following properties:
    \begin{enumerate}[label=(\ref{lemma:f+3}\alph*)]
        \item \label{property:f+3-cost} $f^+_3 \leq \sum_{r \in V}c^+_3(r)\cdot \cost_\calC(r)$.
        \item \label{property:f+3-c+3-infty} $c^+_3(r) \leq 2\left(\frac{|\varphi^+_2(r)|\cdot |\varphi^+_3(r)|}{\beta\cdot d^2(r)}  + \frac{|\varphi^+_2(r)|}{\beta\cdot d(r)} + \frac{|\varphi^+_3(r)|}{d(r)}\right)$, for every $r \in V$.
        \item \label{property:f+3-c+3-1} $|c^+_3|_1 \leq 2 \sum_{u \in U} \left(\frac{|\varphi^+_2(u)|\cdot|\varphi^+_3(u)|}{\beta \cdot d^2(u)} + \frac{|\varphi^+_3(u)|}{\beta \cdot d(u)} + \frac{|\varphi^+_3(u)|}{d(u)}\right)$.
    \end{enumerate}
\end{lemma}
\begin{proof}\textcolor{red}{TOPROVE 8}\end{proof}

With Lemma~\ref{lemma:f+2} and Lemma~\ref{lemma:f+3}, we can then bound $f^+_2 + f^+_3$: 
\begin{lemma}
    $f^+_2 + f^+_3 \leq \frac4\beta \cdot \cost^k_\calC$. 
\end{lemma}
\begin{proof}\textcolor{red}{TOPROVE 9}\end{proof}

For the cost of the remaining $-$edges, i.e. $f^-_2$, we also analyze the cost coefficients of each vertex.

\begin{lemma}
    \label{lemma:f-2-properties}
    There exists a vector $c^-_2 \in \R_{\geq 0}^{n}$ with the following properties:
    \begin{enumerate}[label=(\ref{lemma:f-2-properties}\alph*)]
        \item \label{property:f-2-cost} $f^-_2 \leq \sum_{r \in V}c^-_2(r)\cdot \cost_\calC(r)$.
        \item \label{property:f-2-c-2-infty} $c^-_2(r) \leq \frac{2}{1-\beta}$, for every $r \in V$.
        \item \label{property:f-2-c-2-1} $|c^-_2|_1 \leq \frac{2k}{1-\beta}$.
    \end{enumerate}
\end{lemma}
\begin{proof}\textcolor{red}{TOPROVE 10}\end{proof}

With Lemma~\ref{lemma:f-2-properties}, we can then bound $f^-_2$.

\begin{lemma}
    $f^-_2 \leq \frac{2}{1-\beta} \cdot \cost^k_\calC$.
\end{lemma}
\begin{proof}\textcolor{red}{TOPROVE 11}\end{proof}

So the final ratio for Algorithm \ref{alg:pre-clusteringlp} is 
\begin{align*}
1 + \frac4\beta + \frac2{1-\beta}.
\end{align*} 
Let $\beta = 0.5858$, then the ratio is at most $12.66$.  This finishes the proof of Lemma~\ref{lemma:boundratiomain}.





 
\section{Implementing Algorithm \ref{alg:pre-clusteringlp} in Nearly Linear Time}
\label{sec:nearly-linear}
In this section, we show how to run Algorithm \ref{alg:pre-clusteringlp} approximately in nearly linear time. Indeed, the algorithm can be implemented in MPC model with $O(1)$ rounds. More precisely, we will show the following theorem:

\begin{restatable}{theorem}{thmefficientsettingLP} \label{thm:efficientpre-clusteringlp}
Let $\epsilon> 0$ and $\delta > 0$ be small enough constants. Given a graph  $G = (V, E)$, there exists an MPC algorithm that computes a solution $\{ \tilde{x}_{uv} \}_{u,v \in V}$ such that 
\begin{enumerate}
    \item For any integer $k \in [n]$, we have $\cost^k_{\tilde{x}} \leq (12.66+ \epsilon) \opt^k$.
    \item  For any $u,v,w \in V$, we have $\tilde{x}_{uv} + \tilde{x}_{uw} + \epsilon \geq \tilde{x}_{vw}$. 
\end{enumerate}
The algorithm succeeds with probability at least $1 - 1/n$. Moreover,  the algorithm runs in $O(1)$ rounds, has a total work of $\tilde{O}(m / \epsilon^6)$, requires $O(n^\delta)$ memory per machine and a $\tilde{O}(m / \epsilon^6)$ total memory. 
\end{restatable}






We give the nearly linear time implementation of Algorithm \ref{alg:pre-clusteringlp} in Algorithm~\ref{alg:pre-clusteringlpEfficient}. Line \ref{alg:AllNormCCBySamplingconstructH} constructs the graph $H$ efficiently. Line \ref{alg:AllNormCCBySamplingboundnegativeedgesstart}-\ref{alg:AllNormCCBySamplingboundnegativeedgesend} find the set $K$ of $-$edges we want to assign LP value. For any $-$edge $uv \not\in K$, we will simply set its LP value as $1$. Last, Line \ref{alg:AllNormCCBySamplingsetlpvaluestart} to Line~\ref{alg:AllNormCCBySamplingsetlpvalueend} is to set up $\tilde{x}_{uv}$ satisfying the conditions in~\ref{thm:efficientpre-clusteringlp}. In the remaining of this section, we will discuss these three parts in detail.

\begin{algorithm}[ht!]
\caption{Nearly Efficient Algorithm for Algorithm \ref{alg:pre-clusteringlp}.\\
\textbf{Input}: Graph $G = (V, E), \epsilon\in (0, 1)$\\
\textbf{Output}: $\{ \tilde{x}_{uv} \}_{u,v \in V}$ such that is $(12.66 + 26\epsilon)$-approximate and satisfy approximate triangle inequality. }
\label{alg:pre-clusteringlpEfficient}
\begin{algorithmic}[1]
\Function {\textsc{AllNormCCBySampling}}{$G = (V, E)$}
    \State Construct subgraph $H = (V, E_H \subseteq E)$ using Corollary \ref{coro:constructHbysampling} 
    \label{alg:AllNormCCBySamplingconstructH} \medskip


\For{every $u \in V$} \Comment{Compute set $K$}
    \label{alg:AllNormCCBySamplingboundnegativeedgesstart}
    \State $S_u \leftarrow \emptyset$
    \For{every $v \in d_H(u)$}: add $v$ to $S_u$ with probability $\min(\frac{8\log n}{\epsilon d_H(u)}, 1)$
        \EndFor
\EndFor
    \State $K \gets \{uv \notin E: \exists w \in S_u, vw \in E_H\text{ or } \exists w \in S_v, uw \in E_H\}$ \bigskip
    \label{alg:AllNormCCBySamplingboundnegativeedgesend}
    \State $\tau \leftarrow \frac{400\log n}{\epsilon^5}$      \label{alg:AllNormCCBySamplingsetlpvaluestart} \Comment{Compute the $\tilde x_{uv}$ values}
    \For{every $j \in [1, \log n]$}
    \State $S(j) \leftarrow \emptyset$
    \For{$v \in V$}: add $v$ to $S(j)$ with probability $\min(\tau / 2^j, 1)$
    \EndFor
    \For{every $uv \in E \cup K$ with $M_{uv} \in [2^{j-1}, 2^j)$}:
$\tilde{x}_{uv} \leftarrow 1 - \frac{2^j|N_H(u) \cap N_H(v) \cap S(j)|}{\tau \cdot M_{uv}}$
\EndFor
    \EndFor
    \For{every $uv \in E$}: \textbf{if} $\tilde{x}_{uv} \leq \epsilon$ \textbf{then} $\tilde{x}_{uv} \leftarrow 0$ \EndFor
    \label{alg:AllNormCCBySamplingRoundingstart}
    \For{every $uv \in K$}: \textbf{if} $\tilde{x}_{uv} \geq 1 - \epsilon$ \textbf{then} $\tilde{x}_{uv} \leftarrow 1$ \EndFor
    \label{alg:AllNormCCBySamplingRoundingmid}
    \For{every $uv \in {V \choose 2}\setminus (E \cup K)$}: 
        $\tilde{x}_{uv} \leftarrow 1$
    \EndFor
    \label{alg:AllNormCCBySamplingsetlpvalueend}
    \EndFunction
\end{algorithmic}
\end{algorithm}



\subsection{Line~\ref{alg:AllNormCCBySamplingconstructH}: Construction of Subgraph $H$}
To construct graph $H$ in Line~\ref{alg:AllNormCCBySamplingconstructH} of Algorithm \ref{alg:pre-clusteringlpEfficient}, we can use the sampling method in \cite{cohen2021correlation}:

\begin{restatable}{theorem}{thmcnostructH} [Lemma 3.11 of \cite{cohen2021correlation}]
\label{theorem:constructHbysampling}
Let $\delta, \epsilon, \beta \in (0, 1)$ be constants. There exists an MPC algorithm that, given a graph $G = (V, E)$, outputs a ``Yes/No'' answer for every $uv \in E$, such that the following happens with probability at least $1 - \frac{1}{n^6}$.
\begin{itemize}
    \item For every $uv \in E$ with $|N(u) \Delta N(v)| \leq \beta M_{uv}$, algorithm outputs ``Yes'' for $uv$.
    \item For every $uv \in E$ with $|N(u) \Delta N(v)| \geq (1+\epsilon) \beta M_{uv}$, algorithm outputs ``No'' for $uv$. 
\end{itemize}
The algorithm runs in $O(1)$ rounds, with a total work of $\tilde{O}(m/ \epsilon^2)$, $O(n^\delta)$ memory per machine, and $\tilde{O}(m/\epsilon^2)$ total memory.
\end{restatable}

In \cite{cohen2021correlation}, it is required that the algorithm outputs ``Yes'' for $uv \in E$ if $|N(u) \Delta N(v)| \leq 0.8 \beta M_{uv}$ and outputs ``No'' for $uv \in E$ if $|N(u) \Delta N(v)| \geq \beta M_{uv}$. However, we can replace 0.8 with any constant less than 1. Additionally, the algorithm succeeds with probability $1 - \frac{1}{n}$, but we can increase this to $1 - \frac{1}{n^c}$ for any constant $c$. For completeness, we prove Theorem \ref{theorem:constructHbysampling} in Appendix~\ref{sec:constructH}. The theorem leads to the following corollary:
\begin{coro}
    \label{coro:constructHbysampling}
    Consider the setting in Theorem~\ref{theorem:constructHbysampling}, and let $H = (V, E_H)$ with $E_H$ being the set of edges $uv \in E$ for which the algorithm outputs ``Yes''. Let $x_{uv} = 1 - \frac{|N_H(u) \cap N_H(v)|}{M_{uv}}$ for every $uv \in {V \choose 2}$ and $x_{uu} = 0$ for every $u \in V$. Then for any $k \in [1, n]$, we have $\cost^k_x \leq 12.66(1 + \epsilon) \opt^k$.
\end{coro}



From now on, we define $x_{uv}$ as in Corollary~\ref{coro:constructHbysampling}: $x_{uv} = 1 - \frac{|N_H(u) \cap N_H(v)|}{M_{uv}}$ for every $uv \in {V \choose 2}$ and $x_{uu} = 0$ for every $u \in V$. Notice that Algorithm~\ref{alg:pre-clusteringlpEfficient} does not compute or main $x$ explicitly; it is introduced only for the sake of analysis. 

\subsection{Line~\ref{alg:AllNormCCBySamplingboundnegativeedgesstart}-\ref{alg:AllNormCCBySamplingboundnegativeedgesend}: Construction of $K$} 
After obtaining $H$, there might be $O(n^2)$ $-$edges for which we need to assign $x$ values in Algorithm \ref{alg:pre-clusteringlp}. Fortunately, most of these $-$edges will have $x$ values greater than $1 - \epsilon$, allowing us to simply disregard them. By doing this, we achieve an approximate triangle inequality rather than a strict triangle inequality.  

The key observation is as follows: for any $-$edge $uv \in {V \choose 2} \setminus E$ with $x_{uv} \leq 1 - \epsilon$, then $\frac{|N_H(u) \cap N_H(v)|}{M_{uv}} = 1 - x_{uv} \geq \epsilon$, which is equivalent to $|N_H(u) \cap N_H(v)| \geq \epsilon M_{uv}$. If we sample $O\left(\frac{\log n}{\epsilon}\right)$ nodes from $N_{H}(u)$,  at least one node is in $N_H(u) \cap N_H(v)$ with high probability. By considering all neighbors of the sampled nodes for $u$, we will cover all $-$edges $uv$ where $x_{uv} \leq 1 - \epsilon$. Additionally, note that for any neighbor $w \in N_H(u)$, their degrees will not differ significantly. Since we only have $O\left(\frac{\log n}{\epsilon}\right)$ nodes to consider, we will only need to consider $O\left(\frac{d(u) \log n}{\epsilon}\right)$ nodes for each node $u$, which is still a nearly linear number of nodes in total. 



We let $K$ be the set constructed in the algorithm. 
\begin{lemma}
    \label{lemma:samplingnegativesizebound}
$\forall uv \in {V \choose 2}\setminus E \text{ with } x_{uv} \leq 1 - \epsilon, \Pr[uv \in K] \geq 1 - n^{-6}$. 
Moreover \( |K| = O\left(\frac{m \log n}{\epsilon}\right) \) with probability at least \( 1 - \frac{1}{n^7} \).
\end{lemma}

\begin{proof}\textcolor{red}{TOPROVE 12}\end{proof}



\subsection{Line~\ref{alg:AllNormCCBySamplingsetlpvaluestart}-\ref{alg:AllNormCCBySamplingsetlpvalueend}: Computing $\tilde{x}_{uv}$ for $E \cup K$}

Once we identify all edges to assign LP values to, we use the sampling method to compute $x_{uv}$, as described in~\cite{davies2023fast}. The key observation is that the size of a given set can be evaluated by sampling $O(\log n)$ elements. Special care is needed when the set is too small; hence, we set $\tilde{x}_{uv}$ to $0$ or $1$ if the size of the set is either too small or too large.

\paragraph{Algorithm Description}
Lines \ref{alg:AllNormCCBySamplingsetlpvaluestart} to \ref{alg:AllNormCCBySamplingsetlpvalueend} describe the setup of $\tilde{x}_{uv}$. Let $\tau = \frac{\log n}{\epsilon^2}$. To evaluate $|N(u) \cap N(v)|$, we define the set $S(j)$ as a subset of nodes obtained by sampling every node in the graph independently with probability $\min(\tau / 2^j, 1)$ (Line 11). We will have $O(\log n)$ different values for $j \in [1, \log n]$. Define $j_u = \max\{j \mid 2^j \leq d(u)\}$ as the maximum integer that is a power of 2 and at most $d(u)$. For any $uv \in E \cup K$, we then set 
\begin{align*}
    \tilde{x}_{uv} = 1 - \frac{ 2^j |N_H(u) \cap N_H(v) \cap S(j_u)|}{ \tau M_{uv}}.
\end{align*}
Additionally, for each $uv \in E$, if $\tilde{x}_{uv} \leq \epsilon$, we set $\tilde{x}_{uv} = 0$. For any $uv \in K$, if $\tilde{x}_{uv} \geq 1 - \epsilon$, we set $\tilde{x}_{uv} = 1$ (Lines \ref{alg:AllNormCCBySamplingRoundingstart}-\ref{alg:AllNormCCBySamplingsetlpvalueend}). Our main lemma regarding the approximate LP value $\tilde{x}_{uv}$ is given below:

\begin{restatable}{lemma}{lemmasmpaling}
\label{lemma:samplingfinalvalue}
With probability at least $1 - 1 / n^6$, Algorithm \ref{alg:pre-clusteringlpEfficient} outputs $\tilde{x}_{uv}$ such that
\begin{enumerate}[label=(\ref{lemma:samplingfinalvalue}\alph*)]\item For any $uv \in E$, we have $\tilde{x}_{uv} \leq (1 + \epsilon) x_{uv}$. For any $uv \in {V \choose 2} \setminus E$, we have $1 - \tilde{x}_{uv} \leq (1 + \epsilon)(1 - x_{uv})$.
    \label{lemma:samplingfinalvaluelpvalue}
    \item For any $u, v, w \in V$, we have $\tilde{x}_{uv} + \tilde{x}_{uw} + 3\epsilon \geq \tilde{x}_{vw}$.
    \label{lemma:samplingfinalvaluetriangle}
\end{enumerate}
\end{restatable}

The proof is similar to the sampling method proof presented in \cite{davies2023fast}. The full proof of Lemma \ref{lemma:samplingfinalvalue} is provided in Appendix \ref{sec:proofofsamplingfinalvalue}.

\subsection{Wrapping up: Proof of Theorem \ref{thm:efficientpre-clusteringlp}}

We can now prove Theorem \ref{thm:efficientpre-clusteringlp}. Once we construct the graph $H$ in Algorithm \ref{alg:pre-clusteringlpEfficient}, by setting $x_{uv} = 1 - \frac{|N_H(u) \cap N_H(v)|}{\max(d(u), d(v))}$, we achieve $\cost^k_x \leq (12.66 + 26\epsilon) \opt^k$. By Lemma \ref{lemma:samplingfinalvalue}, we know that at the end, the algorithm outputs the approximate LP value $\tilde{x}_{uv}$. Therefore, we can bound the cost as $\cost^k_{\tilde{x}} \leq (1 + \epsilon) \cost^k_x \leq (12.66 + 50\epsilon) \opt^k$, where $\opt^k$ is the cost of the optimal correlation clustering solution using the top-$k$ norm objective. The approximate triangle inequality is implied by Lemma \ref{lemma:samplingfinalvaluetriangle}. The final ratio is obtained by scaling $\epsilon$ to $\epsilon / 50$.

The running time of Algorithm \ref{alg:pre-clusteringlpEfficient} is derived from the discussion in this section. Constructing $H$ takes $O(1)$ rounds in the MPC model and $\tilde{O}(m /\epsilon^2)$ total work. By Lemma \ref{lemma:samplingnegativesizebound}, we know that we will only assign at most $\tilde{O}(\frac{m}{\epsilon})$ edges. Since the size of $|N_H(u) \cap N_H(v) \cap S(j_u)|$ is bounded by $\tau = O(\frac{\log n}{\epsilon^5})$, each edge takes $\tilde{O}(1/\epsilon^5)$ time to compute $\tilde{x}_{uv}$. In total, Algorithm \ref{alg:pre-clusteringlpEfficient} takes $\tilde{O}(\frac{m}{\epsilon^6})$ work and total memory. Note that all sampling and for-loops are naturally parallelized, so it only takes $O(1)$ rounds to output $\tilde{x}_{uv}$.



 
\section{Rounding}
\label{sec:MPC-solve-rounding}
We will present a nearly linear time rounding algorithm. Furthermore, our algorithm only takes $\Tilde{O}(1)$ rounds in the MPC model. The purpose of this section is to show 

\thmroundingmain*

We emphasize that even if the LP values satisfy the exact triangle inequality, rather than an approximate triangle inequality, the $\epsilon$ terms in the approximate ratio will still be present. These $\epsilon$ terms arise from two sources: the approximate inequality itself and the inherent characteristics of our MPC algorithm.

Given Theorem \ref{thm:efficientpre-clusteringlp} and Theorem \ref{thm:roundingmaintheorem}, we are now able to show the main result of this paper.

\begin{proof}\textcolor{red}{TOPROVE 13}\end{proof}



\subsection{Rounding Algorithm}
Assume that we are given an instance graph $G = (V, E)$ and a LP solution $( x_{uv} )_{u,v \in V}$ such that $x_{uv} + x_{uw} + \epsilon \geq x_{vw}$ for any $u, v, w \in V^3$.  Given a subgraph $V_t$ and node $w$, radius $r$, define the ball centering at $w$ with radius $r$ as $\ball_{V_t}(w, r) = \{ u \mid u \in V_t, x_{wu} \leq r \} $. Define  
\begin{align*}
    L_t(w) = \sum_{u \in \ball_{V_t}(w, r) }( r - x_{uw} )
\end{align*}

Note that $L_t(w) \geq r$ since $w$ itself is in $ \ball_{V_t}(w, r)$.

\paragraph{Algorithm Description}Our algorithm works as follows: in each round, we choose a set of cluster centers and choose the ball with radius $\twofive$ as a cluster. More precisely. At step $t$, $V_t$ is the set of unclustered vertices. We first compute $L_t^{\textmd{max}}$ to ensure that for each $u\in V_t$, we have $L_t(u) < (1+\epsilon) L_t^{\textmd{max}}$. For any node $u$, if $L_t(u) \geq L_t^{\textmd{max}}$, we will add $u$ to the set of candidate cluster centers $M_t$~(Line \ref{alg:mpcroundingfindcandidatestart}-\ref{alg:mpcroundingfindcandidateend}). Then, we compute cluster centers by adding vertices in the $M_t$ set to the $S_t$ set with probability $p_t$, where the more vertices in $M_t$ are in $\ball(radius=\twofive)$ with each other, the smaller the probability~(Line \ref{alg:mpcroundingfindclustercenterstart}-\ref{alg:mpcroundingfindclustercenterend}). After that, to avoid conflicts, we remove some cluster centers in $S_t$ if they are too close to each other, and derive the final cluster center set $H_t$~(Line \ref{alg:mpcroundingaviodconflicts}). Let $F_t = \ball_{V_t}(H_t,\twofive)$ be the nodes clustered at step $t$. Then we add each $u \in F_t$ to the cluster from $H_t$ with minimum ID. We will remove the clustered nodes and repeat the above process until all vertices are clustered.


\begin{algorithm}[ht!]
\caption{The Rounding Algorithm.\\
\textbf{Input}: Graph $G = (V, E)$, LP solution $( x_{uv} )_{u, v\in V}$ satisfying $x_{uv} + x_{uw} + \epsilon \geq x_{vw}$ for any $u, v, w \in V^3$.\\
\textbf{Output}: A function of clustering $\calC$; i.e., $C(u)=C(v)$ iff $u$ and $v$ belongs to the same cluster. }
\label{alg:mpcrounding}
\begin{algorithmic}[1]
\Function {\textsc{ParallelRounding}}{$G = (V, E),  (x_{uv}) $}
    \State $V_1 \gets V$.
    \State $C(v)\gets \emptyset$ for all $v\in V$.
    \State $t\gets 1$.
    \While{$V_{t}\neq\emptyset$}
        \State $M_t \gets \emptyset, S_t\gets \emptyset$
        \label{alg:mpcroundingfindcandidatestart}
        \State $L_t^{\textmd{max}} = \textmd{max}\{ r (1+\epsilon)^j | r (1+\epsilon)^j \leq \textmd{max}_{w \in V_t}{L_t(w)}, \text{where $j$ is an integer} \}$ \label{alg:randomizedPIVOTLvalue}
        \For{$u \in V_{t}$ } \Comment{Find cluster center candidate}
            \If{$L_t(u) \ge L_t^{\textmd{max}}$ }.
            \State $M_{t} \gets M_{t} \cup \{ u \}$ 
            \EndIf
        \EndFor
        \label{alg:mpcroundingfindcandidateend}
        \State $\Delta_t \gets \textmd{max}_{u \in M_t}{|\ball_{V_t}(u, \twofive) \cap M_t|}$.\label{alg:mpcroundingfindclustercenterstart}
        \State $p_t \gets 1 / (2\Delta_t)$.
\For{$u \in M_{t}$ }
        \State Add $u$ to $S_t$ with probability $p_t$ \Comment{Choose cluster centers in parallel.}
        \EndFor
        \label{alg:mpcroundingfindclustercenterend}
        \State $H_t = \{u \in S_t \mid \nexists\,v \in S_t \cap V_t \textmd{ such that } x_{uv} \leq \twofive \}$ \Comment{remove cluster centers that conflict.}
        \label{alg:mpcroundingaviodconflicts}
        \State $F_t \gets H_t \cup \ball_{V_t}(H_t, \twofive)$ \Comment{the set of settled vertices in round $t$.}
        \For{$u \in F_t$ }
            \State Let $v$ be the vertex with minimum ID among $\ball_{V_t}(u, \twofive) \cap H_t$.
            \State $C(u) \gets v$ \Comment{adding $u$ to the cluster of $v$.}
        \EndFor
        \State $V_{t + 1} \gets V_t \setminus F_t$, $t\gets t+1$
    \EndWhile
    \State \Return $\calC$
    \EndFunction
\end{algorithmic}
\end{algorithm}

\subsection{Approximate Ratio}
\paragraph{Analysis Framework} We will follow the proof of the sequential rounding algorithm~\cite{kalhan2019correlation}, which is also used in \cite{davies2023fast} for approximate triangle inequality. However, we suffer different difficulties that each round we choose multiple nodes whose $L_t$ value is within $\frac{1}{1 + \epsilon}$ times the maximum $L_t$ value. Let $LP(uv)$ be the LP cost of the edge $uv: LP(uv) = x_{uv}$ if $uv \in E$ and
$uv: LP(uv) = 1 - x_{uv}$ if $uv \in {V\choose2} \setminus E$. Let $ALG(uv) = \mathbb{1}(uv \textmd{ is in disagreement})$. We can define 
\begin{align*}
    \profit(u) = \sum_{uv \in E} LP(uv) - r \sum_{uv \in E} ALG(uv)
\end{align*}
where $r = \onefive - 2\epsilon$. We want to show that $\profit(u) \geq 0$ for all nodes. Let 
\begin{align*}
    \Delta E_t = \{ uv: u \in F_t \textmd{ or } v \in F_t\}
\end{align*}
be the set of edges removed at step $t$. Next, let 
\begin{align*}
    \profit_t(uv) =  \begin{cases}
            LP(uv) - r \cdot ALG(uv), &uv\in \Delta E_t\\
            0, &otherwise
        \end{cases}
\end{align*}
and the profit at step $t$ is defined as
\begin{align*}
    \profit_t(u) = \sum_{v \in V_t} \profit_t(uv) = \sum_{uv \in \Delta E_t} LP(uv) - r \cdot \sum_{uv \in \Delta E_t} ALG(uv)
\end{align*}
We will later show that $\profit_t(u) \geq 0$ for each $t$. Thus, we have 
\begin{align*}
    \profit(u) = \sum_{t} \profit_t(u) \geq 0
\end{align*}
and we can bound the cost of the algorithm by 
\begin{align*}
    \sum_{uv \in E} ALG(uv) \leq \frac{1}{r} \sum_{uv \in E} LP(uv) \leq (5 + 55 \epsilon) \sum_{uv \in E} LP(uv)
\end{align*}
which gives us the following lemma,
\begin{lemma}
\label{lem:correctnessofRoundnig}
Assume that $\epsilon \leq 1/ 20$. Given graph $G = (V, E)$ and a set of LP value $ (x_{uv})_{u,v \in V} $ such that $x_{uv} + x_{uw} + \epsilon \geq x_{vw}$ for any $u,v,w \in V^3$. Let $y_u = \sum_{uv \in E} x_{uv} + \sum_{uv \in {V \choose 2} \setminus E}(1 - x_{uv})$ be the LP disagreement for node $u$. Algorithm \ref{alg:mpcrounding} outputs a clustering $\calC$ such that for any node $u$,
\begin{align*}
    \cost_\calC(u) \leq (5 + 55\epsilon) y_u
\end{align*}
\end{lemma}

The remaining part of this section is to show $\profit_t(u) \geq 0$. We first show that we can bound the profit of any $-$edge $uv$. Recall that $r = \onefive - 2\epsilon$.

\begin{lemma}[Analogue of 5.3 of \cite{kalhan2019correlation}] 
\label{lem:roundingcostfornegativeedge}
 Let $u, v \in V_t$ and $uv \in {V \choose 2} \setminus E$, then $\profit_t(uv) \geq 0$.
\end{lemma}
\begin{proof}\textcolor{red}{TOPROVE 14}\end{proof}


To show that $\profit_t(u) \geq 0$, we take a different view on adding nodes to the cluster center: We sort cluster centers $H_t$ by the ID in increasing order. Let $w_1, w_2, ...,$ be the cluster centers from $H_t$ after sorting, then we will add nodes $\ball_{V_t}(w_1, \twofive)$ to $w_1$'s cluster, remove the clustered nodes, and repeat with the next center on the unclustered nodes. The whole process terminates once we process all cluster centers. 

Let $w$ be the first cluster center such that $x_{uw} \leq \fourfive$ when we do the above process. If all nodes in $\ball_{V_t}(w, r)$ are still unclustered, \cite{kalhan2019correlation} already shows that the cost for short $+$edges of $u$ can be covered by the profit of $\ball_{V_t}(w, r)$. The difficulty of the proof mainly comes from the fact that some nodes from  $\ball_{V_t}(w, r)$ may have been clustered. We will divide $x_{uw}$ into several cases and show that in all cases, we have $\profit_t(u) \geq 0$.

The first two cases are when $w$ does not exist or $x_{uw} \leq \onefive$, we will show that under these two cases, $\profit_t(uv) \geq 0$ for any edge $uv \in \Delta E_t$. Note that by Lemma \ref{lem:roundingcostfornegativeedge}, we have $\profit(uv) \geq 0$ for any $uv \in {V \choose 2} \setminus E$. Therefore, we only need to consider $+$edges.

\begin{lemma}
\label{lem:firstcaseprofit}
If the above $w$ does not exist, then $\profit_t(uv) \geq 0$.
\end{lemma}
\begin{proof}\textcolor{red}{TOPROVE 15}\end{proof}

\begin{lemma}
\label{lem:secondcaseprofit}
If $x_{uw} \leq \onefive$, then $\profit_t(uv) \geq 0$.
\end{lemma}
\begin{proof}\textcolor{red}{TOPROVE 16}\end{proof}

By Lemma \ref{lem:firstcaseprofit} and \ref{lem:secondcaseprofit}, we know that when $w$ does not exist or $x_{uw} \leq \onefive$, we have $\profit_t(u) = \sum_{uv \in \Delta E_t} \profit_t(uv) \geq 0$.  For the remaining cases, we will follow the blue-print of the proof in~\cite{kalhan2019correlation}. Note that $\profit_{t}(uv) < 0$ only if $uv \in E$ and $x_{uv} \leq r$. We will charge the negative profit to the profit of $\ball_{V_t}(w,r)$. More precisely, let 

\begin{align*}
    \profit_t(u) = \underbrace{\sum_{v \in \ball_{V_t}(w, r)}\profit_t(uv)}_{P_{\textrm{high}}(u)} + \underbrace{\sum_{v \in V_t \setminus \ball_{V_t}(w, r)}\profit_t(uv)}_{P_{\textrm{low}}(u)}
\end{align*}
We will show that $P_{\textrm{high}}(u) \geq (1+\epsilon) L_t(w)$ and $P_{\textrm{low}}(u) \geq -L_t(u)$. Note that Algorithm \ref{alg:mpcrounding} always chooses nodes whose $L_t$ value is within $\frac{1}{1 + \epsilon}$ times of the maximum $L_t$ value as cluster center, so the profit $P_{\textrm{low}}(u)$ can be covered by the profit of $\ball_{V_t}(w, r)$, which is at least $P_{\textrm{high}}(u)$. The following two lemmas connects $P_{\textrm{high}}(u)$ with $(1+\epsilon) L_t(w)$. We again have two different cases for $x_{uw}$ and show in each case, we have $\profit_t(uv) \geq (1+\epsilon)(r - x_{vw})$.
\begin{lemma}
\label{lem:thirdcaseprofit}
If $x_{uw} \in (\twofive, \fourfive]$, for any $v \in \ball_{V_t}(w, r)$, we have $\profit_{t}(uv) \geq (1 + \epsilon)(r - x_{vw})$.
\end{lemma}
\begin{proof}\textcolor{red}{TOPROVE 17}\end{proof}

The last case is $x_{uw} \in (\onefive, \twofive]$.
\begin{lemma}
\label{lem:fourthcaseprofit}



If $x_{uw} \in (\onefive, \twofive]$, for any $v \in \ball_{V_t}(w, r)$, we have $\profit_{t}(uv) \geq (1 + \epsilon)(r - x_{vw})$.
\end{lemma}
\begin{proof}\textcolor{red}{TOPROVE 18}\end{proof}

Once we bound the profit for each edge $uv$ such that $v \in \ball_{V_t}(w, r)$, we now are able to lower bound the $P_{\textrm{high}}(u)$.

\begin{lemma}
\label{lem:highprofit}
If $x_{uw} \in (\onefive, \fourfive]$, then $P_{\textrm{high}}(u) \geq (1 + \epsilon) L_t(w)$.
\end{lemma}
\begin{proof}\textcolor{red}{TOPROVE 19}\end{proof}
We still have to bound the profit of $P_{\textrm{low}}(u)$, this has been shown in \cite{kalhan2019correlation} and \cite{davies2023fast}. We give the corresponding lemma for completeness. 
\begin{lemma}
\label{lem:lowprofit}
If $x_{uw} \in (\onefive, \fourfive]$, then $P_{\textrm{low}}(u) \geq - L_t(u)$.
\end{lemma}
\begin{proof}\textcolor{red}{TOPROVE 20}\end{proof}

Combining the profit for $P_{\textrm{high}}(u)$ and $P_{\textrm{low}}(u)$, we can deduce that the profit for any node $u$ is non-negative when $ x_{uw} \in (\onefive, \fourfive]$
\begin{lemma}
If $x_{uw} \in (\onefive, \fourfive]$, then $\profit_t(u) \geq 0$.
\end{lemma}
\begin{proof}\textcolor{red}{TOPROVE 21}\end{proof}

\subsection{Running time}

\begin{lemma}
\label{lem:mpcroundingrunningtimebase}
At some round $s$, let $L^{\max}_s$ and $\Delta_s$ be the values we set up at lines \ref{alg:randomizedPIVOTLvalue} and \ref{alg:mpcroundingfindclustercenterstart}. Then, after $O(\log n)$ rounds, at round $e = s + O(\log n)$, with high probability, either $L^{\max}_e < L^{\max}_s$ or $\Delta_e \leq \frac{\Delta_s}{2}$.
\end{lemma}

\begin{proof}\textcolor{red}{TOPROVE 22}\end{proof}
The next lemma gives us an upper bound of the number of rounds,

\begin{lemma}
\label{lem:mpcroundingtimelogn}
Algorithm \ref{alg:mpcrounding} terminates after at most $O(\log^3 n / \epsilon)$ rounds, w.h.p.
\end{lemma}
\begin{proof}\textcolor{red}{TOPROVE 23}\end{proof}

Now we can prove the main theorem in this section.

\begin{proof}\textcolor{red}{TOPROVE 24}\end{proof}






%
 
\section{A Constant Round MPC Algorithm}
\label{sec:MPC-solve-LP}
We will show Theorem \ref{thm:constantMPCAlgorithmForCC} in this section. We repeat for convenience, 
\thmconstantMPCAlgorithmForCC*



\subsection{Algorithm}
Theorem \ref{thm:mainthmlp} gives us an $O(\log^3 n)$ rounds MPC algorithm. The bottleneck is the rounding procedure. To achieve a constant rounds MPC algorithm, instead of setting up the LP and rounding, we use the pre-clustering algorithm from \cite{cohen2021correlation}, which is very useful for $\ell_1$-norm correlation clustering. We show that the pre-clustering algorithm can also provide an $O(1)$-approximate ratio for all monotone symmetric
norms simultaneously.

\paragraph{Algorithm Description} The algorithm from \cite{cohen2021correlation} is parameterized by $\beta$ and $\lambda$. It has three steps: 
\begin{enumerate}
    \item The first step is the same as the first step of Algorithm \ref{alg:pre-clusteringlp}, where we compute the graph $H$ (Line \ref{alg:pre-clusteringfirst}).
    \item The algorithm marks a node as light if it loses more than a $\lambda$ fraction of its neighbors in the first step. Otherwise, it marks the node as heavy. The algorithm removes all edges between two light nodes in $H$ (Line \ref{alg:pre-clusteringsecondstart} - Line \ref{alg:pre-clusteringsecondend}).
    \item The last step is to output the connected components $F$ of the final graph.
\end{enumerate}

\begin{algorithm}[ht!]
\caption{Pre-clustering – Algorithm 1 in \cite{cohen2021correlation}. \\
\textbf{Input}: Graph $G = (V, E)$, \\
\textbf{Output}: Clustering $F$.}
\label{alg:pre-clustering}
\begin{algorithmic}[1]
\Function {\textsc{PreClustering}}{$G = (V, E)$}
 \State Let $E_H = \{uv \in E: |N(u) \Delta N(v)| \leq \beta \cdot \max(d(u), d(v))\}$ and $H = (V, E_H)$
 \label{alg:pre-clusteringfirst}
\For{$v \in V$} \label{alg:pre-clusteringsecondstart}
\If{$d_H(v) < (1 - \lambda) d(v)$} Mark it as light
\Else \quad Mark it as Heavy
\EndIf
\EndFor  
\State Let $E_{\tilde{G}} = \{ uv \in E_H: u \mathrm{\ or\ } v \mathrm{\ is\ heavy} \}$ and $\tilde{G} = (V, E_{\tilde{G}})$ \label{alg:pre-clusteringsecondend}
\State Compute its connected components on $\tilde{G}$, denoted as $F$,
\Return $F$.
\EndFunction
    \end{algorithmic}    
\end{algorithm}

The main reason we can achieve a constant rounds MPC algorithm is the simplicity of steps 2 and 3. \cite{cohen2021correlation} already showed that Algorithm \ref{alg:pre-clustering} can be implemented within $O(1)$ rounds and is $O(1)$-approximate for correlation clustering under $\ell_1$-norm objective. We extend their proof for approximate ratio and show that Algorithm \ref{alg:pre-clustering} outputs an $O(1)$-approximate clustering $F$ for any top-$k$ norm objective. More concretely, we have the following lemma:

\begin{lemma}
\label{lemma:preclusteringratio}
Assume that $8\beta + \lambda \leq 3/8$. Given any graph $G$, Algorithm \ref{alg:pre-clustering} outputs a clustering $F$ such that for any integer $k \in [n]$, we have 
\begin{align*}
    \cost^k_{F} \leq \left(\frac{3}{\beta} + \frac{1}{\lambda} + \frac{1}{\beta\lambda} + 8\right) \cdot \opt^k
\end{align*}
where $\opt^k$ is the cost of the optimum solution under the top-$k$ norm.
\end{lemma}

\begin{proof}\textcolor{red}{TOPROVE 25}\end{proof}


\subsection{Approximate Ratio}
The proof follows the blueprint of the proof in \cite{cohen2021correlation} and resembles the proof used to bound the approximate ratio for Algorithm \ref{alg:pre-clusteringlp} in Section \ref{sec:boundtopknormcost}. We will first bound the approximate ratio loss for steps 1 and 2, then we compare the final clustering $F$ with the optimal top-$k$ clustering in the graph where we remove all edges deleted in steps 1 and 2.



\paragraph{Notations} 
We fix the integer $k \in [n]$. $\mathcal{C}$ is the clustering that minimizes the top-$k$ norm of the disagreement vector.  For every $v \in V$, $C(v)$ is the cluster in $\calC$ that contains $v$. Let $U$ be the set of $k$ vertices $u$ with the largest $\cost_F(u)$ values. So, the top-$k$ norm of $F$ is $\sum_{u \in U}\cost_F(u)$.




 Similarly, we divide all $+$edges in $G$ into three parts. First, we separate out the parts easily constrained by $\cost^k_\calC$. Let $\varphi^+_1$ be the set of $+$edges in $E_G$ that are cut in $\calC$. For the remaining $+$edges in $E_G$ that are not cut in $\calC$, we divide them into two categories depending on when we remove the edge. Let $\varphi^+_2$ be the set of $+$edges in $E_G\setminus E_H$ that are not cut in $\calC$, and $\varphi^+_4$ be the set of $+$edges in $E_H\setminus E_{\tilde{G}}$ that are not cut in $\calC$. In other words,  $\varphi^+_2$ and $\varphi^+_4$ are edges that are not cut in $\calC$ and removed at steps 1 and 2. respectively. Formally, we set 
\begin{align*}
    \varphi^+_1 &:= \{uv \mid uv \in E_G, C(u) \not= C(v) \},\\
    \varphi^+_2 &:= \{uv \mid uv \in E_G \setminus E_H, C(u) = C(v) \},   \\
    \varphi^+_4 &:= \{uv \mid uv \in E_H \setminus E_{\tilde{G}}, C(u) = C(v) \}, \\
\end{align*} 

For every $i \in \{1, 2, 4\}$ and $u \in V$, we let $\varphi^+_i(u)$ be the set of pairs in $\varphi^+_i$ incident to $u$. 
For every $i \in \{1, 2, 4\}$,  we define $g^+_i = \sum_{u \in U}|\varphi^+_i(u)|$. Therefore, when we remove edges from the graph $G$, we will lose at most $ g^+_1 + g^+_2 + g^+_4$ number of edges for nodes from $U$. 

We have $g^+_1 \leq \cost^k_{\mathcal{C}}$. Note that in Lemma \ref{lemma:f+2}, we actually show a stronger argument, $\sum_{u \in U}|\varphi^+_2(u)|$ is bounded by $\frac{2}{\beta}\cost^k_{\mathcal{C}}$. This gave us the following Corollary for $g^+_2$. 

\begin{coro}
    \label{coro:g+2}
    There exists a vector $c^+_2 \in \R_{\geq 0}^{n}$ with the following properties:
    \begin{enumerate}[label=(\ref{coro:g+2}\alph*)]
        \item \label{property:g+2-cost} $g^+_2 \leq \sum_{r \in V}c^+_2(r)\cdot \cost_\calC(r)$.
        \item \label{property:g+2-c+2-infty} $c^+_2(r) \leq \frac{2}{\beta} \cdot \frac{|\varphi^+_2(r)|}{d(r)} \leq \frac{2}{\beta}$, for every $r \in V$.
        \item \label{property:g+2-c+2-1} $|c^+_2|_1 \leq \frac2\beta\sum_{u \in U}\frac{|\varphi^+_2(u)|}{d(u)} \leq \frac{2k}{\beta}$.
    \end{enumerate}
\end{coro}

We still need to bound $g^+_4$.

\begin{lemma}
    \label{lemma:g+3}
    There exists a vector $c^+_4 \in \R_{\geq 0}^{n}$ with the following properties:
    \begin{enumerate}[label=(\ref{lemma:g+3}\alph*)]
        \item \label{property:g+3-cost} $g^+_4 \leq \sum_{r \in V}c^+_4(r)\cdot \cost_\calC(r)$.
        \item \label{property:g+3-c+3-infty} $c^+_4(r) \leq  \frac{1}{\lambda} + \frac{1}{\beta} + \frac{1}{\lambda\beta} $, for every $r \in V$.
        \item \label{property:g+3-c+3-1} $|c^+_4|_1 \leq \big( \frac{1}{\lambda} + \frac{1}{\beta} + \frac{1}{\lambda\beta} \big)k$.
    \end{enumerate}
\end{lemma}








\begin{proof}\textcolor{red}{TOPROVE 26}\end{proof}

\begin{lemma}
\label{lemma:g+1+2+3}
$g^+_1 + g^+_2 + g^+_4 \leq (\frac{3}{\beta} + \frac{1}{\lambda} + \frac{1}{\beta\lambda} + 1)\cost^k_{\calC}$
\end{lemma}
\begin{proof}\textcolor{red}{TOPROVE 27}\end{proof}

Lemma \ref{lemma:g+1+2+3} gives us a way to bound the cost of deleted edges. Now consider the non-complete graph $G_2$ obtained from $G$ by removing any $+$edge $(u, v)$ (i.e., changing it into a "neutral" edge, and the cost of this edge is $0$) where $u$ and $v$ belong to different connected components of $\tilde{G}$. Note that for any clustering, the cost in $G_2$ is no more than the cost in $G$ since we set some $+$edge costs to 0. Now we are going to bound the cost of $F$ in $G_2$, where $F$ is the clustering output by Algorithm \ref{alg:pre-clustering}. This can give us the relationship between $F$ and $\calC$ in $G_2$. 

Since we will deal with multiple input graphs $G_2$, we include $G_2$ explicitly when necessary. Given a correlation clustering instance $G_2$, an integer $k$, and a clustering $\mathcal{C}$, we denote the disagreement vector in $G_2$ by $\cost_\calC(G_2)$. Consequently, the disagreement for a node $u$ in $G_2$ is specified by $\cost_\calC(G_2, u)$. For a subset $S \subseteq V$, the top-$k$ value on $G_2$ is represented by $\cost^k_\calC(G_2, S) = \max_{T \subseteq S, |T| = k} \sum_{u \in T} \cost_\calC(G_2, u)$. When $k \geq |S|$, then $\cost^k_\calC(G_2, S) = \sum_{u \in S} \cost_\calC(G_2, u)$

The following lemma provides the key insights that $F$ is a good clustering in $G_2$.

\begin{lemma}[Lemma 3.4 of \cite{cohen2021correlation}]
\label{lemma:preclusteringconnecttomostnodesinsameclustering}
    Suppose $5\beta + 2\lambda < 1$. Let $CC \in F$ be a connected component of $\widetilde{G}$ such that $|CC|\ge 2$. Then for each vertex $u\in CC$ we have that 
    \begin{align*}
        d(u,CC)\ge (1-8\beta-\lambda)|CC|.
    \end{align*}
\end{lemma}

Lemma \ref{lemma:preclusteringconnecttomostnodesinsameclustering} tells us that each node $u \in V$ is connected to no less than $\frac{5}{8}$ fractions of the vertices that belong to the same cluster in $F$ when assuming $8\beta + \lambda \leq 3/8$. Now we can bound the cost of $F$ in $G_2$.

\begin{lemma}
\label{lem:nearoptimalindeletedgraph}
Let $G_2$ be a non-complete graph obtained from G by removing any $+$ edge {u, v} (i.e., changing it into a
“neutral” edge) where u and v belong to different connected components of $\Tilde{G}$. Let $\mathcal{C}^*$ be the top-$k$ optimal clustering for graph $G_2$. Assuming $8\beta + \lambda \leq 3/8$. Then, our algorithm outputs solution $F$ such that 
\begin{align*}
    \cost^k_{F}(G_2) \leq 7\cdot \cost^k_{\calC^*}(G_2).
\end{align*}
\end{lemma}
\begin{proof}\textcolor{red}{TOPROVE 28}\end{proof}

Now we can show our main lemma regarding the approximate ratio.

\begin{proof}\textcolor{red}{TOPROVE 29}\end{proof}















































































%
 



\bibliographystyle{alpha}
\bibliography{local}

\appendix

\section{Reduction from All Monotone Symmetric Norms to Top-$k$ Norms}
\label{sec:all-norm}



\begin{definition}[Ordered Norms]
    For any vector $x\in \mathbb{R}_{\geq 0}^n$, let $x^\downarrow$ denote the vector $x$ with its coordinates sorted in non-increasing order. Given weight vector $w \in \R_{\geq 0}^n$ with $w_1 \geq w_2 \geq \cdots \geq w_n$, the $w$-ordered norm of $x$ is defined as $\order(w;x)=\sum_{i=1}^n w_i x_i^\downarrow$.
\end{definition}


\begin{lemma}[Lemma 5.2 of \cite{chakrabarty2019approximation}]\label{lem:Ordered2all}
    For any monotone and symmetric norm $f:\mathbb{R}^n\rightarrow \mathbb{R}_+$, define the set $\mathbb{B}_+(f):=\{x\in \mathbb{R}_+^n:f(x)\le 1\}$, and $W=\{w\in \mathbb{R}_+^n : w_1\ge w_2 \ge \cdots \ge w_n, w\ is\ a\ subgradient\ of\\ f\ at\ some\ x\in \mathbb{B}_+(f)\}$. Then we have $f(x)=\max_{w\in W} \order(w;x)$ for every $x\in \mathbb{R}_{\geq 0}^n$.
\end{lemma}

\lemmatopktolpnorm*
\begin{proof}\textcolor{red}{TOPROVE 30}\end{proof}
%
 
\section{Proof for Theorem \ref{theorem:constructHbysampling}}
\label{sec:constructH}
We repeat Theorem \ref{theorem:constructHbysampling} for convenience. 
\thmcnostructH*


\begin{proof}\textcolor{red}{TOPROVE 31}\end{proof} 
\section{Proof for Lemma \ref{lemma:samplingfinalvalue}}
\label{sec:proofofsamplingfinalvalue}
We repeat Theorem \ref{lemma:samplingfinalvalue} for convenience.
\lemmasmpaling*

\begin{proof}\textcolor{red}{TOPROVE 32}\end{proof} 





\end{CJK*}\end{document}
