\documentclass[11pt, a4paper, twoside]{article}
\sloppy
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{amssymb} 
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{pgfplots}
\usepackage{caption}
\usepackage{subcaption}
\usepackage[margin=1in]{geometry}
\usepgfplotslibrary{fillbetween}
\usetikzlibrary{fadings}
\usetikzlibrary {decorations.markings}
\usepackage{bbm}
\usepackage[shortlabels]{enumitem}
\usepackage{float}
\usepackage{stackengine}
\usepackage{accents}
\usepackage{appendix}
\usepackage{xcolor}
\definecolor{darkred}{rgb}{0.5,0,0}
\definecolor{lightblue}{rgb}{0,0.4,0.8}
\definecolor{darkgreen}{rgb}{0,0.5,0}
\usepackage{listings} \usepackage{xfrac}
\usepackage{comment}
\usepackage[export]{adjustbox}

\newcounter{sideremark}
\newcommand{\marrow}[1]{\stepcounter{sideremark}\marginpar{$\color{#1}\boldsymbol{\longleftarrow\scriptstyle    \color{#1}\arabic{sideremark}}$}}

\newcommand{\fnote}[1]{
	\ifdraft{
		\textsf{{\color{teal}*** (Frederik) \marrow{teal} #1 ***}}}\fi}

\newcommand{\rnote}[1]{
	\ifdraft{
		\textsf{{\color{darkgreen}*** (Rai) \marrow{darkgreen} #1 ***}}}\fi}

\newcommand{\gnote}[1]{
	\ifdraft{
		\textsf{{\color{orange}*** (Giordano) \marrow{orange} #1 ***}}}\fi}


\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
	language=Java,
	aboveskip=3mm,
	belowskip=3mm,
	showstringspaces=false,
	columns=flexible,
	basicstyle={\small\ttfamily},
	numbers=none,
	numberstyle=\tiny\color{gray},
	keywordstyle=\color{blue},
	commentstyle=\color{dkgreen},
	stringstyle=\color{mauve},
	breaklines=true,
	breakatwhitespace=true,
	tabsize=3
}
\usepackage[citecolor=blue]{hyperref}
\hypersetup{
	colorlinks=true,
	linkcolor=blue,
	urlcolor=blue,
}
\urlstyle{same}
\usepackage{graphicx}
\hypersetup{colorlinks=true, linktoc=all, linkcolor=blue}
\usetikzlibrary{arrows.meta}

\pgfplotsset{compat=newest}
\newcommand\smallO{
	\mathchoice
	{{\scriptstyle\mathcal{O}}}{{\scriptstyle\mathcal{O}}}{{\scriptscriptstyle\mathcal{O}}}{\scalebox{.7}{$\scriptscriptstyle\mathcal{O}$}}}
\DeclareRobustCommand{\bigO}{\text{\usefont{OMS}{cmsy}{m}{n}O}}
\newcommand*{\defeq}{\mathrel{\vcenter{\baselineskip0.5ex \lineskiplimit0pt
			\hbox{\scriptsize.}\hbox{\scriptsize.}}}=}
\newcommand*{\eqdef}{=\mathrel{\vcenter{\baselineskip0.5ex\lineskiplimit0pt
			\hbox{\scriptsize.}\hbox{\scriptsize.} }}}
\newcommand{\ssup}[1]{{\scriptscriptstyle{({#1})}}}
\newcommand{\eps}{\varepsilon}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\NNN}{\overline{\mathbb{N}}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\EE}{\mathbb{E}}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\TT}{\mathcal{T}}
\newcommand{\BT}{\overline{\mathcal{T}}}
\newcommand{\FF}{\mathcal{F}}
\newcommand{\BF}{\overline{\mathcal{F}}}
\newcommand{\HH}{\mathcal{H}}
\newcommand{\DD}{\mathcal{D}}
\newcommand{\ZZ}{\mathcal{Z}}
\newcommand{\PGF}{\mathcal{G}}
\newcommand{\PGFd}{\overline{\mathcal{G}}}
\newcommand{\Val}{\mathcal{V}}
\newcommand{\YY}{\mathbf{Y}}
\newcommand{\XX}{\mathbf{X}}
\newcommand{\VV}{\mathbf{V}}
\newcommand{\DZ}{\mathbf{Z}}
\newcommand{\DF}{\mathcal{B}}
\newcommand{\CC}{\mathcal{C}}
\newcommand{\II}{\mathbbm{1}}
\newcommand{\DM}{\mathcal{M}}
\newcommand{\as}{\:\text{a.s.}}

\DeclareMathOperator*{\esup}{ess\,sup} \DeclareMathOperator{\supp}{supp}
\DeclareMathOperator{\Geom}{Geom}
\DeclareMathOperator{\Unif}{Unif}
\DeclareMathOperator{\Ber}{Ber}
\DeclareMathOperator{\Par}{Par}
\DeclareMathOperator{\Beta}{B}
\DeclareMathOperator{\IHR}{IHR}
\DeclareMathOperator{\DHR}{DHR}
\DeclareMathOperator{\IHRE}{IHRE}
\DeclareMathOperator{\DHRE}{DHRE}
\DeclareMathOperator{\HIHRE}{HIHRE}
\DeclareMathOperator{\HDHRE}{HDHRE}
\DeclareMathOperator{\NBU}{NBU}
\DeclareMathOperator{\NWU}{NWU}
\DeclareMathOperator{\NBUE}{NBUE}
\DeclareMathOperator{\NWUE}{NWUE}
\DeclareMathOperator{\HNBUE}{HNBUE}
\DeclareMathOperator{\HNWUE}{HNWUE}
\DeclareMathOperator{\Var}{Var}

\usepackage[capitalize]{cleveref}
\crefname{equation}{}{}
\hbadness=10331
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{remark}{Remark}[section]
\newtheorem{definition}{Definition}[section]
\numberwithin{equation}{section}
\title{IID Prophet Inequality with Random Horizon:\\ Going Beyond Increasing Hazard Rates}
\author{Giordano Giambartolomei\setcounter{footnote}{1}\thanks{King's College London.}, Frederik Mallmann-Trenn\footnotemark[2] and Raimundo Saona\setcounter{footnote}{7}\thanks{Institute of Science and Technology Austria.}
}
\date{}

\begin{document}
        \maketitle
        \thispagestyle{empty}
        \begin{abstract}
        	Prophet inequalities are a central object of study in optimal stopping theory. In the \textit{iid} model, a gambler sees values in an online fashion, sampled independently from a given distribution. 
        	Upon observing each value, the gambler either accepts it as a reward or irrevocably rejects it and proceeds to observe the next value. The goal of the gambler, who cannot see the future, is maximising the expected value of the reward while competing against the expectation of a prophet (the offline maximum). 
        	In other words, one seeks to maximise the gambler-to-prophet ratio of the expectations. 
        	
        	This model has been studied with infinite, finite and unknown number of values. 
        	When the gambler faces a random number of values, the model is said to have random horizon. 
        	We consider the model in which the gambler is given \textit{a priori} knowledge of the horizon's distribution. 
        	Alijani~et~al.~(2020) designed a single-threshold algorithm achieving a ratio of $\sfrac{1}{2}$ when the random horizon has an increasing hazard rate and is independent of the values. 
        	We prove that with a single threshold, a ratio of $\sfrac{1}{2}$ is actually achievable for several larger classes of horizon distributions, with the largest being known as the $\PGF$ class in reliability theory. 
        	Moreover, we show that this does not extend to its dual, the $\PGFd$ class (which includes the decreasing hazard rate class), while it can be extended to low-variance horizons. 
        	Finally, we construct the first example of a family of horizons, for which multiple thresholds are necessary to achieve a nonzero ratio. We establish that the Secretary Problem optimal stopping rule provides one such algorithm, paving the way towards the study of the model beyond single-threshold algorithms.
        \end{abstract}
\clearpage
         \thispagestyle{empty}
        \setcounter{tocdepth}{1} \tableofcontents
       \clearpage
       \pagenumbering{arabic}
    \setcounter{footnote}{0}
	\section{Introduction}
	Prophet inequalities are a central object of study in optimal stopping theory. A gambler sees nonnegative values in an online fashion, sampled from an instance of independent random variables $\{X_i\}$ with known distributions $\{\mathcal{D}_i\}$, in adversarial, random or selected order, depending on the particular model. When observing each value, the gambler either accepts it as a reward, or irrevocably rejects it and proceeds with observing the next value. The goal of the gambler, who cannot see the future, is to maximise the expected value of the reward while competing against the expectation of a prophet (out of metaphor, the offline maximum or supremum, depending on whether the instance is finite or not). In other words, one seeks to maximise the gambler-to-prophet ratio of the expectations.
	
	The gambler represents any online decision maker, such as an algorithm or stopping rule. Probabilistically, we will refer to it as a \textit{stopping time} $\tau$. The term \textit{online} implies that the gambler, unable to see the future, will always stop at a time $\tau$ such that the event $\{\tau=i\}$ depends, informally speaking, only on the first $i$ values observed.

	Due to the online nature of prophet inequalities, some terminology from \textit{competitive analysis} is usually borrowed. Somewhat informally, it is common to refer to a worst-case gambler-to-prophet ratio of the algorithm (that is a ratio known to be achievable for any given instance by the algorithm) as \textit{competitive ratio}. Let $c\in[0,1]$ be a real constant. An algorithm is said to be $c$-competitive (or equivalently, a $\sfrac{1}{c}$-approximation) if it has a competitive ratio of $c$. An upper bound on any algorithm's highest possible competitive ratio for a given instance will be called \textit{hardness} of the instance. Saying that a prophet inequality model has a hardness of $c$ (or equivalently is $c$-hard ), means that there is a $c$-hard instance for that problem. A hardness for a model is said to be \textit{tight} when it is matched by the competitive ratio of an algorithm solving it. Similarly, the competitive ratio $c$ of an algorithm solving a model is tight when it is matched by a hardness of $c$ for that model (equivalently, the algorithm is a tight $\sfrac{1}{c}$-approximation).
	
	\subsection{Prophet inequality models}\label{model}
	To date, several models and extensions of prophet inequalities are present in the literature. In this section we introduce the classical model and describe two variants of it, briefly reviewing the state-of-the-art concerning their hardness and the competitiveness of known algorithms.
	
	\paragraph{Classical Prophet Inequality.}
	The very first prophet inequality model, typically referred to as classical (or adversarial) Prophet Inequality (PI), is due to Krengel and Sucheston \cite{KrenSuch77,KrenSuch78}. The given instance is composed of  countably (possibly infinitely) many integrable independent nonnegative random variables $\{X_i\}$ with known distributions $\{\mathcal{D}_i\}$ in a fixed given order, usually referred to as \emph{adversarial} order. As per the general dynamics previously described, the gambler observes in an online fashion the sequence of sampled values in the fixed order. Formally, the goal is to maximise the gambler-to-prophet ratio $\EE X_\tau$ over $\EE\sup_iX_i$. When working with \textit{infinite} instances, it is customary to consider only finite stopping rules and and variables such that $\EE\sup_iX_i<\infty$.\footnote{Formally, we maximise the gambler-to-prophet ratio with respect to all stopping times $\tau$ such that $\PP(\tau<\infty)=1$.} In~\cite{KrenSuch78}, it was shown that the $\sfrac{1}{2}$-hardness of PI (shown by Garlin \cite{KrenSuch77}) is tight. Later, a single-threshold $2$-approximation was constructed in \cite{Sam84}.
	
	\paragraph{IID Prophet Inequality.}
	Denoted as IID PI, it is a specialised model of PI, where the random variables $\{X_i\}$ are further assumed independent and identically distributed (\textit{iid}) according to a given distribution $\DD$. The hardness of this problem has been shown to be $\sfrac{1}{\beta}\approx 0.7451$, where $\beta\in\mathbb{R}$ is the unique solution to $\int_0^1 \frac{dx}{x(1-\log x)+\beta-1} = 1$ in~\cite{Kertz86}. The $\beta$-approximation \textit{quantile strategy} devised in~\cite{Correa17} shows that the aforementioned hardness is tight. 

	\paragraph{IID Prophet Inequality with Random Horizon.}
	The IID PI with Random Horizon (RH) was first introduced in \cite{HajKleSan07}. Consider a random variable $H$, which we will call the (random) \textit{horizon}, with given discrete distribution $\HH$, that is, supported on an arbitrary subset of $\NN$. This assumption comes with no loss of generality (formally, it rules out that the horizon has mass at zero). $H$ will be assumed finite ($\PP(H<\infty)=1$) and integrable ($\EE H <\infty$), which we denote $H\in\mathcal{L}^1(\Omega)$. RH is a relaxation of the IID PI, considering integrable \textit{iid} random variables $X_1,\ldots,X_H$, with given distribution $\DD$ independent of $\HH$. This setup, supported on a probability space $(\Omega,\FF,\PP)$, models the game where the gambler, facing an unknown number of values $X_1,\ldots,X_H$, can still use \textit{a priori} knowledge of $\HH$ when maximising returns. More precisely, denote $[n]=\{1,\ldots,n\}$. The goal is to maximise the gambler-to-prophet ratio of $\EE X_\tau$ over $\EE M_H$, where $M_H\defeq \max_{i\in[H]}X_i$ and both expectations run over the randomness of the \textit{iid} copies of $X\sim\DD$ and $H\sim\HH$. The gambler must select the value in ignorance of whether it is the last or not. If the gambler fails to stop by the time the last value has been inspected, the return is zero. This ignorance, added to the usual nonanticipative constraint, yields a no easier model than the IID PI. On top of the usual disadvantage of playing against a prophet, which knows all future realisations of the values, and chooses the largest; the gambler now competes against a prophet, which can also foresee the random number of values. We leave more in depth measure-theoretic details of the model for \Cref{opt}. 

	\subsection{Previous bounds on the IID Prophet Inequality with Random Horizon}\label{bounds}
	No constant-approximations are possible for RH \cite[Theorem~1.6]{AliBanGolMunWan20}. Distributional knowledge of the horizon is not enough to guarantee, in expectation, that the gambler's return can attain a worst-case constant multiple of the prophet's return. Therefore, we impose additional restrictions on the distribution of the horizon.  The largest distributional class for which a constant-approximation has been found so far is that of horizons with increasing hazard rates $\lambda(h)$. For simplicity, we will use \textit{increasing} and \textit{decreasing} instead of \textit{nondecreasing} and \textit{nonincreasing} respectively. Recall that the hazard rate $\lambda(h)$ of a horizon $H$ is defined as follows. Denote the survival function $S(h)=\PP(H\ge h)$. If $|\supp(H)|=\infty$, for every $h\in\NN$, $\lambda(h)\defeq \PP(H=h)/S(h)$. If $|\supp(H)|<\infty$, for every $h\in\NN$, $\lambda(h)$ is defined analogously for all $ h\le\sup\supp(H)$, whereas it is set to $1$ for all $h>\sup\supp(H)$. \begin{definition}[$\IHR$ class]
		$H$ is \emph{Increasing Hazard Rate ($\IHR$)} if for every $h\in\NN$, $\lambda(h)\le\lambda(h+1)$.
	\end{definition}
	The \textit{dual} class is defined by reversing the inequality on the support of the horizon $\supp(H)$.\begin{definition}[$\DHR$ class]
		$H$ is \emph{Decreasing Hazard Rate ($\DHR$)} if for every $h\ge\inf\supp(H)$, $\lambda(h)\ge\lambda(h+1)$.
	\end{definition}
	The geometric distribution is the only discrete distribution that has constant hazard rate, and is therefore the only discrete distribution that belongs to both classes.
	
	The state-of-the-art for RH consists in the findings of \cite{AliBanGolMunWan20}. In \cite[Theorem~3.2]{AliBanGolMunWan20}, through the use of \emph{second order stochastic dominance}, it is shown that if $H\in\IHR$, $\EE X_\tau\ge  (2-\sfrac{1}{\mu})^{-1} \EE M_H$, where $\mu\defeq\EE H\ge 1$. This ensures a (uniform) $2$-approximation on the $\IHR$ class.\footnote{For brevity, outside formal statements, we will often omit the dependence on distributional parameters when referring to competitive ratios. For example, in this sentence \textit{$2$-approximation} means, more precisely, $2-\sfrac{1}{\mu}$-approximations.}  Remarkably, this is achieved by a single-threshold algorithm \cite[Theorem~3.1]{AliBanGolMunWan20}, and it is tight \cite[Theorem~3.5]{AliBanGolMunWan20}, since if $H$ is geometrically distributed, a parametric family of two-point distributions can be shown to make the prophet-to-gambler ratio approach $2-\sfrac{1}{\mu}$, as the parameters vary suitably. The emphasis on monotone hazard rate classes stems from \cite{HajKleSan07}, which is the first work that considers RH, to the best of our knowledge.
	
	\subsection{Our contributions}\label{contributions}
	Implicitly, the motivation behind the approach that considers monotone hazard rate classes stems from interpreting RH from a reliability theory point of view: an optimal stopping game under evolving risk (or ageing) that it ends by the next step. In reliability theory several classes of distribution are used to model this aspect. However, despite more than $15$ years have passed, no progress has been made on classes larger than the $\IHR$ class. In this paper we contribute to the study of RH by:
	\begin{itemize}[noitemsep]
		\item Extending existing characterisations of the optimal algorithm to unbounded horizons.
		\item Complementing and extending key ideas from \cite{AliBanGolMunWan20} to yield the existence of single-threshold $2$-approximations on important superclasses of the $\IHR$ class, culminating in the $\PGF$ class.
		\item Showing that the $\PGFd$ class (dual of the $\PGF$ class) admits hard horizons for single-threshold algorithms..
		\item Deriving single-threshold constant-approximations for sufficiently concentrated horizons with finite second moments.
		\item Showing that RH admits families of horizons for which single-threshold algorithms are not competitive, despite the Secretary Problem (SP) optimal stopping rule providing a constant-approximation (see \Cref{related} for details on SP and its variants).
	\end{itemize}
	
	In this section we give formal statements of the above results, with the exception of \cref{optalg} in \Cref{opt}, due to the overall degree of technicalities involved. Informally speaking, this theorem characterises the optimal algorithm in terms of a discounted infinite optimal stopping problem, with a special focus on unbounded horizons. As a technical tool, the equivalence of stopping rules for RH with stopping rules for the discounted problem is leveraged not only to obtain hardness results for single-threshold algorithms, but also to formalise heuristic arguments involving the optimal algorithm under geometric horizon. More specifically: we exploit the equivalence between stopping rules for RH and corresponding rules for the discounted problem to extend hardness to neighbouring instances; we show that with a geometric horizon, if the value distribution is bounded, the optimal algorithm of RH is single-threshold, pinpointing the structural equation for the threshold. 
	
	The next three results involve a stochastic order, which we now introduce. Standard facts regarding stochastic orders are often relied upon throughout this work. They will be recalled as remarks, whose proof can be found in~\cite{ShakShan07}. In the following definitions, $H$ and $G$ always refer to horizons. 
	\begin{definition}[Probability Generating Function Order]
		Given two horizons $H$, $G$, we say that $G$ is dominated by $H$ in the \emph{probability generating function (pgf) order}, and denote it as $G\prec_{pgf} H$, if for every $t\in(0,1)$, $\EE\left( t^G\right)\ge\EE\left( t^H\right)$.
	\end{definition}
	\begin{remark}
		$G\prec_{pgf} H$ if and only if for all $t\in(0,1)$, $\sum_{i=1}^\infty S_G(i)t^i\le\sum_{i=1}^\infty S_H(i)t^i$.
	\end{remark}
	The $\PGF$ class, consisting of every positive discrete distribution that dominates, in the pgf order, the ($\NN$-valued) geometric distribution with the same mean, has been introduced in \cite{Klef83a}.
	\begin{definition}[$\PGF$ class]\label{Gdef}
		The $\PGF$ class consists of every horizon that dominates, in the pgf order, the geometric distribution with the same mean: $\PGF\defeq\{H:\, G\prec_{pgf} H,\, G\sim\Geom(\sfrac{1}{\EE H})\}$.
	\end{definition}
	The dual class is obtained by reversing the pgf ordering, and is denoted as $\PGFd$. It is well known that $\IHR\subset\PGF$, with the inclusion being strict. Similarly, $\DHR\subset\PGFd$. In applications of reliability theory, the classes within $\mathcal{G}$ and its dual are a staple of modelling many aspects of ageing and more in general of an evolving risk of failure.
	\begin{theorem}\label{Ghorizon}
		For every $H\in\PGF$ with $\EE H\eqdef\mu$, a single-threshold $2-\sfrac{1}{\mu}$-approximation exists.
	\end{theorem}
	From \cite[Theorem~3.5]{AliBanGolMunWan20} it follows that any $2$-approximation on the $\PGF$ class is tight. More specifically, the $2$-approximation yielding \Cref{Ghorizon} is essentially\footnote{The argument and the algorithm provided in \cite{AliBanGolMunWan20}, although valuable and insightful, are somewhat informal and require some tweaks and filling some gaps. A more in depth discussion is given in \Cref{Gclass}, where we show that RH is amenable enough to allow for stochastic tie-breaking, a standard techniques which combines well with the aforementioned tweaks.} the same as the one used on the $\IHR$ class in \cite[Theorem~3.1]{AliBanGolMunWan20}. The key feature of the stopping rule is determining the value $p$ such that $\PP(X>p)=\sfrac{1}{\mu}$, and then accepting the first value exceeding $p$. The $\PGF$ class may be somewhat abstract, compared to its subclasses having immediate intuitive interpretations in terms of ageing concepts, such as the $\IHR$ class. Its possible interpretations are quite general and go beyond mere ageing, as can be read in \cite{Klef83a}. To gain some intuition of how much more general of a class it is, the reader is referred to \Cref{Gclass}, where we will describe some of its largest most well-known subclasses, as evidence that \Cref{Ghorizon} generalises significantly \cite[Theorem~3.1,~Theorem~3.2]{AliBanGolMunWan20}.\footnote{It may be beneficial to give a concrete example of a distribution that, although elementary, could not be handled without our extension. Let $H$ with $S(h)$ given as $1, \sfrac{1}{2}, \sfrac{1}{4}, \sfrac{1}{5}$ for $h=1,2,3,4$ respectively and $0$ for all $h>4$. The list of the corresponding hazard rates $\lambda(h)$, $\sfrac{1}{2}, \sfrac{1}{2}, \sfrac{1}{5}, 1,\ldots$, is not monotone increasing because $\lambda(3)$ drops. The $\PGF$-class condition is, however, satisfied, thus our result can ensure a $2$-approximation (to be specific, a $2-\sfrac{20}{39}\approx1.49$-approximation). It is straightforward to compute $\EE H =\sfrac{39}{20}$, so \Cref{Gdef} requires $\EE t^G\ge\EE t^H$ for all $0<t<1$, with $G\sim\Geom\left(\sfrac{20}{39}\right)$, and the two pgf's being respectively $20t/(39-19t)$ and $\sfrac{t}{2}+\sfrac{t^2}{4}+\sfrac{t^3}{20}+\sfrac{t^5}{5}$.} We conclude by mentioning that statistical tests for the $\mathcal{L}$ class (the continuous equivalent of the $\PGF$ class) have also been developed \cite{Kle83b}. The possibility of adapting them into tests for the $\PGF$ class adds to the practical significance of the result.
	
	Our next result requires introducing the stopping rule for SP with deterministic horizon $m$. This consists of rejecting the first $r-1$ values, and subsequently accepting the first value ranked better than all the previous ones. The waiting time $r_m\sim\sfrac{m}{e}$ as $m\longrightarrow\infty$ yields the optimal stopping rule (see \Cref{related} for details). In the following, we provide the first example of a family of horizons, for which no single-threshold constant-approximation is possible, and yet an adaptation of the SP stopping rule to random horizons is proven to provide a constant-approximation. The family of horizons we will consider is of the form $\{H_m\}_{m\ge M}$, with pmf's parametrised by $\eps>0$, $M\in\NN$ large enough and $m\defeq \sup\supp(H_m)<\infty$, and will therefore be denoted as $\mathcal{H}_M(\eps)$. Let $\zeta_{r_m}$ be the (optimal) stopping rule for SP with deterministic horizon $m$. Taking the minimum $\tau_m\defeq\zeta_{r_m}\wedge(H_m+1)$ produces a natural adaptation of the SP stopping rule to the random horizon $H_m$.
	\begin{theorem}\label{future}
		There exists a family of horizons $\mathcal{H}_M(\eps)$, such that for all fixed $\eps$ small enough and arbitrary $M$, no single-threshold constant-approximation is possible. Nonetheless, for every fixed $\eps$, no matter how small, the random horizon SP rule $\tau_m$ is an approximate constant-approximation, as long as $M=M(\eps)$ is fixed large enough.
	\end{theorem}
	For brevity, we say that the family of horizons $\mathcal{H}_M(\eps)$ is \textit{hard} for single-threshold algorithms, but the random horizon SP rule is \textit{competitive} on it.\footnote{We omit the qualifier \textit{approximate} appearing in the statement of \Cref{future},  a technicality which we now clarify. \textit{Approximate} refers to the fact that the competitive ratio ensured, which is a positive, increasing function of $\eps$, denoted as $g(\eps)$, is approached as $M\longrightarrow\infty$. At the large $\eps$ regime, $g(\eps)\approx\sfrac{1}{e^2}$. As $\eps$ grows, the family of horizons $\mathcal{H}_M(\eps)$ admits constant-competitive single-threshold algorithms too. However, at small $\eps$ regime, these algorithms fail, whereas $\tau_m$ guarantees approximately a $g(\eps)$ fraction of $\EE M_{H_m}$. Specifically, for all $m\ge M$, a ratio of $g(\eps)-\delta$ is guaranteed, where $\delta=\delta(M)\longrightarrow 0^+$ as $M\longrightarrow\infty$. Hence the dependence $M=M(\eps)$ with $M(\eps)$ growing large as $\eps$ vanishes.} Considering the extensive number of classes of distributions for which single-threshold algorithms are competitive, this result is quite surprising, especially because the SP stopping rule only needs information regarding the relative ranks of the values, not the values themselves.
	
	Since we showed that single-threshold $2$-approximations exist for the $\PGF$ class, finding whether single-threshold constant-approximations exist for the $\PGFd$ class turns into an interesting problem. The equivalent characterisation of stopping rules for RH in terms of a discounted stopping problem, combined with a perturbation argument on $\mathcal{H}_M(\eps)$, yields a negative answer.
	\begin{theorem}\label{Gdhorizon}
		No single-threshold constant-approximation is possible on the $\PGFd$ class.
	\end{theorem}
	In \Cref{Gdclass} we will describe more in detail some of the largest most well-known subclasses of the $\PGFd$ class. They are no less important than the corresponding subclasses of the $\PGF$ class. For example, discrete $\DHR$ distributions are crucial in several areas: they have been shown to describe well the number of seasons a show is run before it gets cancelled and the number of periods until failure of a device (a system, or a component, that functions until first failure) governed by a continuous $\DHR$ life distribution in the grouped data case \cite{Langberg80}. \Cref{Gdhorizon} motivates our conjecture that single-threshold $2$-approximations are not possible on the $\DHR$ class either. Since our characterisation result of stopping rules for RH extends to unbounded horizons, it will be a crucial tool in proving this conjecture within our framework, as $\DHR$ horizons have support of the form $[a,\infty)\cap\NN$. Furthermore, suitably combining the techniques yielding \Cref{Ghorizon,Gdhorizon} with the fact that the tight geometric instance for $2$-approximations belongs to the $\PGF$ class, motivates our second conjecture that the $\PGF$ class is essentially optimal for $2$-approximations on $\mathcal{L}^1$ horizons.
	
	In our last result, we go back to single-threshold algorithms, but consider only horizons $H$ satisfying th e sole condition $\EE( H^2)<\infty$, denoted $H\in\mathcal{L}^2(\Omega)$. Here no particular information is available regarding the classification of the distribution in terms of the $\PGF$. We show that it is possible to exploit concentration bounds, in order to ensure that the same single-threshold algorithm used for the $\PGF$ is still a $2$-approximation. In the following result, $W_0$ denotes the principal branch of the Lambert function.
	\begin{theorem}\label{L2horizon}
		For every $H$ having both $\mu\defeq \EE H$ and $\sigma^2\defeq\Var H$ finite, such that
	\begin{equation}\label{concentration}
        \sigma^2\le\mu^2\left[ 1-\frac{1}{\mu}+W_0\left(-\left(2-\frac{1}{\mu}\right)e^{-\left(2-\frac{1}{\mu}\right)}\right)\right],
        \end{equation}
		a single-threshold $2-\sfrac{1}{\mu}$-approximation exists.
	\end{theorem}
	The Lambert function is readily simulated in many languages, with well-studied error bounds. Therefore, one can reliably compute numerically the magnitude of concentration required by \Cref{concentration}, so that a $2$-approximation is ensured. To exemplify, consider the large-market limit, that is $\mu\longrightarrow\infty$, which is both interesting for practical applications involving horizons with large expectation, and simplifies computations. Under this hypothesis, taking square-root on both sides of \Cref{concentration} yields approximately $\text{CV}\defeq\sfrac{\sigma}{\mu}\le0.770$, where $\text{CV}$ is the \textit{coefficient of variation} of the horizon. 
	Variations over this procedure show that both weaker and stronger than $2$ constant-approximations are possible for all suitably concentrated horizons. Consider again the upper bound on $\text{CV}$ provided by square-rooting \Cref{concentration} in the large-market limit: $\text{CV}\le \sqrt{1+W_0(-2e^{-2})}$. Replacing $2$ with $C>2$ yields $\text{CV}\le \sqrt{C-1+W_0(-Ce^{-C})}$. This will ensure a large-market limit weaker $C$-approximation. As $x\ge1$ grows, $W_0(-xe^{-x})$ is negative strictly increasing (valued $-1$ at $x=1$) and vanishing. Thus the upper bound on the $\text{CV}$ ensuring a $C$-approximation for $C$ growing large scales roughly as $\sqrt{C-1}$, meaning that there will be, although progressively weaker, constant-approximations, as long as $\text{CV}<\sqrt{C-1}$.
	In the other direction, stronger $C$-approximations are ensured when taking \textit{admissible} $1<C<2$. When reducing $C$, we clearly cannot go past $e/(e-1)$, where the upper bound on the $\text{CV}$ vanishes, meaning that the horizon is required to progressively concentrate until it becomes degenerate. At this point our estimates hit the deterministic optimum for single-threshold IID PI, $1-\sfrac{1}{e}$ \cite{Esh18}. Following similar ideas, the following holds without large-market hypothesis.
	\begin{corollary}\label{largemarket}
		For every $H$ having both $\mu\defeq \EE H$ and $\sigma^2\defeq\Var H$ finite, and every constant $C\ge[1-\left(1-\sfrac{1}{\mu}\right)^{\mu}]^{-1}$, if \[\text{CV}\le \sqrt{C-1+W_0(-Ce^{-C})},\] then a single-threshold $C$-approximation exists.
	\end{corollary}
	At a closer look, it is not surprising that stronger approximations are possible within the class of sufficiently concentrated horizons. Although it is true that the geometric horizon is tight for $2$-approximations in $\mathcal{L}^2(\Omega)$ (which is the reason for which \Cref{L2horizon} is formulated in terms of them), the aforementioned properties of $W_0(-xe^{-x})$ readily imply that no geometric horizon satisfies \Cref{concentration}.\footnote{It might be beneficial to give another concrete example of a distribution that, although elementary and sufficiently concentrated for our result to ensure a $2$-approximation (to be specific, a $1.89$-approximation), could not be handled by previous results. Consider $H$ having Zipfâ€™s distribution on $[20]$ with shape $a = 0.3$, that is with probability mass function $f(h)\defeq \sfrac{h^{-a}}{Z}$ for all $h\in[20]$, where $Z\defeq\sum_{h=1}^{20} h^{-a} = 10.9$. The list of hazard rates $\lambda(h)$ is approximately $0.0914, 0.0818, 0.0789, 0.0785, 0.0797, 0.0820, 0.0853, 0.0896, 0.0950, \ldots$, so it is not monotone. Yet the distribution satisfies \Cref{concentration}, since $\Var H\approx 34.6$ is smaller than the right-hand side of \Cref{concentration}, which evaluates to approximately $37.0$ (all values provided are numerical approximations up to machine precision).}
	
	\subsection{Our techniques}\label{techniques}
	The toolbox we assembled, can be described in three key-points.
	
	\textbf{Discounted infinite problem.} The optimal algorithm is characterised via a correspondence between stopping rules for RH and stopping rules for a discounted optimal stopping problem, with the discount factors being the survival function of the horizon. More general optimal stopping problems (unbounded random horizon) do not always admit an optimal algorithm~\cite{Sam96}. We show that for RH, this is always possible. This is crucial for geometric and $\DHR$ horizons, which are unbounded. The generalisation is the product of an original measure-theoretic construction, leveraging trace $\sigma$-algebras.\footnote{The reader not familiar with measure-theoretic probability should not be discouraged, since the rest of our main proofs leverage only the intuitive meaning of this, which will be made clear.}
	
	\textbf{Stochastic orders.} The extension of the $2$-approximation from the $\IHR$ class to the $\PGF$ class and $\mathcal{L}^2$ horizons is obtained by exploiting stochastic orders, which have never before been applied to prophet inequality problems. Several distributional classes, starting from the $\IHR$ class, can be equivalently defined through some stochastic-ordering with respect to the geometric distribution (as previously seen, the largest of such classes exploits the pgf order). Thus our \cref{Ghorizon} not only extends to the $\PGF$ class, but simplifies the arguments of \cite{AliBanGolMunWan20}. The proof of \Cref{L2horizon} leverages extremal properties of specific Bernoulli distributions with respect to another stochastic order, the Laplace transform order (see \Cref{L2} for details), for distributions having finite variance.
	
	\textbf{Hard instance.} Our analysis of the hard instance for single-thresholds algorithms is also novel. The literature on prophet inequalities has focused only on bounded discrete instances that are hard for every algorithm. On the other hand, the instance we provide exploits a continuous Pareto distribution, which, combined with a suitable family of horizons, is both hard for single-threshold algorithms, and competitive for the SP stopping rule, against the offline maximum. This requires a more sophisticated and analytic approach. The phenomenon, for which certain algorithms fail on an instance but others do not, is subtle. To the best of our knowledge, a competitive analysis of the SP stopping rule against the offline maximum with a random horizon is also new (see \Cref{related}). It is worth noting that this hard instance is designed to live in a close neighbourhood of the $\PGFd$-class, so that perturbing it suitably yields, at the same time, a hard family in the $\PGFd$ class.

	\subsection{Related work}\label{related}
	The literature on prophet inequalities models with deterministic horizon is vast. The techniques required for RH differ significantly, and the reader interested in the deterministic setting is referred to surveys such as \cite{Correa18}, which contains many recent developments; \cite{Lucier17,Correa19}, for an economic point of view (including applications to Posted Price Mechanisms and more broadly online auctions); \cite{HillKertz92}, for classical results concerning infinite instances. Besides the classical and IID PI, already reviewed in \Cref{model}, the other two main models with deterministic horizon we mention here are the Random Order and the Order Selection PI. In the first the random values arrive in a uniform random order (this model has been shown to be $0.688$-competitive~\cite{ChenHuangLiTang24} and $0.7235$-hard~\cite{Giam23}), in the second the order can be chosen by the gambler (this model has been shown to be $0.7258$-competitive~\cite{BubChip23} and is $\sfrac{1}{\beta}$-hard, since the IID PI is a particular case). Very recently also Oracle-augmented PI have been introduced \cite{HarHarbLiv24}. The existing literature strictly related to RH is limited to \cite{AliBanGolMunWan20, HajKleSan07,MahSab06}, which explore the topic from the point of view of online automated mechanism design (see \cite{Sand03} for a survey), with a strong emphasis on the multiple-items setting. Although the setting treated in this work is the single-item one, it is crucial, since a standard machinery has already been developed in \cite{AliBanGolMunWan20} to extend results for single-item RH to multiple items, as long as the horizons for each item are uniformly bounded. To the best of our knowledge, \cite{MahSab06} is the first paper to consider the problem of unknown market size (that is, random horizon with unknown distribution). As an example of practical applications of RH, the original motivation proposed by the authors was that in search keyword auctions such as Google, the supply (search queries that the users enter, resulting in a search results page on which ads can be displayed) is not known in advance, and arrives in an online fashion. This is a multi-unit auction of unknown size for a perishable item (the search queries, who perish within a fraction of a second), to be allocated to buyers (the ads that will be displayed on the search results page). Their analysis provides constant-approximations of both the online and offline optimal single-price algorithm. These results are limited to unknown horizons. This assumption has later been shown to be hard for RH in \cite{HajKleSan07}. In practice, distributional knowledge is available via the history of past transactions. Therefore, in \cite{HajKleSan07} the focus shifted to admitting known horizons. As to obtaining constant-approximations, however, they focus solely on generalising to multiple items the \textit{additive} prophet inequality of \cite{HillKertz83} for \textit{bounded} value distributions, whereas our main focus is on multiplicative prophet inequalities, in line with \cite{AliBanGolMunWan20}, already extensively reviewed (see \Cref{bounds}). We conclude this paragraph by mentioning that, on a technical level, RH is also well-recognised as connected to other online stochastic matching problems with random number of queries, which have recently been shown to admit $2$-approximations \cite{AouMa23}. A potential ground for applications of our techniques would be improving such guarantees.
	
	In the remaining of this section we will focus on another well-known problem in theoretical computer science, the Secretary Problem (SP), extensively studied in the random horizon setting, partially inspiring our approach. For the history and variations on the problem see \cite{GilMost66,Free83}. The classical SP is as follows: let $n$ be a known number of secretaries for hire. They are interviewed in an online fashion by the employer, all $n!$ possible orderings being equally likely. The employer is able, at the end of each interview, to rank all the secretaries that have been presented so far ( \textit{relative ranks}). Each time, the employer must either hire (and stop with the interview), or irrevocably reject the secretary (and proceed with the next interview, except for the last one: the employer will have to hire the last secretary, if the previous $n-1$ were rejected). The objective is maximising the probability of choosing the best secretary, that is the one having \textit{absolute rank} $1$. The problem is solved in \cite{Lind61}: the optimal stopping rule consists in rejecting all the first $r-1$ of the $n$ secretaries, and to accept the first secretary, which is better than all previous ones (relative rank $1$) thereafter. The number $r=r_n$ can be characterised such that as $n\longrightarrow\infty$, both $\sfrac{r}{n}$ and the probability of selecting the best (secretary) tend to $\sfrac{1}{e}\approx 0.3679$. This result is also obtained in \cite{Dynk63} through an alternative study of an underlying Markov chain. Consider the following process $\{x_t\}_{t\in[n]_0}$: if the employer enumerates the secretaries in the order in which they are encountered, and then lets $x_0=1$, and for $t\in[n]$, lets $x_t$ be the number of the first secretary, which is preferable to the one denoted by $x_{t-1}$; then we have that $\{x_t\}$ is a Markov chain with state space $[n]$. Potential-based optimal stopping techniques yield the aforementioned results. The Markovian approach is crucial to the analysis of random horizon SP.
	
	When the number of secretaries is random, it is denoted by $N$, and the secretaries are interviewed in uniform random order, conditionally on $N$. At each interview the employer, who has distributional knowledge of $N$, makes the decision of stopping in ignorance of whether there will be a secretary to interview next. The employer would obtain no reward, in the eventuality that the secretary just rejected was indeed the last. This model was first studied in \cite{PresSon72} through a generalisation of the Markovian approach of \cite{Dynk63}. The optimal stopping rule is no longer as simple as in the deterministic case, where we had that it is optimal to stop after a waiting time, as soon as a secretary having relative rank $1$ is interviewed. This is called one \textit{stopping island} scenario. Depending on the horizon's distribution, we can have a more complicated set $\Gamma$ composed of multiple islands, in which it is optimal to stop at a secretary with relative rank $1$, and outside which it is not. For uniform, geometric and Poisson horizons there still is only one island yielding non-vanishing asymptotics of the probability of choosing the best, but the value varies. In \cite{AbdBatTrus82} it was later shown that a hard family of horizons $\{N_m\}$ (with $\supp(N_m)=[m]_0$, for every $m\in\NN$) could be constructed, meaning that as $m\longrightarrow\infty$, the probability of choosing the best vanishes. In order to recover a constant nonzero limiting value, more information is needed.

	SP has also been studied in its \textit{full-information} variant, that is when the common continuous distribution, from which the quality measurement of each of the $n$ secretaries is sampled independently, is known by the employer, who can proceed to directly inspect, at each interview, the secretary's true quality. A first heuristic solution to this problem was given in \cite{GilMost66} via a backward induction argument. A rigorous proof of this form of the optimal stopping rule was successively derived in \cite{Boj78} via the Markovian approach. The full-information variant has also been studied with random horizon by \cite{Por87}, where the Markovian approach of \cite{Boj78} is successfully extended. To the best of our knowledge, a hard instance is not yet known for the full-information problem with random horizon. 

	Both no- and full-information SP have also been studied under the harder hypothesis of \textit{random freeze}, respectively in \cite{Sam95,Sam96}. In this case, given $n$ secretaries, interviews may have to stop  at an independent random time $N\le n$, with distribution known to the employer. Finally, we mention a variant of SP studied in \cite{ChoMorRobSam64}: instead of seeking to maximise the probability of choosing the best, the employer seeks to minimise the expected rank of the chosen secretary. This problem was extended to bounded random horizons $N$ in \cite{Gia79}, and studied for a parametric family of $\IHR$ horizons. To the best of our knowledge this is the first work, which investigates broad distributional classes (note that they are defined in terms of hazard rate properties) rather than families of distributions of a given form. More recent developments extend SP to combinatorial structures and unknown horizons~\cite{HoefKod17,GharVon13}.
	
	\subsection{Organisation of the paper and notation}
	
	The paper is organised as follows. In \Cref{opt} we give preliminaries on the formal approach to random horizon optimal stopping problems, and prove the equivalence with discounted optimal stopping problem. In \Cref{Gclass} we prove \Cref{Ghorizon}.  In \Cref{hardsecretary} we prove \Cref{future}. In \Cref{Gdclass} we prove \Cref{Gdhorizon}. In \Cref{L2} we prove \Cref{L2horizon}. The Supplementary materials contain a detailed version of all the proofs.
	
	\section{Preliminaries}\label{opt}
	In this section we set up our notation, give finer details regarding the probabilistic model of RH and characterise the optimal algorithm by deriving a useful equivalence between stopping rules for RH and stopping rules of an associated discounted (possibly infinite) stopping problem.
	
	\subsection{Notation} 
	We denote $\NN_0\defeq\NN\cup\{0\}$, $\NNN\defeq\NN\cup\{\infty\}$, $\overline{\RR}\defeq\RR\cup\{\infty,-\infty\}$, $[n]\defeq\{1,2,\ldots,n\}\subset\NN$, $[0]=\{0\}$, $[n]_0\defeq\{0\}\cup[n]$, $a\wedge b\defeq\min(a,b)$ and $a\vee b\defeq \max(a,b)$ for all $a,b\in\RR$. Given nonnegative functions $f(x)$ and $g(x)$ (or equivalently the corresponding sequences $\{f_n\}$ and $\{g_n\}$ if we consider $x=n\in\NN$) and a point $a\in\overline{\RR}$, assume, for simplicity, that $g(x)$ is strictly positive in a punctured neighbourhood of $a$ (if $a=\infty$, this is understood to mean for all $x$ large enough). Then we denote:
	\begin{itemize}[noitemsep]
		\item $f(x)=\bigO(g(x))$ as $x\longrightarrow a$ if $\limsup_{x\longrightarrow a}\frac{f(x)}{g(x)}<\infty$.
		\item $f(x)=\smallO(g(x))$ as $x\longrightarrow a$ if $\lim_{x\longrightarrow a}\frac{f(x)}{g(x)}=0$.
		\item $f(x)=\Omega(g(x))$ as $x\longrightarrow a$ if $\liminf_{x\longrightarrow a}\frac{f(x)}{g(x)}>0$.
		\item $f(x)\sim g(x)$ as $x\longrightarrow a$ if $\lim_{x\longrightarrow a}\frac{f(x)}{g(x)}=1$.
		\item $f(x)\asymp g(x)$ as $x\longrightarrow a$ if $f(x)=\bigO(g(x))$ and $f(x)=\Omega(g(x))$.
	\end{itemize}
	Some of the asymptotic notation above, as customary, can also be extended to a function $f(x)$ possibly taking negative values. When this occurs, it is understood that we consider $|f(x)|$ instead. We will adopt the convention of omitting the dependence on all parameters not involved in the main limiting process (which will be clear from the context), regardless of whether uniformity (when not relevant to the context) holds within the parametric range.\footnote{For example, the sentence: as $x\longrightarrow\infty$, for every $0<\eps<1$, $f(x,\eps)\asymp g(x,\eps)$; means that $\asymp$ refers to $x$, for every $\eps$ fixed; we avoid $\asymp_{\eps}$, and there is no intended reference to whether, as $\eps$ varies in $(0,1)$, the asymptotic constants hold uniformly or not.} In evaluating integrals we denote $f(x)\vert_a^b\defeq f(b)-f(a)$. \textit{Absolutely continuous} distributions will be referred to as \textit{continuous} for brevity. \textit{Discontinuous} distributions are assumed to satisfy the usual regularity assumption (isolated jumps). Convergence in distribution of random variables is denoted as weak convergence: $\overset{w}{\longrightarrow}$. \textit{Almost surely} is abbreviated $\as$ When random variables $H$, $G$ are identically distributed, it is denoted by $H\sim G$. The essential supremum of a collection of random variables is denoted with the symbol $\esup$. $S(h)\defeq \PP(H\ge h)$ is taken as the \textit{survival function} of $H$ (when necessary the notation $S_H(h)$ will be used), while we denote $\overline{F}(h)\defeq S(h+1)=\PP(H> h)$. We follow the standard probabilistic functional notation for powers: for example $\PP^2(\cdot)\defeq\left[\PP(\cdot)\right]^2$ and $\EE^2(\cdot)\defeq\left[\EE(\cdot)\right]^2$.

    \subsection{Probabilistic model}
	As aforementioned in this section (and in this section alone) RH is studied from the point of view of optimal stopping, hence horizons admitting mass at zero will be also considered.\footnote{This has been shown to be preferable\cite{Sam96}.} In \Cref{model} we ruled out mass at zero, since in all later sections, concerned with competitive analysis, this will be the standard assumption. A formal convention in fact restores the equivalence with a scenario admitting mass at zero, from a competitive analysis point of view. Add a value $X_0\defeq 0$, and set $X_\tau\defeq X_0\eqdef M_{[0]}$ for all $\omega\in\{H=0\}$. Informally, when the game does not start, both the gambler and the prophet obtain no return. Adopt the convention $\sfrac{0}{0}=1$. Having set $X_0=0$ ensures that the game always starts, unless $\omega\in\{H=0\}$. Assuming absence of penalty for entering the game even though it does not start yields the equivalence: the ratio being $1$ does not affect the worst case competitive ratio. 

	In RH the gambler must select the value in ignorance of whether it is the last or not: at each step $i\in\NN_0$ the gambler only learns whether the inspected value $x_i$ is the last or not (that is, whether $\omega\in\{H=i\}$ or $\omega\in\{H>i\}$) \textit{after} the decision of accepting or rejecting it has been made (that is in the $i+1$st step). Probabilistically, it is standard to model this as follows. The random list $X_1,\ldots,X_H$ is constructed from the infinite underlying process $\XX\defeq\{X_i\}$ of \textit{iid} copies of $X$, whose natural filtration (intuitively, the information associated with the history of the process up to the present) is the sequence of $\sigma(X_1,\ldots,X_i)\defeq\DF_i$, for every $i\in\NN_0$, where $\DF_0\defeq\{\emptyset,\Omega\}$ (no information at the start: this is the implicit starting point of all filtration we will introduce). Let $\nu\defeq\EE X<\infty$. The nonanticipative condition and the ignorance aforementioned are modelled by requiring that $\{\tau=i\}\in\sigma(\II_{\{H=0\}},X_1,\ldots,\II_{\{H=i-1\}},X_i)\eqdef \FF_i$ for every $i\in\NN_0$. Intuitively, this second filtration represents the information associated with the history of the game, which combines both the history of the process of the values, and the state of the game in all preceding steps (that is, whether it ended or it is still running). We recover the original $X_1,\ldots,X_H$ by imposing that the reward is zero if the gambler fails to stop by time $H$ on the infinite sequence, that is defining the reward sequence $\{Y_i\}$, where $Y_i\defeq X_i\II_{\{H\ge i\}}$, to which we add $Y_0\defeq X_0$.
	
	For technical reasons, we consider the slightly less common class of all possibly infinite stopping times for the problem $\XX$($\YY$), denoted as $\TT$($\TT^*$). Formally, $\tau\in\TT(\TT^*)$, means that $\{\tau=i\}\in\DF_i(\FF_i)$ for all $i\in\NN$, admitting also $\PP(\tau=\infty)> 0$. This requires that we \textit{compactify} the problem, by prescribing a conventional, yet natural, reward value, in the event that the gambler never stops. We set $Y_\infty\defeq \limsup_{i\longrightarrow\infty} Y_i=0$, since $\PP(H<\infty)=1$, so if the gambler never stops, $\as$ the horizon will have been reached, so there can be no reward. This way infinite stopping rules do not increase the largest possible expected reward and the equivalence with the original formulation is maintained. In conclusion, RH has been reformulated as an infinite optimal stopping problem with reward sequence $\YY\defeq\{Y_i\}_{i\in\NNN_0}$, underlying process $\XX$ and filtration $\FF\defeq \{\FF_i\}_{i\in\NNN_0}$, where $\FF_\infty\defeq \sigma\left(\bigcup_{i\in\NN_0} \FF_i\right)$ (the full information about the history of the process, which is the ending point of all infinite filtration we will introduce), with respect to the class $\TT^*$. We seek to find the value of the problem $\Val(\YY)\defeq\sup_{\tau\in\TT^*}\EE Y_\tau$ and, if it exists, an optimal stopping rule $\bar{\tau}\in\TT^*$. \textit{Optimal} means that $\EE Y_{\bar{\tau}} = \Val(\YY)$. The reference to the horizon $H$ can be made explicit, whenever the necessity arises, via the notation $\YY^\ssup{H}\defeq\{Y_i^\ssup{H}\}_{i\in\NNN_0}$.
 
	\subsection{Optimal algorithm}
	Under suitable hypotheses, an optimal stopping problem with independent horizon $H$, having survival function $S(i)$, is equivalent to a finite (if $H$ is bounded) or infinite (if $H$ is unbounded) optimal stopping problem \textit{discounted} by factors $S(i)$. The discounted problem is $\DZ\defeq\{Z_i\}_{i\in\NNN_0}$, where $Z_0\defeq Y_0$, $Z_i\defeq S(i)X_i$ for every $i\in\NN$, $Z_\infty\defeq\limsup_{i\longrightarrow\infty} Z_i=0=Y_\infty$. That is, we change only the reward function, not the underlying process. Thus, for $\DZ$ we use the filtration $\DF\defeq\{\DF_i\}$, having set $\DF_0$ and $\DF_\infty$ as usual. The latter is used when the problem is infinite. Thus $\Val(\DZ)\defeq\sup_{\zeta\in\TT}\EE Z_\zeta$, and the equivalence aforementioned means that $\Val(\YY)=\Val(\DZ)$. 

	In \cite{Sam96} the case where the horizon $H$ is bounded (finite support $\supp(H)$) is solved. Let $m\defeq\sup\: \supp(H)<\infty$, and take the discounted problem $Z_0, Z_1, \ldots, Z_m, 0,\ldots$ denoted $\DZ^\ssup{m}$. Then \textit{backward induction}, starting at $m$, yields the optimal stopping rule $\bar{\zeta}\in\TT^m$ attaining $\Val(\DZ)$, where $\TT^m$ is the class of all all stopping rules in $\TT$ that stop no later than time $m$. The stopping rule $\bar{\tau}\defeq \bar{\zeta}\wedge (H+1)$ then is shown to yield $\Val(\YY)$. The setting considered in \cite{Sam96} does not allow, in general, for the existence of an optimal stopping rule for unbounded horizons, which require an infinite $\DZ$. Only the equivalence with the discounted problem holds in general. The unbounded case is crucial for RH, as an optimal stopping rule is needed explicitly. The main contribution of our \Cref{optalg} is showing that RH with unbounded horizon $H$ admits an optimal stopping rule of the form $\bar{\tau}\defeq \bar{\zeta}\wedge (H+1)$ as well, where we will take $\bar{\zeta}$ to be the \textit{Snell stopping rule} for $\DZ$, since for an infinite problem, backward induction is not possible. Let us recall a few facts, adapted from \cite{ChoRob63,ChoRobSieg71} to our compactified framework. Let $\TT_i$ be the class of all all stopping rules in $\TT$ that do not stop before step $i$.
	\begin{definition}[Snell envelope]
		The \emph{Snell envelope} of $\DZ$ is $\{V_i\}$, where $V_i\defeq\esup_{\zeta\in\TT_i}\EE_{\DF_i}Z_\zeta$.
	\end{definition}
	Note that $\Val(\DZ)=V_0$. The intuitive interpretation of the stochastic process $\VV=\{V_n\}$ is the best expected return available at step $i$ among rules that have reached step $i$.
	\begin{remark}\label{snell}
		Since $\EE \sup_{i\in\NN} Z_i<\infty$ for every $i\in\NN_0$ the Snell envelope satisfies 
		\begin{equation}\label{dpe}
			V_i=Z_i\vee\EE_{\DF_i}V_{i+1}.
		\end{equation}
	\end{remark}
	Informally, \Cref{dpe} represents an extension of the dynamic programming principle for the infinite process $\{V_i\}$: at time $i$ it is optimal to stop whenever the currently inspected value $Z_i$ is greater than the expected highest expected future return as per the previous informal characterisation of $V_i$. We use \Cref{dpe} to compute $V_0$. Note that for finite problems, it coincides with backward induction.
	\begin{definition}[Snell stopping rule]\label{snellstoprule}
		The \emph{Snell rule} is $\bar{\zeta}\defeq\inf\{i\in\NN_0:\,Z_i\ge \EE_{\DF_i}V_{i+1}\}$.
	\end{definition}
	\begin{remark}\label{snellstop}
		If $\EE \sup_{i\in\NN} Z_i<\infty$ and $Z_\infty\defeq \limsup_{i\longrightarrow\infty} Z_i\in\RR$, then the \emph{Snell rule} $\bar{\zeta}$ is optimal.
	\end{remark}
	We are now ready to prove the main result. The details of the proof are provided in \Cref{suppopt}.
	\begin{theorem}\label{optalg}
		Let $H$ be a finite horizon with $\mu\defeq\EE H$ and possibly mass at zero, and let $m\defeq\sup \supp(H)$, $\YY$ and $\DZ$ as previously defined. It holds that:
		\begin{itemize}[noitemsep]
			\item$\Val(\YY) = \Val(\DZ)$; 
			\item there exists $\bar{\zeta}\in\TT$ such that $\EE Z_{\bar{\zeta}} = \Val (\DZ)$;
			\item $\bar{\tau}\defeq \bar{\zeta}\wedge (H+1)$ is such that $\EE Y_{\bar{\tau}} = \Val(\YY)$.
		\end{itemize}
		Furthermore, $\bar{\zeta}$ is characterised as:
		\begin{itemize}[noitemsep]
			\item The \emph{backward induction stopping rule} for $\DZ^\ssup{m}$, if $m<\infty$;
			\item The \emph{Snell rule} for $\DZ$, if $m=\infty$.
		\end{itemize}
	\end{theorem}
        	\begin{proof}\textcolor{red}{TOPROVE 0}\end{proof}
	
	The following result gives the intuitive interpretation of $\EE_{\DF_i}V_{i+1}$ as the best expected return available at step $i$ among rules that will not stop \textit{by or at} step $i$. We believe that it is known within the optimal stopping community, but we could not find a reference nor a proof. The proof is provided in \Cref{suppopt} for ease of the reader. As usual, assume that the process $\DZ$, adapted to a filtration $\DF$, satisfies $\EE\sup_{i\in\NN_0}Z_i<\infty$ and $\limsup_{i\longrightarrow\infty}Z_i=Z_\infty=0=Z_0$.
	\begin{lemma}\label{snellfuture}
		Let $\VV$ be the Snell envelope of $\DZ$. Then
		$\EE_{\DF_i}V_{i+1}=\esup_{\zeta\in\TT_{i+1}}\EE_{\DF_i}Z_\zeta \as$
	\end{lemma}
	
	\section{Single-threshold $2$-approximation on the $\PGF$ class}\label{Gclass}
	We build familiarity with the model by proving \Cref{Ghorizon}, which derives a $2$-approximation on the $\PGF$ class, extending the results of \cite[Section~3]{AliBanGolMunWan20}. Recall that $X\sim\DD$ is the distribution of the values and $H\sim\HH$ of the horizon (with no mass at zero). The expected return of a single-threshold algorithm is readily computed. The details are included in \Cref{suppGclassalgp} for ease of the reader.
	\begin{lemma}\label{algp}
		Let $\pi>0$ and consider the stopping rule $\tau_\pi\defeq\inf\{i\in\NN:\,Y_i\ge \pi\}\in\TT^*$, where $Y_i\defeq X_i\II_{\{H\ge i\}}$. Then $\EE X_{\tau_\pi}= c_\pi\EE(X|X\ge \pi)$, where $c_\pi=c_\pi(\HH,\DD)\defeq 1-\EE\left[\PP^H(X<\pi)\right]$.
	\end{lemma}
	
	The quantity $c_\pi$ yields the competitive ratio. With this in mind, we need to upper-bound $\EE M_H$. This is done by \textit{ex-ante} relaxation of the prophet, combined with a straightforward variational argument.\footnote{\Cref{algp} and a slightly incomplete, informal version of \Cref{max} leveraging mathematical programming already appeared in \cite[Theorem~3.2]{AliBanGolMunWan20}. The contribution is valuable and insightful. The alternative argument we provide however is not only simpler, but formalises the claim and fills some gaps, which will demand our attention later.} Further details and the complete proof of \Cref{max} are provided in \Cref{suppGclassmax}.
	\begin{lemma}\label{max}
		Let $X$ be continuous and $p>0$ such that $\PP(X\geq p)=\sfrac{1}{\mu}$. Then $\EE M_H\le\EE(X|X\ge p).$
	\end{lemma}
        \begin{proof}\textcolor{red}{TOPROVE 1}\end{proof}
	
	To prove \Cref{Ghorizon} we extend \Cref{max} to discontinuous $X$ via \textit{stochastic tie-breaking}. The technique is standard with deterministic horizons, but with random horizons it is not yet clear that the assumptions imposed on RH are robust enough. We will show that an underlying property of the prophet, Uniform Integrability (UI), is the key. First we derive the equivalent of \Cref{algp} for a randomised algorithm (randomisation does not increase the optimal value of optimal stopping problems). Attach to every $X_i$ a biased coin flip, that is \textit{iid} Bernoulli random variables $B_i\sim B\sim\Ber(q)$ independent of $X$ and $H$, $q$ denoting the probability of $1$ (heads).\footnote{This extended version of RH, can be treated through the equivalent process $Y_i\defeq X_iB_i\II_{\{H\ge i\}}$, whose history will generate the extended filtration $\BF$ defined via $\BF_i\defeq\sigma(\II_{\{H=0\}},X_1,B_1,\ldots,\II_{\{H=i-1\}},X_i,B_i)$. Formally, randomised stopping rules are those adapted to $\BF$.} We denote the class of adapted, and thus randomised, stopping rules as $\BT$.
	\begin{lemma}\label{algran}
		Let $\pi>0$ and consider the stopping rule $\tau_{\pi,q}\defeq\inf\{i\in\NN:\,Y_i\ge \pi\}\in\BT$.Then $\EE X_{\tau_{\pi,q}}= c_{\pi,q}\EE(X|X\ge \pi)$, where $c_{\pi,q}=c_{\pi,q}(\HH,\DD)\defeq 1-\EE\left\lbrace[1-q\PP(X\ge \pi)]^H\right\rbrace$.
	\end{lemma}
	Rewriting $c_\pi=1-\EE\left\lbrace [1-\PP(X\ge\pi)]^H\right\rbrace$ in \Cref{algp} helps seeing the statement of \Cref{algran} as its natural generalisation. The proof of \Cref{algran} is combinatorially tedious, but straightforward, and is provided in \Cref{suppGclassalgran}. We are now ready to show the main result of this section, \Cref{Ghorizon}. The details of the proof are provided in \Cref{suppGclassGhorizon}.
    \begin{proof}\textcolor{red}{TOPROVE 2}\end{proof}
	
	\begin{remark}\label{algorithm}
		For every horizon $H\in\PGF$ the following hold.
		\begin{itemize}[noitemsep]
			\item If $X$ is continuous, the $2-\sfrac{1}{\mu}$-approximation is a single-threshold algorithm, where the threshold is the value $p$ such that $\PP(X\ge p)=\sfrac{1}{\mu}$.
			\item If $X$ is discontinuous with $\sfrac{1}{\mu}$ in correspondence of a discontinuity jump of the survival function of $X$, the $2-\sfrac{1}{\mu}$-approximation is a single-threshold randomised algorithm, where the threshold is the value $p$ at which the discontinuity jump occurs, and the randomisation parameter is $[\mu\PP(X\ge p)]^{-1}$. 
			\item As a result if $X$ is discontinuous but $\sfrac{1}{\mu}$ does not occur in correspondence of a discontinuity jump, the randomisation parameter is $1$, meaning that the nonrandomised algorithm for the continuous case is enough to yield a $2-\sfrac{1}{\mu}$-approximation.
		\end{itemize}
	\end{remark}
	Some of the most well-known subclasses of the $\PGF$ class, which earned considerable interest in applications, are: $\IHRE$ (Increasing Hazard Rate in Expectation); $\HIHRE$ (Harmonically Increasing Hazard Rate in Expectation); $\NBU$ (New Better than Used); $\NBUE$ (New Better than Used in Expectation). The following tower of inclusions is well-established \cite{BraqRoyXie01,Rol75,Klef82} and known to be strict: $\IHR\subset\IHRE\subset\HIHRE\subset\NBU\subset\NBUE\subset\HNBUE\subset\PGF$. Further details on these subclasses are given in \Cref{suppGclassdetails}.
		
  	The tight instance for the $2-\sfrac{1}{\mu}$-approximation involves a geometric horizon, and was studied in \cite[Theorem~3.5]{AliBanGolMunWan20}. Their basic assumptions, concerning a two-point value distribution, are intuitive facts: that an optimal algorithm with geometric horizon should exist, and that it has a single-threshold. We show mathematically that they both hold, with the second extending, more in general, to any bounded value distribution. As a bonus we obtain an equation for the optimal threshold. The details of the proof of \Cref{geomprice} are provided in \Cref{suppGclassgeomprice}. Recall that $\Geom(1-q)$ denotes the geometric distribution with failure probability $0<q<1$.
	\begin{lemma}\label{geomprice}
		Let $H\sim\Geom(1-q)$, and $X$ such that $\supp(X)=[a,b]$, $0\le a\le b$. Then there exists a threshold $V_0$ such that the corresponding single-threshold algorithm is optimal.
	\end{lemma}
	\begin{proof}\textcolor{red}{TOPROVE 3}\end{proof}
	The tightness then follows by the straightforward computation of \cite{AliBanGolMunWan20}[Theorem~3.5]. For self-containedness we provide the short computation in \Cref{suppGclasstight}, with small variations to better fit our framework.
	
	\section{A first step beyond single-threshold algorithms}\label{hardsecretary}
	In this section we construct a parametric family of horizons, which is hard for any single-threshold algorithm, and yet it allows for an adaptive competitive stopping rule. We also show that it can be perturbed so that it enters the $\PGFd$ class, while remaining hard for single-threshold algorithms.
	
	\subsection{The hard instance}
	In this section we construct the instance, which will provide the hardness result for single-thresholds.
	Let us start with the instance $X$ for the values, which is a Pareto (Type I) distribution with scale parameter $1$ and shape parameter $1+\eps$, where $\eps>0$ is fixed small enough. The corresponding cdf, denoted as $V(x)$, vanishes on $(-\infty,1)$, whereas on $[1,\infty)$
	\begin{equation}\label{hardval}
		V(x)= 1-x^{-(1+\eps)}.
	\end{equation}
	Since $M_n$ has cdf $F_n(x)=V^n(x)$, which is differentiable for all $x\neq 1$, we obtain that its density (defined almost everywhere) $f_n(x)$ vanishes on $(-\infty,1)$, whereas $f_n(x)=nV^{n-1}(x)V'(x)$ on $(1,\infty)$. By direct computation of $\EE M_n \defeq \int_\RR xf_n(x)dx$, exploiting the properties of the Beta function and well-known asymptotics of the Gamma function, the following result holds. The proof is provided in \Cref{supphardsecretarylem}.
	\begin{lemma}\label{asympmax}
		As $n\longrightarrow\infty$, uniformly in $\eps>0$,
		$\EE M_n \sim \Gamma\left(1-\frac{1}{1+\eps}\right)n^{\frac{1}{1+\eps}}$.
	\end{lemma}
In conjunction with the instance $X$ for the values, consider a family of horizons $\{H_m\}$, with $m\in\NN$ large enough, such that for every fixed such $m$, $\supp(H_m)=[\ell,m]\cap\NN$, with the lower limit $1<\ell\in\NN$. For every $m$ fixed, we define the pmf of each horizon by letting, for every $\ell\le h\le m$,
	\begin{equation}\label{hardhorizon}
		p_h^\ssup{m}\defeq \PP(H_m=h)=Z_m^{-1}h^{-\frac{1}{1+2\eps}},
	\end{equation}
	having defined the normalising constant $Z_m\defeq\sum_{h=\ell}^m h^{-\frac{1}{1+2\eps}}$. Through standard integral estimates of summations, it is shown that 
	\begin{equation}\label{asympnorm}
		Z_m\sim [1+(2\eps)^{-1}]m^{\frac{2\eps}{1+2\eps}},
	\end{equation} 
	and that the following holds.
	\begin{lemma}\label{asympmax_H_m}
		As $m\longrightarrow\infty$, uniformly in $\eps>0$ small enough, $\EE M_{H_m}\sim N m^{\frac{1}{1+\eps}}$, where we defined
		$N=N(\eps)\defeq\frac{\Gamma\left(1-\frac{1}{1+\eps}\right)}{\left[1+\frac{\eps}{(1+\eps)(1+2\eps)}\right][1+(2\eps)^{-1}]}$.
	\end{lemma}
	The proof of \Cref{asympnorm} can be found in the proof of \Cref{asympmax_H_m}, which is provided in \Cref{supphardsecretarylem}. By \Cref{asympnorm} and similar integral estimates as in \Cref{asympmax_H_m} it is straightforward to compute that for every $n\in\NN$, as $m\longrightarrow\infty$,
	\begin{equation}\label{expectationn}
		\EE H_m^n\sim\frac{\sum_{h=\ell}^mh^{n+1-\frac{1}{1+2\eps}}}{[1+(2\eps)^{-1}]m^{\frac{2\eps}{1+2\eps}}}\sim\frac{m^{n-\frac{1}{1+2\eps}}}{\left(n+1-\frac{1}{1+2\eps}\right)[1+(2\eps)^{-1}]m^{\frac{2\eps}{1+2\eps}}}= \frac{2\eps m^n}{n+2(n+1)\eps}.
	\end{equation}
Note that from the elementary properties of the Gamma function it holds that as $x\longrightarrow 0^+$, $\Gamma(x)\sim x^{-1}$. As a result, as $\eps$ vanishes, $N\sim2\eps\left(1-(1+\eps)^{-1}\right)^{-1}\longrightarrow 2$ and the following is immediate consequence.
	\begin{remark}\label{asympmaxunif_H_m}
		As $m\longrightarrow\infty$, uniformly in $\eps>0$ small enough, $\EE M_{H_m}\asymp m^{\frac{1}{1+\eps}}$.
	\end{remark}
	
	We are now ready to prove the hardness of the family of horizons $\{H_m\}$ for single-threshold algorithms. A stopping rule with single-threshold $\pi$ is denoted as $\tau_\pi$. A crucial point is that $\pi$ can be tailored to each horizon in the family, that is, the algorithm facing $H_m$ will set a threshold $\pi_m$ exploiting all distributional knowledge regarding $X$ and $H_m$. Yet, this is not enough to obtain a constant-approximation with the family $(X,\{H_m\})$ for suitably small $\eps$. The details of the proof, which involves a quite technical asymptotic analysis, is provided in \Cref{supphardsecretaryprop}. We also include some supporting numerics and heuristic consideration in \Cref{supphardsecretaryheur}, which ease assessing that the family of horizons in question has divergent CV as $\eps$ vanish (thus violating the requirement of \Cref{L2horizon}), and admits horizons outside the $\PGF$ and $\PGFd$ class (thus violating the requirements of \Cref{Ghorizon} and \Cref{Gdhorizon}).
	\begin{proposition}\label{hardsingle}
		For fixed $\eps>0$ small enough, given the instance of copies of $X$ and the family of horizons $\{H_m\}$ of \Cref{hardval,hardhorizon} respectively, we have that for every sequence of thresholds $\{\pi_m\}$,  $\liminf_{m\longrightarrow\infty}\frac{\EE X_{\tau_{\pi_m}}}{\EE M_{H_m}}=0$.
	\end{proposition}

    \begin{proof}\textcolor{red}{TOPROVE 4}\end{proof}
	
	\subsection{Hardness of the $\PGFd$ class}\label{Gdclass}
	In principle, there is no obvious reason why the main result of this section, \Cref{Gdhorizon}, should hold, merely based on duality. However, a hint towards the hardness of the $\PGFd$ class could be that the duality turns the estimates of \Cref{Gclass} the wrong way around. It is straightforward to see that the approach adopted on the $\PGF$ class is mirrored by the duality, meaning that the lower-bound, which provides the competitive ratio ensured, turns into an upper-bound, informally speaking. This mirroring has significance, because the ex-ante relaxation of the prophet is tight on geometric horizons, and is essentially maxed-out by the very definition of the $\PGF$ class. A similar phenomenon occurs with the argument of \cite{AliBanGolMunWan20}, if we restrict to the $\IHR$ and $\DHR$ class. The most notable subclasses of the $\PGFd$ class are the dual of those introduced at the end of the previous section: $\DHR\subset\DHRE\subset\HDHRE\subset\NWU\subset\NWUE\subset\HNWUE\subset\PGFd$.\footnote{Since these classes are obtained by reversing the inequalities in the corresponding definitions, the corresponding acronyms are obtained by replacing \textit{increasing} with \textit{decreasing} and \textit{better} with \textit{worse}.} These facts, combined with \Cref{Gdhorizon}, motivate our conjecture that no constant-approximation is possible on the $\DHR$ class either. The details of the following proof are in \Cref{supphardsecretaryGdhorizon}.
	
	\begin{proof}\textcolor{red}{TOPROVE 5}\end{proof}
	
	\subsection{The competitive Secretary Problem rule}
	In this section we show that a straightforward adaptation of the SP stopping rule with deterministic horizon $m$ is competitive on the horizons $H_m$ defined in \Cref{hardhorizon}. Consider any instance of the values $X$ which is nonnegative, integrable and continuous random variable.\footnote{The hypothesis of absolute continuity is to ensure that there are no ties in the ranking process involved in the SP rule. It is possible to also allow discontinuous random variables, by exploiting the following tie-breaking procedure: consider $(U_1,\ldots,U_m)$, where $U_i$ are \textit{iid} copies of $U\sim\Unif(0,1)$. Then define $\XX=(X_1,U_1,\ldots,X_m,U_m)$, and instead of the usual order relationship that gives rise to the usual ranking for SP, adopt the following ranking order: if $X_i<X_j$, $(X_i,U_i)\preccurlyeq(X_j,U_j)$, whereas if $X_i=X_j$ and $U_i<U_j$, then by breaking ties thanks to the uniform random variables we say that, again, $(X_i,U_i)\preccurlyeq(X_j,U_j)$. Since the uniform random variable are continuous, there are almost surely no ties by ranking via $\preccurlyeq$ instead of $<$. For simplicity, we present the argument only for the continuous case and by using the standard $\le$ as a ranking order.} Consider the process $\XX\defeq(X_1,\ldots,X_m)$, where $X_i$ are \textit{iid} copies of $X$. We equivalently characterise it in the context of SP via a uniform random permutation $\pi:\Omega\longrightarrow S_m$, where $S_m$ is the set of all permutations of $[m]$ and $\Omega$ the supporting probability space. In online notation, $\pi=(\pi_1,\ldots,\pi_m)$. Consider $\XX^\pi\defeq(X_{\pi_1},\ldots,X_{\pi_m})$. Since $\XX$ is exchangeable, $\XX^\pi\sim\XX$. Consider now the waiting time $r_m-1$ for the SP stopping rule, denoted as $\zeta_{r_m}$, and recall that
	\begin{equation}\label{waitingtime}
		r_m\sim \frac{m}{e}.
	\end{equation}
	Consider $\XX^\pi$ conditionally on (the $\sigma$-algebra generated by) $\XX$, denoted as $(\XX^\pi|\XX)$. This is the process of realised values arriving in uniform random order, and constitutes therefore the usual setup for SP. In this framework, the classic result of \cite{Lind61} can be restated as 
	\begin{equation}\label{1/e}
		\PP\left(X_{\pi_{\zeta_{r_m}}}=M_m|\XX\right)\ge \frac{1}{e}.
	\end{equation} 
	Given the rule $\zeta_{r_m}$, denote the event of winning (that is, choosing the best) as $W\defeq\{X_{\pi_{\zeta_{r_m}}}=M_m\}$ and the event of winning at time $i$ as $W_i\defeq \{i=\inf\{j\ge r_m:\:X_{\pi_j}=M_m\}\}$. Note that by partitioning $W$ conditionally on $\XX$, 
	we have that 
	\begin{equation}\label{W}
		\PP(W|\XX)=\sum_{i=r_m}^m \PP(W_i|\XX).
	\end{equation}
The proof of \Cref{1/e} relies on the following fact, which we will also exploit:
	\begin{equation}\label{W_i}
		\PP\left(i=\inf\{j\ge r_m:\:X_{\pi_j}=M_m\}|\XX\right)=\frac{r_m-1}{m}\frac{1}{i-1}.
	\end{equation}   
When considering RH, instead of working with $\XX$ as usual, we start from $\XX^\pi$, by defining accordingly $Y_i^\ssup{H_m}\defeq X_{\pi_i}\II_{\{H_m\ge i\}}$. This is equivalent, since for every stopping rule $\tau\in\TT^*$, $\EE X_\tau=\EE X_\tau^\pi=\EE Y_\tau^{\ssup{H_m}}$. Yet it will be formally advantageous in the following proof of the main result of the section. The details of the argument are provided in \Cref{supphardsecretaryfuture}.


	\begin{proof}\textcolor{red}{TOPROVE 6}\end{proof}
	
	\section{A $2$-approximation for concentrated $\mathcal{L}^2$-horizons}\label{L2}
	In this final section we prove \Cref{L2horizon}, thus extending the $2$-approximation for the $\PGF$ class to sufficiently concentrated horizons. Besides its practical value, this will reveal interesting connections with deterministic prophet inequalities. We will make use of the following stochastic order.
	\begin{definition}[Laplace transform order]
		Given nonnegative random variables $H$ and $G$, $G$ is dominated by $H$ in the \emph{Laplace transform (Lt) order}, denoted as $G\prec_{Lt} H$, if $\forall\:s>0$, $\EE e^{-sG}\ge\EE e^{-sH}$.
	\end{definition}
	\begin{remark}\label{bernoulli}
		Consider a horizon $H$, having finite expectation $\mu$ and variance $\sigma^2$, and random variables $B\sim\Ber\left(\frac{\mu^2}{\mu^2+\sigma^2}\right)$ and $G\defeq\frac{\mu^2+\sigma^2}{\mu}B$. Then $G\prec_{Lt}H$.
	\end{remark}
	We are now ready to prove \Cref{L2horizon}. The details of the proof are provided in \Cref{suppL2}.
	\begin{proof}\textcolor{red}{TOPROVE 7}\end{proof}
	On a practical note, the Lambert function is readily simulated in various languages.\footnote{For example in Python, with standard error bounds of $10^{-8}$, via scipy.special.lambertw. Higher precision alternatives also exist (for example through the bisection method).} Thus it is possible to reliably implement the substitution of suitable values of $C>1$ for $2-\sfrac{1}{\mu}$ in \Cref{concentration}, even without large-market hypothesis (the latter was used to heuristically exemplify this fact in \Cref{contributions}). The only accuracy issue would arise near the branching point of the Lambert function, that is, when $\bar{y}\approx-1$, which is an uninteresting range for RH, since it only concerns horizons with $2-\sfrac{1}{\mu}\approx 1$, that is $\mu\approx 1$, which approaches a degenerate case at which the gambler and the prophet perform equally, since there is only one item to allocate. With this caveat, we conclude by discussing how to derive \Cref{largemarket}. 
	
	As direct consequence of substituting \textit{admissible} values of $C>1$ for $2-\sfrac{1}{\mu}$ in \Cref{replaceC} we obtain the upper-bound \[\text{CV}\le\sqrt{1-\sfrac{1}{\mu}+W_0(-Ce^{-C})}=\sqrt{C-1+W_0(-Ce^{-C})},\] which ensures a $C$-approximation. This highlights interesting degenerate cases, and establishes an important connection with the existing literature. Note that the deterministic case ($\text{CV}=0$) can be trivially recovered directly from \Cref{deterministic}, which can be reformulated as a guarantee that the competitive ratio (given by $1/C$ here) is always greater than \[\frac{1-\left(1-\mu^{-1}\right)^{\mu(1+\text{CV}^2)}}{1+\text{CV}^2}\ge\frac{1-e^{-(1+\text{CV}^2)}}{1+\text{CV}^2}.\]
	Therefore if $\text{CV}=0$, the optimal single-threshold algorithm exploited ensures a competitive ratio of $1-\sfrac{1}{e}$. This value is the global maximum of the function on the right-hand side, and coincides with the optimal competitive ratio for single-threshold algorithms for (deterministic) IID PI \cite{Esh18}[Theorem~21]. Note that the left-hand side tends to the right-hand side in the large-market limit, at which regime the admissible values of $C$ aforementioned are easily computed to be constrained to be larger than $e/(e-1)$, and therefore a competitive ratio of $1-\sfrac{1}{e}$ cannot be exceeded even in a best-case analysis (with respect to the horizon). More in general, dropping the large-market limit, the left-hand side also achieves global maximum $1-\left(1-\sfrac{1}{\mu}\right)^{\mu}$ at $\text{CV}=0$ (this is not surprising, since the deterministic horizon is no harder than the random horizon for the gambler). Hence $C$ will always be constrained to be greater than $[1-\left(1-\sfrac{1}{\mu}\right)^{\mu}]^{-1}$. This proves \Cref{largemarket} and justifies the admissibility condition.

    \section{Conclusion}
    The contributions of this work can be divided into two main groups. The first group of results (in \Cref{Gclass,L2}) extend the distributional classes to which single-threshold algorithms can be applied, which impacts areas such as pricing and mechanism design of online auctions for the following reasons. The classes of distributions in $\PGF$ are well established across several areas, in particular in reliability theory. Furthermore, horizons classified in terms of concentration measures such as the $\text{CV}$ can be of wide use in applications, since their classification only requires the first two moments. Single-threshold algorithms are simple to implement, and thus proving their competitiveness and optimality for as many distributional classes as possible is advantageous. Although our main focus is on results for single item RH, they can be generalised to any number of items via the mechanism designed in \cite{AliBanGolMunWan20}. Furthermore, it is likely that similar results related to optimal stopping problems with $\IHR$ random horizons may benefit from a similar extension to the $\PGF$ class. The key to this extension is clear from the fact that our proof, in essence, relies only on certain stochastic ordering properties of the $\IHR$ class, which is popular not only in reliability theory, but in economics and computer science. RH has several connections to interesting online stochastic matching problems \cite{AouMa23}. A potential implementation of the techniques we established in this work would be extending the use of stochastic orders to these models and see if they improve known guarantees.
        
    The second group of results (in \Cref{opt,hardsecretary}) provide a more theoretical contribution. On one end, extending the characterisation of the optimal algorithm to unbounded horizons is of help in formalising hardness results. On the other hand, it has also helped significantly in showing that there are instances which are hard for single-thresholds, but for which a multiple-thresholds algorithm such as the SP rule is competitive. This last fact, combined with the fact that single-threshold algorithms already fail on the $\PGFd$ class, suggests that the study of RH under (most likely adaptive) multiple-threshold algorithms is a new and promising area of research.
	\section*{Acknowledgements.}This research was partially supported by the EPSRC grants EP/W005573/1 and EP/X021696/1, the ERC CoG 863818 (ForM-SMArt) grant, the ANID Chile grant ACT210005 and the French Agence Nationale de la Recherche (ANR) under reference ANR-21-CE40-0020 (CONVERGENCE project). We would like to thank Jos\'{e} Correa for his precious advice, Bruno Ziliotto and Vasilis Livanos for early conversations.    
\bibliographystyle{abbrv}\bibliography{RandomHorizon}
	\clearpage
        \appendix
        \renewcommand{\appendixpagename}{Supplementary materials}
	\appendixpage
        \section{Proofs omitted from Section \ref{opt}}\label{suppopt}
        
        \subsection{Proof of \Cref{optalg}}
        \begin{proof}\textcolor{red}{TOPROVE 8}\end{proof}
	
	\subsection{Proof of \Cref{snellfuture}}
	\begin{proof}\textcolor{red}{TOPROVE 9}\end{proof}
 
	\section{Proofs omitted from Section \ref{Gclass}}\label{suppGclass}
	
	\subsection{Proof of \Cref{algp}}\label{suppGclassalgp}
	\begin{proof}\textcolor{red}{TOPROVE 10}\end{proof}
	
	\subsection{Proof of \Cref{max} and discussion on amendments to \cite{AliBanGolMunWan20}}\label{suppGclassmax}
	Before giving the details of the proof of \Cref{max}, we further discuss the need for a variational argument to upper-bound $\EE M_H$ for any continuous $X$. The key idea at the base of this argument is similar to that of \cite[Theorem~3.2]{AliBanGolMunWan20}, but they do not state what restrictions their argument imposes on $X$. As a matter of fact, they achieve the upper bound only for $X$ with discrete\footnote{For the value distribution, unlike for horizons, we use \textit{discrete} in the standard, more general, sense.} distribution and admitting a value $p$ such that $\PP(X\geq p)=\sfrac{1}{\mu}$, where $\mu=\EE H$. If their claim was true as stated, the resulting algorithm would not have a consistently defined threshold for more general distributions. It is not clear how these results should be extended from this special class to a more general one, nor it is mentioned by the authors. Perhaps, they have in mind discretised continuous distributions. Yet, this would mean that by some approximation or bounding argument, their result only applies to continuous distributions. We could not find a straightforward way, which might suggest this type of argument as trivial. Examples from the literature on other stochastic optimisation problems, such as the bilateral trade problem, adopt a similar approach, of discretising and then bounding, but the proofs of the bounding step are given and usually nontrivial. See for example \cite{CaiWu23}.
    \begin{proof}\textcolor{red}{TOPROVE 11}\end{proof}

	\subsection{Proof of \Cref{algran}}\label{suppGclassalgran}
	\begin{proof}\textcolor{red}{TOPROVE 12}\end{proof}
	
	\subsection{Proof of \Cref{Ghorizon}}\label{suppGclassGhorizon}
	\begin{proof}\textcolor{red}{TOPROVE 13}\end{proof}
	
	\subsection{Further details on the $\PGF$ class}\label{suppGclassdetails}
    For a better intuition of how much more general the somewhat abstract $\PGF$ class is, compared with the $\IHR$ class, we mention some of its most well-known subclasses, which earned considerable interest in applications. $H$ always denotes a horizon.
	\begin{definition}
		$H$ is \emph{New Better than Used ($\NBU$)} if for every $h,k\ge2$, $S(h+k)\le S(h)S(k)$.
	\end{definition}
	\begin{definition}
		For every $h\in\NN$ the \emph{mean residual life} of $H$ is defined as
		\[m(h)\defeq\begin{cases}
			\EE(H-h|H\ge h),&S(h)>0\\
			0,&S(h)=0.\end{cases}\]
	\end{definition}
	Note that $m(1)=\mu\defeq \EE H$.
	\begin{definition}\label{HNBUEdef}
		$H$ is \emph{Harmonically New Better than Used in Expectation ($\HNBUE$)} if for all $n\in\NN$, \[\frac{n}{\sum_{h=1}^n\frac{1}{m(h)}}\le \mu.\]
	\end{definition}
	Furthermore, consider all the other classes in the tower of inclusions introduced after \Cref{algorithm} in \Cref{Gclass}. For simplicity we omit introducing further definitions, as the denominations are suggestive enough to convey the intuitive meaning, and can be easily found in the literature (see, for example, \cite{BraqRoyXie01} for definitions and some of the inclusion properties we will recall). The $\HNBUE$ class, introduced in \cite{Rol75}, is roughly speaking the largest known, which is a subclass of the $\PGF$ class \cite{Klef82}, with intuitive meaning in terms of ageing properties.


	\subsection{Proof of \Cref{geomprice}}\label{suppGclassgeomprice}
        \begin{proof}\textcolor{red}{TOPROVE 14}\end{proof}
	
	\subsection{Tightness of $2$-approximations}\label{suppGclasstight}
	The following argument appears in \cite[Theorem~3.5]{AliBanGolMunWan20}. We include it for ease of reference, and because of a slight difference in the more rigorous setup we established with \Cref{geomprice}.
        \begin{theorem}[{\cite{AliBanGolMunWan20}[Theorem~3.5]}]
		RH is $2-\sfrac{1}{\mu}$-hard.
	\end{theorem}
	\begin{proof}\textcolor{red}{TOPROVE 15}\end{proof}
 
        \section{Proofs omitted from Section \ref{hardsecretary}}\label{supphardsecretary}
        
        \subsection{Proof of \Cref{asympmax,asympmax_H_m}}\label{supphardsecretarylem}
        \begin{proof}\textcolor{red}{TOPROVE 16}\end{proof}
 
    \begin{proof}\textcolor{red}{TOPROVE 17}\end{proof}
	
	\subsection{Proof  of \Cref{hardsingle}}\label{supphardsecretaryprop}
    \begin{proof}\textcolor{red}{TOPROVE 18}\end{proof}
    
    \subsection{Heuristics for the horizons in the hard instance of \Cref{hardsingle}}\label{supphardsecretaryheur}
    We now comment heuristically regarding why single-threshold approximations should be expected to fail for this instance, perhaps even in higher capacity than what proven in \Cref{hardsingle} (which restricts the instance to the value distribution $X$, in order to show hardness). Note that by exploiting \Cref{expectationn} with $n=1,2$ we have that as $m\longrightarrow\infty$,
	\[\frac{\Var H_m}{\EE^2 H_m}=\frac{\EE H_m^2}{\EE^2 H_m}-1\longrightarrow\frac{(1+4\eps)^2}{4\eps(1+3\eps)}-1.\] Since the ratio in the limit diverges as $\eps$ vanishes, this provides an intuitive reason why the single-threshold $2$-approximation of \Cref{L2} should fail. In this sense, the hypothesis that $\eps$ is small is essential. It is straightforward to compute the derivative of the ratio in the limit and show that it is negative for any $\eps$. Thus this limit decreases as $\eps$ grows and tends to $\sfrac{1}{3}<0.594\approx1+W_0(-2e^{-2})$. This means that for $\eps$ large enough the single-threshold $2$-approximation of \Cref{L2} is still valid, as the distribution $H_m$ has enough concentration in this case. The horizon $H_m$ in \Cref{pgfzoom} instead has $\text{CV}\approx1.074$ due to $\eps$ being sufficiently small and $m$ large enough. In this case not only the single-threshold $2$-approximation of \Cref{L2} is not valid, but no analogous single-threshold $C$-approximation is possible, as $m\longrightarrow\infty$, since no matter how large $C$ is taken, since $1+W_0(-Ce^{-C})\longrightarrow 1<1.074$ from below, as $C\longrightarrow\infty$. Furthermore, plotting the pgf of $H_m$, for $\eps$ small and $m$ large, against the pgf of a geometric horizon with the same mean as in \Cref{pgfzoom}, shows that there is no ordering of the two (due to the crossing point of the graphs), and therefore this family of horizons is outside the $\PGF$ (and $PGFd$) class, which explains heuristically why the $2$-approximation of \Cref{Gclass} fails too.
	\begin{figure}\centering
		\includegraphics[scale=0.6]{pgfhardzoom.png}
		\caption{Zoom on $(0,0.6)$ of the pgf of $H_{100}$, with $\ell=2$, $\eps=0.001$ (red) against the pgf of a geometric horizon with the same mean (blue).}
		\label{pgfzoom}
	\end{figure}

	\subsection{Proof of \Cref{Gdhorizon}}\label{supphardsecretaryGdhorizon}
	\begin{proof}\textcolor{red}{TOPROVE 19}\end{proof}

	\subsection{Proof of \Cref{future}}\label{supphardsecretaryfuture}
    	\begin{proof}\textcolor{red}{TOPROVE 20}\end{proof}
	
	\section{Proofs omitted from Section \ref{L2}}\label{suppL2}
	\begin{proof}\textcolor{red}{TOPROVE 21}\end{proof}
\end{document}