

\documentclass[a4paper,UKenglish,cleveref, thm-restate]{lipics-v2021}


\hideLIPIcs  



\usepackage{mathtools}

\usepackage{tikz}
\usetikzlibrary{tikzmark}

\newtheorem{question}{Question}
\newenvironment{bfDescript}[1]{{\sffamily\normalsize\bfseries#1}}



\usepackage[disable]{todonotes}
\bibliographystyle{plainurl}

\newcommand\po[1]{\todo[inline,size=\scriptsize,backgroundcolor=pink]{#1 - \textbf{Pierre}}}
\newcommand\ac[1]{\todo[inline,size=\scriptsize,backgroundcolor=yellow]{#1 - \textbf{Antonio}}}
\newcommand\nl[1]{\todo[inline,size=\scriptsize,backgroundcolor=teal!40]{#1 - \textbf{Nathan}}}


\newcommand{\re}[1]{\xrightarrow{#1}}
\newcommand{\rer}[1]{\xleftrightarrow{#1}}
\newcommand{\nre}[1]{\not\xrightarrow{#1}}
\newcommand{\rp}[1]{\overset{#1}{\rightsquigarrow}}

\newcommand{\tand}{\text{ and }}
\newcommand{\tor}{\text{ or }}
\newcommand{\tin}{\text{ in }}
\newcommand{\tif}{\text{ if }}
\newcommand{\tow}{\text{ otherwise}}
\newcommand{\tfor}{\text{ for }}


\newcommand{\leb}{\overline{<}}

\newcommand{\cS}{\mathcal S}

\newcommand{\rk}{\mathrm{rk}}

\newcommand{\cadot}{\overset{\leftarrow}{\cdot}}

\newcommand{\ap}{\overset{\leftarrow}{+}}

\newcommand{\col}{\text{col}}

\newcommand{\emptyword}{\varepsilon}

\newcommand{\TL}{\text{TL}}
\newcommand{\TW}{\text{TW}}
\newcommand{\Parity}{\mathrm{Parity}}
\newcommand{\MaxParity}{\mathrm{MaxParity}}


\renewcommand{\epsilon}{\varepsilon}

\newcommand{\eps}{\varepsilon}

\newcommand{\loops}{\overset{\curvearrowright}{\bullet}}

\newcommand{\mininf}{\mathrm{mininf}}

\newcommand{\even}{\mathrm{even}}
\newcommand{\odd}{\mathrm{odd}}

\newcommand{\VE}{V_{\text{Eve}}}
\newcommand{\VA}{V_{\text{Adam}}}

\newcommand{\R}{\mathbb R}

\newcommand{\val}{\mathrm{val}}

\newcommand{\Rbar}{\overline{\R}}

\newcommand{\N}{\mathbb N}
\newcommand{\Z}{\mathbb Z}

\newcommand{\fin}[1]{W_{\mathrm{fin}}}

\newcommand{\MP}{\text{Mean-Payoff}}
\renewcommand{\mp}{\text{mp}}
\newcommand{\Bounded}{\text{Bounded}}

\newcommand{\normal}{\mathcal{N}\xspace}
\newcommand{\cobuchi}{\mathcal{F}\xspace}

\newcommand{\coBuchi}{\text{co-Büchi}}


\newcommand{\reb}[1]{\mathrel{\ooalign{\hfil$\vcenter{
    \hbox{$\scriptscriptstyle\bullet$}}$\hfil\cr$\xrightarrow{#1}$\cr}
  }}

\renewcommand{\ss}{\mathrm{s}}

\newcommand{\ENI}{\text{ENI}}
\newcommand{\END}{\text{END}}
\newcommand{\STI}{\mathrm{SumToInfinity}}
\newcommand{\Finite}{\text{Finite}}

\newcommand{\TB}[1]{\text{Tilted-Bounded}_{#1}}


\newcommand{\ident}[1]{\ensuremath{\texorpdfstring{\mathrm{#1}}{#1}}\xspace}

\newcommand{\lang}{\ident{L}}

\newcommand\sumpath[1]{{w}( #1 )}


\newcommand{\boldclass}[3]{\texorpdfstring{\ensuremath{\mathbf{#1}^{#2}_{#3}}}{Borel}}
\newcommand{\lightclass}[3]{\ensuremath{{#1}^{#2}_{#3}}}

\newcommand{\borel}{\ensuremath{\mathcal B}\xspace}

\newcommand{\bsigma}[1]{\boldclass{\Sigma}{0}{#1}}
\newcommand{\bpi}[1]{\boldclass{\Pi}{0}{#1}}
\newcommand{\bdelta}[1]{\boldclass{\Delta}{0}{#1}}

\newcommand{\asigma}[1]{\boldclass{\Sigma}{1}{#1}}
\newcommand{\api}[1]{\boldclass{\Pi}{1}{#1}}
\newcommand{\adelta}[1]{\boldclass{\Delta}{1}{#1}}

\newcommand{\esigma}[2]{\lightclass{\Sigma}{#1}{#2}}
\newcommand{\epi}[2]{\lightclass{\Pi}{1}{#1}}
\newcommand{\edelta}[2]{\lightclass{\Delta}{#1}{#2}}

\newcommand{\BCSigma}{\mathrm{BC}(\boldclass{\Sigma}{0}{2})}


\newcommand{\lex}{\text{lex}}

\newcommand{\A}{\mathcal{A}}

\newcommand{\casc}{\ltimes}
\newcommand{\supr}{\text{sup}}

\newcommand{\NP}{\textbf{NP}}

\newcommand{\Sigmaeps}{\Sigma\ {\cup} \left\{\eps\right\}}

\newcommand{\ph}{\_}

\newcommand{\B}{\mathcal B}
\newcommand{\U}{\mathcal U}
\newcommand{\BC}{\text{BC}}
\newcommand{\T}{\mathcal T}


\newcommand{\Ssucc}[2]{S_{ #1 }( #2 )}

\newcommand{\cof}{\text{cof}}

\newcommand{\Eve}{\text{Eve}}
\newcommand{\Adam}{\text{Adam}}

\renewcommand{\d}{[d]}

\newcommand{\done}{[d_1]}
\newcommand{\dtwo}{[d_2]^*}

\newcommand{\doneodd}{[d_1]_{\mathrm{odd}}}
\newcommand{\dtwoodd}{[d_2]^*_{\mathrm{odd}}}

\newcommand{\oo}{\omega}
\renewcommand{\SS}{\Sigma}

\newcommand{\out}{\text{out}}

\newcommand{\cleq}{\preccurlyeq}
\newcommand{\cgeq}{\succcurlyeq}


\newcommand{\infOften}{{\mathtt{Inf}}}
\newcommand{\finOften}{\mathtt{Fin}}
\newcommand{\noOcc}{\mathtt{No}}

\newcommand{\ceiltwo}[1]{\lceil #1 \rceil}

\newcommand{\indtau}[1]{\mathsf{ind}_\tau(#1)}
\newcommand{\indextau}[2]{\mathsf{ind}_{#1}(#2)}

\newcommand{\outtau}[2]{\mathsf{out}_\tau(#1,#2)}

\newcommand{\xmin}{x_{\mathrm{min}}}
\newcommand{\ymin}{y_{\mathrm{min}}}
\newcommand{\zmin}{z_{\mathrm{min}}}
\newcommand{\tmin}{t_{\mathrm{min}}}
\newcommand{\imin}{i_{\mathrm{min}}}


\newcommand{\xbreak}{x^{(0)}}

\newcommand{\powne}[1]{\mathcal P_{\neq \varnothing}(#1)}

 

\title{The memory of $\omega$-regular and $\mathrm{BC}(\Sigma_2^0)$ objectives} 


\author{Antonio Casares}{University of Warsaw, Poland \and \url{https://antonio-casares.github.io/}}{antoniocasares@mimuw.edu.pl}{https://orcid.org/0000-0002-6539-2020}{Supported by the Polish National Science Centre (NCN) grant ``Polynomial finite state computation'' (2022/46/A/ST6/00072).}


\author{Pierre Ohlmann}{CNRS, Laboratoire d'Informatique et des Systèmes (LIS), Marseille, France\and \url{https://pageperso.lis-lab.fr/pierre.ohlmann/}}{pierre.ohlmann@lis-lab.fr}{https://orcid.org/0000-0002-4685-5253}{}



\authorrunning{A. Casares and P. Ohlmann} 

\Copyright{Antonio Casares and Pierre Ohlmann} 

\ccsdesc[500]{Theory of computation~Logic and verification}

\keywords{Infinite duration games, memory, omega-regular} 

\category{} 

\relatedversion{} 





\acknowledgements{We thank Nathan Lhote for his participation in the scientific discussions which led to this paper. We also thank Pierre Vandenhove for pointing us to references concerning lifting results for memory.}

\nolinenumbers 







\begin{document}

\maketitle

\begin{abstract}
In the context of 2-player zero-sum infinite duration games played on (potentially infinite) graphs, we ask the following question: Given an objective $W$ in $\mathrm{BC}(\mathbf{\Sigma}_2^0)$, i.e. recognised by a potentially infinite deterministic parity automaton, what is its memory, meaning the smallest integer $k$ such that in any game won by Eve, she has a strategy with $\leq k$ states of memory.
We provide a class of deterministic parity automata that exactly recognise objectives with memory $\leq k$.
This leads to the following results:
\begin{itemize}
\item for $\omega$-regular objectives, the memory can be computed in NP;
\item given two objectives $W_1$ and $W_2$ in $\mathrm{BC}(\mathbf{\Sigma}_2^0)$ and assuming $W_1$ is prefix-independent, the memory of $W_1 \cup W_2$ is at most the product of the memories of $W_1$ and $W_2$.
\end{itemize}
Our results also apply to chromatic memory, the variant where strategies can update their memory state only depending on which colour is seen.
\end{abstract}


\section{Introduction}
\subsection*{Context: Strategy complexity in infinite duration games}

We study infinite duration games on graphs in which two players, called Eve and Adam, interact by  moving a token along the edges of a (potentially infinite) edge-coloured directed graph. Each vertex belongs to one player, who chooses where to move next during a play. This interaction goes on for an infinite duration, producing an infinite path in the graph. The winner is determined according to a language of infinite sequences of colours~$W$, called the objective of the game; Eve aims to produce a path coloured by a sequence in $W$, while Adam tries to prevent this.
This model is widespread for its use in verification and synthesis~\cite{HandbookModelChecking2018}.





In order to achieve their goal, players use strategies, which are representations of the course of all possible plays together with instructions on how to act in each scenario.
In this work, we are interested in optimal strategies for Eve, that is, strategies that guarantee a victory whenever this is possible. More precisely, we are interested in the complexity of such strategies, or in other words, in the succinctness of the representation of the space of plays.

\subparagraph*{Positionality.}
The simplest strategies are those that assign in advance an outgoing edge to each vertex owned by Eve, and always play along this edge, disregarding all the other features of the play.
All the information required to implement such a strategy appears in the game graph itself.
Objectives for which such strategies are sufficient to play optimally are called positional (or memoryless).
Understanding positionality has been the object of a long line of research. The landmark results of Gimbert and Zielonka~\cite{GZ05} and Colcombet and Niwinski~\cite{CN06} gave a good understanding of which objectives are bi-positional, i.e.~positional for both players.

More recently, Ohlmann proposed to use universal graphs as a tool for studying positionality (taking  Eve's point of view)~\cite{Ohlmann23}.
This led to many advances in the study of positionality~\cite{BCRV24HalfJournal,OS24Sigma2}, and most notably, a characterisation of positional $\omega$-regular objectives by Casares and Ohlmann~\cite{CO24Positional}, together with a polynomial time decision procedure (and some other important corollaries, more discussion below).

\subparagraph*{Strategies with memory.} 
However, in many scenarios, playing optimally requires distinguishing different plays that end in the same vertex.
A seminal result of B\"uchi and Landweber~\cite{BL69Strategies} states that in finite games where the objective is an $\oo$-regular language, the winner has a winning strategy that can be implemented by a finite automaton processing the edges of the game; this result was later extended to infinite game graphs by Gurevich and Harrington~\cite{Gurevich1982trees}.
Here, the states of the automaton are interpreted as memory states of the strategy, and a natural measure of the complexity of a strategy is the number of such states.
More precisely, the memory of an objective $W$ is the minimal $k$ such that whenever Eve wins a game with objective $W$, she has a winning strategy with $k$ states of memory.
For $\oo$-regular objectives, this is always finite~\cite{BL69Strategies,Gurevich1982trees}, while the case of positionality discussed above corresponds to memory $k=1$.

\subparagraph*{Chromatic versus general memory.}
In the special case where these automata are only allowed to read the colours on the edges of the game graph, we speak of chromatic memory.
In his PhD thesis, Kopczy\'nski showed that, for prefix-independent $\omega$-regular objectives and over finite game graphs, the chromatic memory can be computed in exponential time~\cite[Theorem~8.14]{Kop08Thesis}.
Recently, it was shown that computing the chromatic memory of some restricted subclasses of $\oo$-regular objectives is in fact $\NP$-complete: for Muller objectives~\cite{Casares22Chromatic} and for topologically open or closed objectives~\cite{BFRV23Regular}.


For the more natural model of not-necessarily chromatic memory (which we will simply call memory\footnote{In the literature, this is sometimes called general memory, or chaotic memory.}), results are sparser. Notable ones include the characterisation for memory of Muller objectives by Dziembowski, Jurdzi\'nski, and Walukiewicz~\cite{DJW1997memory}, or the memory of closed objectives~\cite{CFH14}.
However, these are all rather restricted classes of $\omega$-regular objectives.
Prior to this work, even computing the memory of open $\omega$-regular objectives (sometimes called regular reachability objectives) was not known to be decidable.




\subparagraph{Unions of objectives.} The driving question in Kopczyński's PhD thesis~\cite{Kop08Thesis} is whether prefix-independent positional objectives are closed under union, which has become known as Kopczyński's conjecture. Recently, Kozachinskiy~\cite{Kozachinskiy24EnergyGroups} disproved this conjecture, but only for positionality over finite game graphs, and using non-$\oo$-regular objectives. In fact, the conjecture is now known to hold for $\oo$-regular objectives~\cite{CO24Positional} and $\bsigma 2$ objectives~\cite{OS24Sigma2}. 
Casares and Ohlmann proposed a generalisation of this conjecture from positional objectives to objectives requiring memory~\cite[Conjecture~7.1]{CO25LMCS} (see also~\cite[Proposition~8.11]{Kop08Thesis}):

\begin{conjecture}[{Generalised Kopczyński's conjecture}]\label{conj:Kopcz-Union-Memory}
	Let $W_1,W_2\subseteq \SS^\oo$ be two prefix-independent objectives with memory $k_1$ and $k_2$, respectively. Then $W_1\cup W_2$ has memory at most $k_1 \cdot k_2$.
\end{conjecture}

Using the characterisation of~\cite{DJW1997memory}, it is not hard to verify that the conjecture holds for Muller objectives.

\subparagraph*{$\BCSigma$ languages.}

The results in this work apply not only to $\oo$-regular languages, but to the broader class of $\BCSigma$ languages.
These are boolean combinations of languages in $\bsigma 2$ (countable unions of closed languages), or equivalently, recognised by deterministic parity automata with infinitely many states.
This class includes typical non-$\omega$-regular examples such as energy or mean-payoff objectives, but also broader classes such as unambiguous $\oo$-petri nets~\cite{FSJLS22} and deterministic $\oo$-Turing machines (Turing machines with a Muller condition).


\subsection*{Contributions}

Our main contribution is a characterisation of $\BCSigma$ objectives with memory $\leq k$, stated in Theorem~\ref{thm:main-charac}.
It captures both the memory and the chromatic memory of objectives over infinite game graphs.
The characterisation is based on the notion of $k$-wise $\epsilon$-completable automata, which are parity automata with states partitioned in $k$ chains, where each chain is endowed with a tight hierarchical structure encoded in the $\eps$-transitions of the automaton.

From this characterisation, we derive the following corollaries:
\begin{enumerate}
	\item \bfDescript{Decidability in $\NP$.} Given a deterministic parity automaton $\A$, the memory (resp. chromatic memory) of $L(\A)$  can be computed in $\NP$.



	\item \bfDescript{Generalised Kopczyński's conjecture.} We establish (and strengthen) Conjecture~\ref{conj:Kopcz-Union-Memory} in the case of $\BCSigma$ objectives: if $W_1$ and $W_2$ are $\BCSigma$ objectives with memory $k_1$ and
	 $k_2$, and one of them is prefix-increasing, then the memory of $W_1 \cup W_2$ is $\leq k_1\cdot k_2$.\end{enumerate}


\subparagraph{Main technical tools: Universal graphs and $\eps$-completable automata.}
As mentioned above, Ohlmann proposed a characterisation of positionality by means of universal graphs~\cite{Ohlmann23}.
In 2023, Casares and Ohlmann extended this characterisation to objectives with memory $\leq k$ by considering partially ordered universal graphs~\cite{CO25LMCS}.
Until now, universal graphs have been mainly used to show that certain objectives have memory $\leq k$ (usually for $k=1$); this is done by constructing a universal graph for the objective.
One technical novelty of this work is to exploit both directions of the characterisation, as we also rely on the existence of universal graphs to obtain decidability results.

Our characterisation is based on the notion of $k$-wise $\eps$-completable automata, which extends the key notion of~\cite{CO24Positional} from positionality to finite memory.  

\subparagraph*{Comparison with~\cite{CO24Positional}.}
In 2024, Casares and Ohlmann characterised positional $\oo$-regular objectives~\cite{CO24Positional}, establishing decidability of positionality in polynomial time, and settling Kopczyński's conjecture for $\oo$-regular objectives.
Although the current paper generalises the notions and some of the results from~\cite{CO24Positional} to the case of memory, as well as potentially infinite automata, the proof techniques are significantly different: while~\cite{CO24Positional} is based on intricate successive transformations of parity automata, ours is based on an extraction method in the infinite and manipulates ordinal numbers.
Though somewhat less elementary, our proof is notably shorter, and probably easier to read.

When instantiated to the case of memory $1$, we thus extend Kopczyński's conjecture from $\omega$-regular objectives to positional $\BCSigma$ objectives.
This also gives an easier proof of decidability of positionality in polynomial time\footnote{It is explained in~\cite[Theorem~5.3]{CO24Positional} how decidability of positionality in polynomial time can be derived, with reasonable efforts, from Kopczy\'nski's conjecture.} for $\omega$-regular objectives.
However, some of the results of~\cite{CO24Positional} are not recovered with our methods: the finite-to-infinite and 1-to-2-player lifts, as well as closure of positionality under addition of neutral letters. 
\section{Preliminaries}
We let $\Sigma$ be a countable alphabet\footnote{We restrict our study to countable alphabets, as if $\SS$ is uncountable, the topological space $\SS^\omega$ is not Polish and the class $\BCSigma$ is not as well-behaved.} and $\eps\notin \Sigma$ be a fresh symbol that should be interpreted as a neutral letter.
Given a word $w \in (\Sigma \cup \{\eps\})^\omega$ we write $\pi_\Sigma(w)$ for the (finite or infinite) word obtained by removing all $\eps$'s from $w$; we call it its projection on $\Sigma$.
An objective is a set $W \subseteq \Sigma^\omega$.
Given an objective $W \subseteq \Sigma^\omega$, we let $W^\eps$ denote $\pi_\Sigma^{-1}(W) \subseteq (\Sigma \cup \{\eps\})^\omega$.

\subsection{Graphs, games and memory}

We introduce notions pertaining to games and strategy complexity, as they will be central in the statement of our results.
Nevertheless, we note that all our technical proofs will use these definitions through Theorem~\ref{thm:universal_graphs} below, and will not explicitly use games.

\subparagraph*{Graphs.}
A $\SS$-graph $G$ is given by a set of vertices $V(G)$ and a set of coloured, directed edges $E(G) \subseteq V(G) \times \SS \times V(G)$.
We write $v \re c v'$ for edges $(v,c,v')$.
A path is a sequence of edges with matching endpoints $(v_0 \re {c_0} v_1)(v_1 \re{c_0} v_2) \dots$ which we write as $v_0 \re {c_0} v_1 \re{c_1} \dots$.
Paths can be empty, finite, or infinite, and have a label $c_0c_1\dots$.
Throughout the paper, graphs are implicitly assumed to be without dead-end: every vertex has an outgoing edge.

We say that a vertex $v$ in a $\Sigma$-graph (resp. a $(\Sigma \cup \{\eps\})$-graph) satisfies an objective $W \subseteq \Sigma^\omega$ if the label of any infinite path from $v$ belongs to $W$ (resp. to $W^\eps$).
A pointed graph is a graph with an identified initial vertex.
A pointed graph satisfies an objective $W \subseteq \Sigma^\omega$ if the initial vertex satisfies $W$; a non-pointed graph satisfies an objective if all its vertices do.
An infinite tree is a sinkless pointed graph whose initial vertex is called the root, and with the property that every vertex admits a unique path from the root.

A morphism from a $\Sigma$-graph $G$ to a $\Sigma$-graph $H$ is a map $\phi: V(G) \to V(H)$ such that for any edge $v \re c v'$ in $G$, it holds that $\phi(v) \re c \phi(v')$ is an edge in $H$.
Morphisms between pointed graphs should moreover send the initial vertex of $G$ to the initial vertex of $H$.
Morphisms need not be injective.
We write $G \re{} H$ when there exists a morphism $\phi \colon G \to H$.

\subparagraph*{Games and strategies.}
A game is given by a pointed $(\Sigma \cup \{\eps\})$-graph $G$ together with an objective $W \subseteq \Sigma^\omega$, and a partition of the vertex set $V(G)=V_\Eve \sqcup V_\Adam$ into the vertices controlled by Eve and those controlled by Adam.
A strategy\footnote{We follow the terminology from~\cite{CO25LMCS}. The classical notion of a strategy as a function $f\colon E(G)^*\to V(G)$ can be recovered by considering the graph with vertices $E(G)^*$, and edges $\rho \re e \rho e$.} (for Eve) is a pointed graph together with a morphism $\pi$ towards $G$, satisfying that for every edge $v \re c v'$ in the game, where $v \in V_\Adam$, and for all $u \in \pi^{-1}(v)$, there is an edge $u \re c u'$ such that $u' \in \pi^{-1}(v)$.
A strategy is winning if it satisfies the objective $W$ of the game.
We say that Eve wins if there exists a winning strategy.



\subparagraph{Memory.} 
A finite-memory strategy\footnote{It is common to define a memory structure as an automaton reading the edges of a game graph. This notion can be recovered by taking $\{1,\dots,k\}$ as the states of the automaton.}
is a strategy with vertices $V(G) \times \{1,\dots,k\}$ and projection $\pi(v,m)=v$, with the additional requirement that for every edge $(v,m) \re \eps (v',m')$, it holds that $m=m'$.
We say that $k$ is the memory of the strategy, and numbers $1,\dots,k$ are called memory states.
Informally, the requirement above says that when reading an $\eps$-transition in the game, we are not allowed to change the memory state; this is called $\eps$-memory in~\cite{CO25LMCS} (to which we refer for more discussion), but since it is the main kind of memory in this paper, we will simply call it the memory.

A finite-memory strategy is called chromatic if there is a map $\chi:\{1,\dots,k\} \times (\Sigma \cup \{\varepsilon\}) \to \{1,\dots,k\}$ such that for every edge $(v,m) \re c (v',m')$ in the strategy, it holds that $m'=\chi(m,c)$.
We say that $\chi$ is the chromatic update.
Note that necessarily, we have $\chi(m,\eps)=m$ for every memory state $m$.


The (chromatic) memory of an objective $W$ is the minimal $k$ such that for every game with objective $W$, if Eve has a winning strategy, she has a winning (chromatic) strategy with memory $\leq k$.


\subsection{Automata}

We write $\d$ for the set $\{0,\dots,d\}$.
A parity automaton $\A$ (or just automaton in the following) with index $d$ -- an even number -- and alphabet $\Sigma$, is a pointed $((\Sigmaeps) \times \d)$-graph.
Vertices are called states, edges are called transitions and written $q \re{c:y} q'$, where $c \in \Sigmaeps$ and $y \in \d$.
Elements in $\d$ are called priorities.
Generally, we use the convention that even priorities are denoted with letter $x$, whereas $y$ can be used to denote any priority.
Transitions of the form $q \re{\eps:y} q'$ are called $\eps$-transitions; note that they also carry priorities.

Infinite paths from the initial state $q_0$ are called runs. A run is accepting if the projection of its label on the second coordinate belongs to
\[
    \Parity_d = \{y_0y_1 \dots \in \d^\omega \mid \liminf(y) \text{ is even}\}. \quad \text{ (Note the use of \emph{min}-parity.)}
\]
The language $L(\A)$ of $\A$ is $\pi_\Sigma(L')$, where $L' \subseteq (\Sigmaeps)^\omega$ is the set of projections on the first coordinate of runs which are accepting.
We require that all these projections are infinite words; stated differently, there is no accepting run from $q_0$ labelled by a word in $(\Sigmaeps)^* \eps^\omega$.
An automaton is deterministic if there are no $\eps$-transitions and for any state $q \in V(\A)$ and any letter $a \in \Sigma$ there is at most one transition $q \re{a:\ph} \ph$.
We say that an automaton is determinisable by pruning if one can make it deterministic by removing some transitions, without modifying its language.
A language belongs to $\BC(\bsigma 2)$ if it is the language of a deterministic automaton, and it is $\omega$-regular if the automaton is moreover finite.




We will often identify pointed graphs with automata of index $0$, by labelling all transitions with priority $0$. Note that in this case, all runs are accepting.
This requires making sure that there is no accepting run labelled by a word from $\Sigma^* \eps^\omega$, which, up to assuming that all vertices are accessible from the initial one, amounts to saying that there is no infinite path of $\re \eps$.
We say that such a graph is well-founded.


\subparagraph*{Blowups and $k$-automata.} A $k$-blowup $\B$ of an automaton $\A$ is any automaton with $V(\B) \subseteq V(\A) \times \{1,\dots, k\}$, with initial state in $\{q_0\}\times\{1,\dots,k\}$ and such that for each transition $q \re{a:y} q'$ in $\A$ and each $m \in \{1,\dots,k\}$, there is a transition $(q,m) \re{a:y} (q',\ph)$. We also allow for extra transitions in $\B$. Note that in this case $L(\A) \subseteq L(\B)$.

A $k$-automaton is just an automaton whose states are a subset of $Q \times \{1,\dots,k\}$, for some set $Q$; for instance, $k$-blowups are $k$-automata.
Equivalently, these are automata with a partition of their states in $k$ subsets.
For a state $(q,m)$ in a $k$-automaton, $m$ is called its memory state.
A $k$-automaton is called chromatic if there is a map $\chi:  \{1,\dots, k\} \times (\Sigma \cup \{\eps\}) \to  \{1,\dots, k\}$ such that for all transition $(q,m) \re{a:y} (q',m')$ it holds that $m'=\chi(q,m)$.

\subparagraph{Cascade products.} Let $\A$ be an automaton with alphabet $\Sigma$ and index $d$, and let $S$ be a $\d$-graph.
We define their cascade product $\A \casc S$ to be the $(\Sigmaeps)$-graph with vertices $V(\A) \times V(S)$ and edges
\[
    (q,s) \re c (q',s') \quad \iff \quad \exists y, [q \re {c:y} q' \text{ and } s \re y s'].
\]
If $S$ is a pointed graph with intial vertex $s_0$, then $\A \casc S$ is pointed with initial vertex $(q_0,s_0)$.
It is easy to check that we then have the following lemma.

\begin{lemma}\label{lem:cascade_products}
    Let $\A$ be an automaton with index $\d$ and $S$ be a $\d$-graph satisfying $\Parity_d$.
    Then $\A \casc S$ is well-founded and satisfies $L(\A)$.
\end{lemma}

\subsection{$\eps$-completability and universal graphs}
We now introduce and discuss the key notion used in our main characterisation, which adapts the notion from~\cite{CO24Positional} from positionality to finite memory.

\subparagraph*{$k$-wise $\eps$-completability.}
A $k$-automaton $\A$ with index $d$ is called $k$-wise $\eps$-complete if for each even $x \in \d$, for each memory state $m$ and each ordered pair of states $(q,m),(q',m)$:
\[
    \text{either} \quad (q,m) \re{\eps:x} (q',m) \quad \tor \quad (q',m) \re{\eps:x+1} (q,m). \]

Intuitively, having an edge $(q,m) \re{\eps:x} (q',m)$ means that ``$(q,m)$ is much better than $(q',m)$'', as one may freely go from $(q,m)$ to $(q',m)$ and even see a good priority on the way.
Similarly, $(q',m) \re{\eps:x+1} (q,m)$ means that ``$(q',m)$ is not much worse than $(q,m)$''.


It is also useful for the intuition to apply the definition to both ordered pairs $((q,m),(q',m))$ and $((q',m),(q,m))$.
Since automata exclude accepting runs which are ultimately comprised of $\eps$-transitions, we cannot have $(q,m) \rer{\eps:x} (q',m)$, and therefore $\eps$-completability rewrites as: for each $x$, each memory state $m$ and each unordered pair $(q,m),(q',m)$ of states,
\[
    \text{either} \quad (q,m) \xrightarrow[\eps :x+1]{\eps:x} (q',m) \quad \tor \quad (q,m) \rer{\eps:x+1} (q',m). 
\]
Hence an alternative, maybe more useful view (this is the point of view adopted in~\cite{CO24Positional}) is that, up to applying some adequate closure properties, a $k$-wise $\eps$-complete automaton is endowed with the following structure: for each even priority $x$ and each memory state $m$, the states with memory state $m$ are totally preordered by the relation $\re {x+1}$, and the relation $\re x$ is the strict version of this preorder.
Moreover, for $x'>x$, the $x'$-preorder is a refinement of the $x$-preorder.

A $k$-automaton $\A$ is called $k$-wise $\eps$-completable if one may add $\eps$-transitions to it so as to turn it into a $k$-wise $\eps$-complete automaton $\A^\eps$ satisfying $L(\A^\eps)=L(\A)$.
We simply say ``$\eps$-complete'' (resp. ``$\eps$-completable'') as a shorthand for ``1-wise $\eps$-complete'' (resp. ``1-wise'' $\eps$-completable).


\begin{example}
    Let $\Sigma = \{a, b,c\}$ and 
    \[W = \noOcc(b) \vee \finOften(aa) \vee \infOften(cc).\]
    We show a $2$-wise $\eps$-complete automaton recognising $W$ in Figure~\ref{fig:aut-eps-complete}. By Theorem~\ref{thm:main-charac}, the memory of $W$ is $\leq 2$ (and it is easy to see that this bound is tight).


    \begin{figure}[h]
        \centering
        \includegraphics{Figures/parity-automaton.pdf}
        \caption{A 2-automaton recognising $W=\noOcc(b) \vee \finOften(aa) \vee \infOften(cc)$, where $p$ is assumed to have a different memory state than $q_0,q_1$ and $q_2$. It is $2$-wise $\eps$-completable by adding the indicated $\eps$-transitions. A completion also contains the transitions $q_2\re{\eps:0,1,2} q_0$, omitted for ease of reading. However, it is not chromatic since reading $c$ may or may not switch the memory state.}
        \label{fig:aut-eps-complete}
    \end{figure}
\end{example}




The following theorem, a key result in~\cite{CO25LMCS} where it is called the structuration lemma, will also be crucial to this work.
Recall that we see well-founded pointed graphs as automata with only $0$-transitions, and we apply the terms $k$-blowup and $\eps$-completable to them accordingly.

\begin{restatable}[{Adapted from Lemma 3.4 in \cite{CO25LMCS}}]{theorem}{structuration}
    \label{thm:structuration}
    Let $G$ be a well-founded pointed graph satisfying an objective $W$ which is assumed to have (chromatic) memory $\leq k$ over games of size $\leq 2^{|G|}$.
    There is a (chromatic) $k$-blowup $G'$ of $G$ which is well-founded, $k$-wise $\eps$-complete, and satisfies $W$.
\end{restatable}

For completeness, we give a proof of Theorem~\ref{thm:structuration} in Appendix~\ref{app:structuration}.

\subparagraph*{Universal graphs.}
Given an objective $W$ and a cardinal $\kappa$, we say that a graph $U$ is $(\kappa,W)$-universal if for any infinite tree $T$ of cardinality $|V(T)|<\kappa$ satisfying $W$, there is a morphism $\phi:T \to U$ such that $\phi(t_0)$ satifies $W$ in $U$, where $t_0$ is the root of $T$.
We may now rephrase the main theorem of~\cite{CO25LMCS} in terms of $\eps$-complete universal graphs.

\begin{theorem}[Theorem 3.1 in \cite{CO25LMCS}]\label{thm:universal_graphs}
Let $W$ be an objective.
Then $W$ has (chromatic) memory $\leq k$ if and only if for every cardinal $\kappa$ there exists a $(\kappa,W)$-universal graph which is (chromatic and) $k$-wise $\eps$-complete.
\end{theorem}


We now give an explicit definition of a $(\kappa,\Parity_{d})$-universal graph $S^\kappa_{d}$ which is $\eps$-complete.
These ideas date back to the works of Streett and Emerson, who coined the name signatures~\cite{SE89}, and were made more explicit by Walukiewicz~\cite{Walukiewicz96}. 
Vertices are tuples of ordinals $<\kappa$, indexed by odd priorities in $d$ and ordered lexicographically (with the smaller indices being the most significant).
For a tuple $s$ and index $y$, we let $s_{< y}$ be the tuple obtained from $s$ by restricting to coordinates with index $< y$. Edges are given by
\[
    s \re y s \quad \iff \quad \begin{cases}
        s_{< y} \geq s'_{< y} \text{ and $y$ is even; or}\\
        s_{\leq y} > s'_{\leq y} \text{ and $y$ is odd,}
    \end{cases}
\]
and $\eps$-edges are given by $s \re{\eps} s'$ if and only if $s \geq s'$.

\begin{lemma}[{\cite[Lemma~2.7]{CO24Arxiv}}]
The graph $S^\kappa_d$ is $(\kappa,\Parity_d)$-universal.
\end{lemma}

We will work extensively with such tuples, as well as their prefixes and suffixes.
For readability, we also use subscripts to indicate which (odd) coordinates are concerned, for instance $s_{<x}$ will be our notation for tuples of ordinals $< \kappa$ indexed with odd priorities $<x$, and similarly for $s_{>x}$.
Concatenation of tuples is written like for words, therefore $s_{<x} s_{>x}$ denotes a tuple indexed by all odd priorities (i.e. an vertex of $S_\kappa^d$).
We treat $s_{<x}$ and $s_{>x}$ as different variables, which we may quantify independently.
 
\section{Characterisation of objectives in $\BC(\bsigma 2)$ with memory $\leq k$}
We state our main characterisation theorem and its decidability consequences for $\oo$-regular languages.
We assume that the alphabet $\Sigma$ is countable, therefore automata can also be taken with countable sets of states.


\begin{theorem}[Main characterisation]\label{thm:main-charac}
Let $W$ be a $\BCSigma$ objective and let $k \in \N$.
The following are equivalent:
\begin{enumerate}[(i.)]
    \item\label{item:memory-small-games} $W$ has memory $\leq k$ (resp. chromatic memory $\leq k$) on games of size $\leq 2^{2^{\aleph_0}}$.
    \item\label{item:existence-automata} For any automaton $\A$ recognising $W$, there is a (chromatic) $k$-blowup $\B$ of $\A$ which is $k$-wise $\eps$-complete and recognises $W$.
    \item\label{item:existence-det-automata} There is a deterministic (chromatic) $k$-automaton $\A$ which is $k$-wise $\eps$-completable and recognises $W$. If $W$ is recognised by a deterministic automaton of size $n$, then $\A$ can be taken of size $kn$.
    \item\label{item:existence-universal-graph} For every cardinal $\kappa$, there is a (chromatic)  $(\kappa,W)$-universal graph which is well-founded and $k$-wise $\eps$-complete.
    \item\label{item:memory-arbitrary-games} $W$ has (chromatic) memory $\leq k$ on arbitrary games.
\end{enumerate}
\end{theorem}

For $\omega$-regular $W$ given by a deterministic automaton $\B$ of size $n$, this allows to compute the (chromatic) memory in $\NP$: guess a deterministic automaton $\A$ of size $\leq kn$ and a (chromatic) $k$-wise $\eps$-completion $\A^\eps$, and check if  $L(\B) \subseteq L(\A)$ and if $L(\A^\eps) \subseteq L(\B)$, which can be done in polynomial time, since $\A$ and $\B$ are deterministic~\cite{ClarkeDK93Unified}.
Prior to our work, neither computing the memory nor the chromatic memory was known to be decidable (although the chromatic memory over finite graphs can be computed in exponential time~\cite{Kop08Thesis}).

\begin{corollary}[Decidability in $\NP$]\label{thm:NP-computation-memory}
    Given an integer $k$ and a deterministic automaton $\A$, the problem of deciding if $L(\A)$ has (chromatic) memory $\leq k$ belongs to $\NP$.
\end{corollary}

Our main contribution is the implication from (\ref{item:memory-small-games}) to (\ref{item:existence-automata}), which is the object of Section~\ref{sec:existence_automata}.
We proceed in Section~\ref{sec:existence-det-automata} to show that (\ref{item:existence-automata}) implies (\ref{item:existence-det-automata}) which is straightforward.
The implication (\ref{item:existence-det-automata}) $\implies$ (\ref{item:existence-universal-graph}) is adapted from~\cite{CO24Positional} and presented in Section~\ref{sec:existence-universal-graphs}.
Finally, the implication (\ref{item:existence-universal-graph}) $\implies$ (\ref{item:memory-arbitrary-games}) is the result of~\cite{CO25LMCS} (Theorem~\ref{thm:universal_graphs}), and the remaining one is trivial.

\subsection{Existence of $k$-wise $\eps$-complete automata: Proof overview}\label{sec:existence_automata}

We start with the more challenging and innovative implication: how to obtain a $k$-wise $\eps$-complete automaton given an objective in $\BC(\bsigma 2)$ with memory $k$ (that is, (\ref{item:memory-small-games}) $\implies$ (\ref{item:existence-automata})).
In this Section, we give a detail overview of the proof. 
Full details are given in Appendix~\ref{app:existence-eps-automata}.

\begin{restatable}{proposition}{existenceEpsComplete}
    \label{prop:existenceEpsComplete}
    Let $W$ be an objective recognised by an automaton $\A$, and assume that $W$ has (chromatic) memory $\leq k$ on games of size $\leq 2^{2^{\aleph_0}}$.
    Then there is a (chromatic) $k$-blowup $\B$ of $\A$ recognising $W$ which is $k$-wise $\eps$-complete.
\end{restatable}

We assume that $W$ has memory $\leq k$ on games of size $\leq \kappa$; we discuss the chromatic case at the end of the section.
Let $\A$ be an automaton recognising $W$; we aim to construct a $k$-blowup $\B$ of $\A$ which is $\eps$-complete.
We let $S$ denote $S^\kappa_d$, the $(\kappa,\Parity_{d})$-universal graph defined above.


We consider the cascade product $\A \casc S$; this is a $(\Sigma \cup \{\eps\})$-graph which intuitively encodes all possible accepting behaviours in $\A$.
Then we apply the structuration result (Theorem~\ref{thm:structuration}) to $\A \casc S$ which yields a $k$-blowup $G$ of $\A \casc S$ which is well-founded and $k$-wise $\eps$-complete (as a graph).
Stated differently, up to blowing the graph $\A \casc S$ into $k$ copies, we have been able to endow it with many $\eps$-transitions, so that over each copy, $\re \eps$ defines a well-order.
Note that the states of $G$ are of the form $(q,m,s)$, with $q\in V(\A)$, $m\in\{1,\dots,k\}$ and $s\in V(S)$.

The states of $\B$ will be $V(\B)=V(\A) \times \{1,\dots,k\}$.
The challenge lies in defining the transitions in $\B$, based on those of $G$.


Given a state $(q,m)\in V(\B)$ and a transition $q \re {c:y} q'$ in $\A$, where $c \in \Sigma \cup \{\eps\}$, by applying the definitions we get transitions of the form $(q,m,s) \re c (q',m',s')$ in $G$, for different values of $m'$, whenever $s \re y s'$ in $S$.
We will therefore define transition $(q,m) \re{c:y} (q',m')$ in $\B^\eps$ if $m'$ matches suitably many transitions as above; for now, we postpone the precise definition.

We should then verify that the obtained automaton $\B$:
\begin{itemize}
    \item is a $k$-blowup of $\A$,
    \item is $\eps$-complete, and
    \item recognises $W$.
\end{itemize}
The first two items above state that $\B$ should have many transitions: at least those inherited from $\A$, and in addition a number of $\eps$-transitions.
This creates a tension with the third item, which states that even with all these added transitions, the automaton $\B$ should not accept too many words.

Let us focus on the third item for now, which will lead to a correct definition for $\B$.
Take an accepting run
\[
    (q_0,m_0) \re{c_0:y_0} (q_1,m_1) \re{c_1,y_1} \dots
\]
in $\B$, where $x = \liminf_i y_i$ is even; for the sake of simplicity, assume that all $y_i$'s are $\geq x$.
We should show that its labelling $w=c_0c_1\dots$ belongs to $W$.
To this end, we will decorate the run with labels $s_0,s_1,\dots \in S$ so that
\[
    (q_0,m_0,s_0) \re{c_0} (q_1,m_1,s_1) \re{c_1} \dots
\]
defines a path in $G$, which concludes since $G$ satisfies $W$.

Recall that the elements of $S$ are tuples of ordinals $<\kappa$ indexed by $d/2$ odd priorities. We use $s_{<x}$ (resp. $s_>x$)to refer to a tuple indexed by priorities up to $x-1$ (resp. from $x+1$).


To construct the $s_i$'s, we fix a well chosen prefix $s_{<x}\in \kappa^{x_\odd}$ which will be constant, and proceed as follows.

\begin{enumerate}[(a.)]
    \item If $y_i=x$, then we set $s_i=s_{<x} s_{>x}$, for some $s_{>x}$ which depends only on $c_i$.
    \item If $y_i<x$, then we set $s_i=s_{<x} s_{>x}$, for some $s_{>x}$ which depends on $c_i$ as well as $s_{i+1}$.
\end{enumerate}

At this stage the reader may be worried that the backward induction underlying the above definition is not well-founded; however, since the first case occurs infinitely often, the backward induction from the second item is only performed over finite blocks (see also Figure~\ref{fig:backtrack} in Appendix~\ref{app:existence-eps-automata}).

This leads to the following definition for $\B$, where $x$ is an even priority:
\[
    \begin{array}{rcl}
     (q,m) \re{c:x} (q',m') \tin \B  &\iff&  \exists s_{<x} \exists s_{>x} \forall s'_{>x}, (q,m,s_{<x}s_{>x}) \re{c} (q',m',s_{<x} s'_{>x}) \tin G, \\
     (q,m) \re{c:x+1} (q',m') \tin \B  &\iff&  \exists s_{<x} \forall s'_{>x} \exists s_{>x}, (q,m,s_{<x}s_{>x}) \re{c} (q',m',s_{<x} s'_{>x}) \tin G.
    \end{array}
\]

The first line corresponds to point (a.) above, where $s_{>x}$ can be chosen independently of $s'_{>x}$, whereas the second line corresponds to point (b.), since the choice of $s_{>x}$ is conditioned on the value of $s'_{>x}$.
(For priorities $>x+1$, we may apply either the first or second line, depending on the parity, to get the required conclusion.)

The remaining issue is that the choice of the fixed common prefix $s_{<x}$ should be made uniformly, regardless of the transition.
This is achieved thanks to an adequate extraction lemma (which extends the pigeonhole principle to the case at hands), which finds a large enough subset $T$ of $\kappa^{\d_\odd}$, so that transitions $(q,m,s)\re{}(q',m',s')$ are similar for different choices of $s,s' \in T$.
This ensures that $s_{<x}$ can be chosen uniformly.

There remains to verify that $\B$ is indeed a $k$-blowup of $\A$ and that it is $\eps$-complete, which will follow easily from the definitions and $\eps$-completeness of $G$ (this is because, after removing ``$\exists s_{<x}$'' from the definition above, the second line resemble the negation of the first).

For the chromatic case, the proof is exactly the same, we should simply check that if $G$ is chromatic (which is guaranteed by Theorem~\ref{thm:structuration}), then the so is the obtained automaton $\B$.

\subsection{Existence of deterministic $\eps$-completable automata}\label{sec:existence-det-automata}

We now prove the implication from (\ref{item:existence-automata}) to (\ref{item:existence-det-automata}) in Theorem~\ref{thm:main-charac}.
Take $\A$ to be a deterministic automaton recognising $W$, and let $\B^\eps$ be the obtained $k$-blowup of $\A$ which is (chromatically) $k$-wise $\eps$-complete and recognises $W$.
Then let $\B$ be obtained from $\B^\eps$ by only keeping, for each state $(q,m) \in V(\B)$ and each transition $q \re{a:y} q'$ in $\A$, a single transition of the form $(q,m) \re {a:y} (q',m')$ chosen arbitrarily.
Note that $\B$ is a $k$-blowup of $\A$, so we have: $L(\A) \subseteq L(\B) \subseteq L(\B^\eps)$.
We conclude that $\B$ is a deterministic (chromatically) $k$-wise $\eps$-completable automaton recognising $W$, as required.


\subsection{From deterministic $\eps$-completable automata to universal graphs}\label{sec:existence-universal-graphs}

We now prove the implication from (\ref{item:existence-det-automata}) to (\ref{item:existence-universal-graph}) in Theorem~\ref{thm:main-charac}.
This result was already proved in~\cite[Prop.~5.30]{CO24Arxiv} for the case of $k=1$; extending to greater values for $k$ presents no difficulty.

Let $\B$ be a deterministic\footnote{For the purpose of this proof, history-determinism would be sufficient (we refer to~\cite{BL23SurveyHD} for the definition and context on history-determinism).} (chromatic) $k$-wise $\eps$-completable automaton recognising $W$ and $\B^\eps$ be an $\eps$-completion.
Fix a cardinal $\kappa$ and let $S$ denote $S_d^\kappa$.
Consider the cascade product $U = \B^\eps \casc S$.
We claim that $U$ is (chromatic) $k$-wise $\eps$-complete and $(\kappa,W)$-universal.
Universality follows from the facts that $\B$ is deterministic and $S$ is $(\kappa,\Parity_d)$-universal.

\begin{claim}
    The graph $U = \B^\eps \casc S$ is $(\kappa,W)$-universal.
\end{claim}

\begin{claimproof}
    Take a tree $T$ of size $<\kappa$ which satisfies $W$.
    We should show that $T \to U$.
    Since $\B$ is deterministic, we can define a labelling $\rho\colon V(T) \to V(\B)$ by mapping $t_0$ to $q_0$ and $t \mapsto q$ if the run of $\B$ on the finite word labelling the path $t_0\re{}t$ ends in $q$. Then, any infinite path from $t_0$ in $T$ is mapped to a run in $\B$ that is accepting (since $T$ satisfies $W$).
    Therefore the tree $T'$ obtained by taking $T$ and replacing edge-labels by the priorities appearing in their $\rho$-images satisfies $\Parity_d$ and has size $<\kappa$, so there is a morphism $\mu:T \to S$.
    It is a direct check that $(\rho,\mu) \colon V(T) \to V(\B) \times V(S) = V(U)$ indeed defines a morphism.
\end{claimproof}

Showing that $U$ is $k$-wise $\eps$-complete is slightly trickier.

\begin{claim}
    The graph $U = \B^\eps \casc S$ is well-founded and (chromatic) $k$-wise $\eps$-complete.
\end{claim}

\begin{claimproof}
    Well-foundedness of $U$ follows directly from Lemma~\ref{lem:cascade_products}.
    Let us write $B_1,\dots,B_k$ for the $k$ parts of $\B^\eps$; the $k$ parts of $U$ will be $B_1 \times V(S),\dots, B_k \times V(S)$.
    If applicable, chromaticity is a direct check.
    Let $(b,s),(b',s') \in V(U)$ be in the same part, i.e.~$b,b' \in B_i$ for some $i$.
    Let $x_0$ be the minimal even priority such that 
    $b \re{\eps:x_0} b'$ in $\B^\eps$ 
    (if such an $x$ does not exist then $x_0=d+2$).
    Then let $y_0$ be the minimal odd priority such that $s'_{\leq y_0} > s_{\leq y_0}$ (as previously, if $s=s'$ then we let $y_0=d+1$).
    We distinguish two cases.
    \begin{enumerate}[(1)]
        \item If $x_0 < y_0$. Then we have $b \re{\eps:x_0} b'$ in $\B^\eps$ and $s_{<x_0} = s'_{<x_0}$ which gives $s \re{x_0} s'$ in $S$.
        Thus we get $(b,s) \re{\eps} (b',s')$ in $U$.
        \item If $y_0 < x_0$. Then $s'_{\leq y_0} > s_{\leq y_0}$, which gives $s' \re{y_0} s$ in $S$.
        Since $\B^\eps$ is $k$-wise $\eps$-completable and by definition of $x_0$, we also have $b' \re{\eps:y_0} b$ in $\B^\eps$, therefore $(b',s') \re{\eps} (b,s)$ in $U$.
    \end{enumerate}

    We conclude that either $(b,s)\re{\eps}(b',s')$ or $(b',s') \re{\eps} (b,s)$, as required.
\end{claimproof}
 

\section{Union of objectives}\label{sec:union}


In this section, we establish a strong form of the generalised Kopczyński conjecture for $\BCSigma$ objectives.
An objective $W \subseteq \Sigma^\omega$ is prefix-increasing\footnote{In other papers~\cite{Ohlmann21PhD,Ohlmann23,CO25LMCS} this notion is called prefix-decreasing, as Eve is seen as a ``minimiser'' player who aims to minimise some quantity.}
 if for all $a \in \Sigma$ and $w \in \Sigma^\omega$, it holds that if $w \in W$ then $aw \in W$.
In words, one remains in $W$ when adding a finite prefix to a word of $w$.
Examples of prefix-increasing objectives include prefix-independent and closed objectives.
\ac{Other interesting clases? }


\begin{theorem}\label{thm:union}
    Let $W_1, W_2 \subseteq \SS^\oo$ be two $\BC(\bsigma 2)$ objectives over the same alphabet, such that $W_{2}$ is prefix-increasing.
    Assume that $W_{1}$ has memory $\leq k_1$ and $W_{2}$ has memory $\leq k_2$.
    Then $W_1 \cup W_2$ has memory $\leq k_1 k_2$.
\end{theorem}

\begin{remark}
The assumption that one of the two objectives is prefix-increasing is indeed required: for instance if $W_1=aa(a+b)^\omega$ and $W_{2}=bb(a+b)^\omega$, which are positional but not prefix-increasing, the union $(aa+bb)(a+b)^\omega$ is not positional (it has memory $2$).

\end{remark}
\begin{remark}
    The bound $k_1k_2$ in Theorem~\ref{thm:union} is tight: For every $k_1, k_2$, there are objectives $W_1$, $W_2$ with memories $k_1, k_2$ respectively, such that $W_1 \cup W_2$ has memory exactly $k_1k_2$. 
    One such example is as follows: let $\SS = \{a_1,\dots, a_{k_1}, b_1,\dots, b_{k_2}\}$ and 
    $W_1 = \{ w \mid w \text{ contains at } \text{least } \text{two } \text{different } a_i \text{ infinitely often} \}$ and $W_2 = \{ w \mid w$ contains at least two  $b_i$ infinitely often$\}$. 
    We can see that $W_1$, $W_2$ and $W_1\cup W_2$ have memory, respectively, $k_1, k_2$, and  $k_1\cdot k_2$ by building the Zielonka tree of these objectives and applying~\cite[Thms.~6,~14]{DJW1997memory}.
\end{remark}

The rest of the section is devoted to the proof of Theorem~\ref{thm:union}.
We explicit a construction for the union of two parity automata, inspired from the Zielonka tree of the union of two parity conditions, and show that it is $(k_1 k_2)$-wise $\eps$-completable.



\subparagraph{Union of parity languages.} We give an explicit construction of a deterministic parity automaton $\mathcal{T}$ recognising the union of two parity languages, which may be of independent interest.  This corresponds to the automaton given by the Zielonka tree of the union (a reduced and structured version of the LAR).

We let $\done = \{0, 1,\dots,d_1\}$  and $\dtwo= \{1^*,\dots,d_2^*\}$.
Let $\doneodd$ and $\dtwoodd$ denote the restrictions of $\done$ and $\dtwo$ to odd elements.
By a slight abuse of notation, we sometimes treat elements in $\dtwo$ as natural numbers (e.g. when comparing them).\\

\textit{Alphabet.} 
The input alphabet is $\done \times \dtwo$, and we write letters as $(y,z)$. 
The index of $\T$  is $d_1 + d_2$, we use the letter $t$ for its output priorities.\\


\textit{States}. 
States are given by interleavings of two strictly increasing sequences of $\doneodd$ and $\dtwoodd$. Any state can be taken as initial. For instance, for $d_1=d_2=6$, an example of a state is:\\[-4mm]
\[  \hspace{15mm} \tau = \langle \tikzmark{tau1}1,\tikzmark{tau2}3,\tikzmark{tau3}1^*,\tikzmark{tau4}5, \tikzmark{tau5}3^*, \tikzmark{tau6}5^* \rangle .\]

\begin{tikzpicture}[remember picture, overlay] \foreach \lbl in {1,...,6} {
        \node[below, xshift = 1.4mm, yshift = -1mm, scale=0.7, color = lipicsGray] at (pic cs:tau\lbl) {$\lbl$};
    }
\end{tikzpicture}



\vspace{-2.5mm}

We use $\tau$ to denote such a sequence, which we index from $1$ to $(d_1+d_2)/2$, and write $\tau[i]$ for its $i$-th element.
For $x\in \done$, $x$ odd,  we let $\indtau{x} = i$ for the index such that $\tau[i]=x$. For $x\geq 2$ even, we let $\indtau{x}$ be the index $i$ such that $\tau[i]=x-1$. We let $\indtau{0} = 0$. We use the same notation for $y\in \dtwo$, $y\geq 1$. For example, in the state above, $\indtau{2^*}=3$.

The intuition is that a state stores a local order of importance between input priorities.\\

\textit{Transitions.}
Let $\tau$ be a state and $(y,z)$ an input letter. We define the transition $\tau \re{(y,z):t}\tau'$ as follows:

Let $i = \min \{ \indtau{y}, \indtau{z}\}$. 
In the following, we assume $i = \indtau{y}$ (the definition for $i = \indtau{z}$ is symmetric). 
We let $t = 2i$, if $y$ even, and  $t = 2i-1$ if $y$ is odd.
If $y$ is even, we let $\tau' = \tau$.
If $y$ is odd, let $i'$ be the smallest index $i<i'$ such that $\tau[i']\in \dtwo$, and let $\tau'$ be the sequence obtained by inserting $\tau[i']$ on the left of $\tau[i]$ (or $\tau' = \tau$ if no such index $i'$ exists). 
Formally,
\[ \tau'[j] = \tau[j] \tfor j<i \tand i'<j,  \quad \tau'[i] = \tau[i'] \quad \tand \quad \tau'[j] = \tau[j-1] \tfor i<j\leq i' .\]

For example, for the state above, we have:
$ \langle 1,3,1^*, 5, 3^*, 5^* \rangle  \re{ (3,2^*):3} \langle 1, 1^*, 3, 5, 3^*, 5^* \rangle. $\\
\begin{lemma}\label{lem:language-of-T}
    The automaton $\T$ recognises the language \[L = \{w \in (\done \times \dtwo)^\omega \mid \pi_1(w) \in \Parity_{d_1} \ \mathrm{or} \ \pi_2(w) \in \Parity_{d_2}\}.\]
\end{lemma}
\begin{proof}\textcolor{red}{TOPROVE 0}\end{proof}


\subparagraph{$0$-freeness of automata for prefix-increasing objectives.} The fact that $W_2$ is prefix-increasing will be used via the following lemma. It recasts the fact that we can add $\re{\eps:1}$ transitions everywhere to automata recognising prefix-increasing objectives.

\begin{lemma}\label{lem:prefix-increasing}
    Let $W$ a be prefix-increasing objective with memory $\leq k$.
    There exists a deterministic $k$-wise $\eps$-completable automaton $\A$ recognising $W$ and an $\eps$-completion $\A^\eps$ of $\A$ such that $\A^\eps$ does not have any transition with priority $0$.
\end{lemma}

\begin{proof}\textcolor{red}{TOPROVE 1}\end{proof}



\subparagraph{Main proof: $\eps$-completion of the product.}
We now proceed with the proof of Theorem~\ref{thm:union}.
Using Theorem~\ref{thm:main-charac}, for $l =1,2$, we take  deterministic $k_l$-wise $\eps$-completable automata $\A_l$ of index $d_l$ recognising $W_l$, and its $\eps$-completion $\A_{l}^\eps$. 
For $l=2$, we assume thanks to Lemma~\ref{lem:prefix-increasing} that $\A_{2}^\eps$ does not have any transition with priority $0$.


We consider the product $\A=(\A_{1} \times \A_{2}) \casc \T$, with states $V(\A_1) \times V(\A_2) \times V(\T)$ and transitions $(q_1,q_2,\tau) \re{a:t} (q'_1,q'_2,\tau')$ if $q_1\re{a:y}q_1'$ in $\A_1$, $q_2\re{a:z}q_2'$ in $\A_2$, and $\tau \re{(y,z):t} \tau'$ in~$\T$. 
The correctness of such a construction is folklore.\footnote{$\A_1\times \A_2$ can be seen as a Muller automaton with acceptance condition the union of two parity languages. The composition with $\T$ yields a correct parity automaton, as $\T$ recognises the acceptance condition.}


\begin{claim}
    The automaton $\A$ is deterministic and recognises $W=W_{1} \cup W_{2}$.
\end{claim}


Therefore, there only remains to show the following lemma.

\begin{lemma}\label{lem:product-with-T-epsCompl}
    The automaton $\A=(\A_{1} \times \A_{2}) \casc \T$ is $(k_1k_2)$-wise $\eps$-completable.
\end{lemma}

The $\eps$-completion of $\A$ will be a variant of a product of the form $\A_1^\eps \times \A_2^\eps \casc \overline \T$, where $\overline \T$ is a non-deterministic extension of $\T$ with more transitions, but which still recognises the same language.\\

\textit{The automaton $\overline \T$.}
Intuitively, we obtain $\overline \T$ by allowing to reconfigure the elements of index $>i$ in a state $\tau$ by paying an odd priority $2i-1$, as well as allowing to move elements of $\done$ to the left. We precise this idea next.

We order the states of $\T$ lexicographically, where we assume that $x < y$ for $x\in \done$ and $y\in \dtwo$.
Formally, we let $\tau < \tau'$ if for the first position $j$ where $\tau$ and $\tau'$ differ, $\tau[j]\in \done$ (and therefore necessarily $\tau'[j]\in \dtwo$).
We let $\tau[..i]$ be the prefix of $\tau$ up to (and including) $\tau[i]$.
We write $\tau <_i \tau'$ if $\tau[..i] < \tau'[..i]$. 

Let $\tau \re{(y,z):t}$ be a transition in $\T$ as above, and $i_0 = \min \{ \indtau{y}, \indtau{z}\}$ be the index determining $t$ (i.e. $t\in \{ 2i_0-1, 2i\}$). The automaton $\overline \T$ contains a transition $\tau \re{(y,z):t'} \tau'$ if:

\begin{enumerate}
    \item $t' = t$ is odd, and $\tau' \leq_{i_0-1} \tau$; or
    \item $t' = t$ is even, and $\tau' \leq_{i_0} \tau$; or
    \item $t' \in \{2i'-1, 2i'\}$ for some $i' \leq i_0$ and $\tau' <_{i'} \tau$. (Note that, if $t$ is odd, this includes all (possibly even) $t'\leq t+1$.)
\end{enumerate}

In words, we are allowed to output a small (i.e. important) priority when following a strict decrease on sufficiently small components in $\tau$. 
Note that transitions in $\T$ also belong to $\overline \T$ thanks to the rules (1) and (2).



\begin{lemma}\label{lem:overline-T}
    The automaton $\overline \T$ recognises the same language as $\T$.
\end{lemma}
\begin{proof}\textcolor{red}{TOPROVE 2}\end{proof}


\textit{The $\eps$-completion.} 
We define $\A^\eps$ as a version of the cascade product of $\A_{1}^\eps \times \A_{2}^\eps$ with~$\T$, in which $\eps$-transitions are also allowed to use the transitions of $\overline \T$.
We let $\cleq$ denote the preference ordering over priorities, given by
$ 1 \cleq 3 \cleq \dots\cleq d-1 \cleq d \cleq  \dots 2 \cleq 0 \text{ ($d$ even)}$.
The transitions in $\A^\eps$ are defined as follows:
\begin{itemize}
    \item For $a\in \SS$:
    $(q_1,q_2,\tau) \re{a:t} (q'_1,q'_2,\tau')$ if this transition appears in $(\A_{1} \times \A_{2})\casc \T$. \item $(q_1,q_2,\tau) \re{\eps:t} (q'_1,q'_2,\tau')$ if $q_1\re{\eps:y}q_1'$ in $\A_1^\eps$, $q_2\re{\eps:z}q_2'$ in $\A_2^\eps$, and $\tau \re{(y,z):t'} \tau'$ in $\overline \T$ with $t' \cleq t$.
\end{itemize}

Note that the condition $t' \cleq t$ simply allows to output a less favorable priority, so it does not create extra accepting runs.
By definition, $\A^\eps$ has been obtained by adding $\eps$-transitions to $\A$.
It is a folklore result that composition of non-deterministic automata also preserves the language recognised, so this construction is correct.
\begin{claim}
    The automaton $\A^\eps$ recognises $W$.
\end{claim}

Therefore, we there only remains to prove the following lemma.

\begin{restatable}{lemma}{AepsCompleteUnion}\label{lem:A-eps-complete-union}
     The automaton $\A^\eps$ is $(k_{1}k_{2})$-wise $\eps$-complete. 
 \end{restatable}

The formal proof of this statement is presented in Appendix~\ref{app:unon}.
The $k_1$ parts of $\A_1^\eps$ and the $k_2$ parts of $\A_2^\eps$ naturally induce a partition of the states of $\A^\eps$ into $k_1k_2$ parts.
Then, given two states $r = (q_1,q_2,\tau)$ and $r' = (q_1',q_2', \tau')$ in the same part of $\A^\eps$, we consider the longest common prefix of $\tau$ and $\tau'$. We perform a case analysis: Depending on the priorities of $\done$ or $\dtwo$ appearing in this prefix, and the transitions $q_l \re{\eps:x} q_l'$ of the automata $\A_1^\eps$, $\A_2^\eps$, we will find transitions $r\re{\eps:x} r'$ or $r'\re{\eps:x+1} r$ for all even $x$.
 

\section{Conclusions and open questions}

We characterised objectives in $\BCSigma$ with memory (or chromatic memory) $\leq k$ as those recognised by a well-identified class of automata.
In particular, this gives the first known characterisation of (chromatic) memory for $\omega$-regular objectives, and proves that it is decidable (in fact even in $\NP$).
We also settled (a strengthening of) Kopczyński's conjecture for $\BCSigma$ objectives.
We now discuss some directions for future work.

\subparagraph*{Memory in finite games.}
This paper focuses on games over potentially infinite game graphs. 
A wide body of literature studies the memory over finite game graphs~\cite{Kop08Thesis,BRORV22}; we believe that, for $\oo$-regular objectives, both notions should coincide, and our results should characterise the memory of $\oo$-regular objectives over finite games too. 
More precisely, we believe that in item~(\ref{item:memory-small-games}) in Theorem~\ref{thm:main-charac}, it suffices to assume that $W$ has memory $\leq k$ over games of size $f(n)$, for some finite bound $f(n)$, where $n$ is the size of a deterministic automaton representing $W$. 

\begin{question}[{Version of~\cite[Conjecture~9.1.2]{Vandenhove23Thesis}}]\label{quest:ptime}
	Show that if an $\oo$-regular objective has memory $\leq k$ over finite game graphs, then it has memory $\leq k$ over all game graphs.
\end{question}

The hypothesis on $\oo$-regularity is necessary, as this statement already fails in the case of positionality and closed objectives~\cite{CFH14}.
We believe that one should be able to adapt the proof of Proposition~\ref{prop:existenceEpsComplete} to obtain this result, but some new ideas seem to be required.
As a follow-up question one could investigate the bound $f(n)$: can it be assumed polynomial?


\subparagraph*{Exact complexity of computation of memory.} We established that computing the (chromatic) memory of an $\omega$-regular objective is in $\NP$.
In fact, computing the chromatic memory is $\NP$-hard already for simple classes of objectives, such as Muller~\cite{Casares22Chromatic} or safety ones~\cite{BFRV23Regular}.
However, no such hardness results are known for non-chromatic memory.

\begin{question}\label{quest:ptime}
	Given a deterministic parity automaton $\A$ and a number $k$, can we decide whether the memory of $L(\A)$ is $\leq k$ in polynomial time?
\end{question}

This question is open already for the simpler case of regular open objectives (that is, those recognised by reachability automata).


\subparagraph{Assymetric 1-to-2-player lifts.} A celebrated result of Gimbert and Zielonka~\cite{GZ05} states that if for an objective $W$ both players can play optimally using positional strategies in finite games where all vertices belong to one player, then $W$ is bipositonal over finite games. This result has been extended in two orthogonal directions: to objectives where both players require finite chromatic memory~\cite{BRORV22,BRV23} (symmetric lift for memory), and to $\oo$-regular objectives where Eve can play positionally in $1$-player games~\cite{CO24Positional} (asymmetric lift for positionality).
In this work, we have not provided an asymmetric lift for memory, as in most cases no such result can hold.
For  $\BCSigma$ objectives, it is known to fail already for positional objectives~\cite[Section~7]{GK22Submixing}.
For non-chromatic memory, it cannot hold for $\oo$-regular objectives neither, because of the example described below.

\begin{restatable}{proposition}{counterexampleLift}
\label{prop:1-2-player-counterexample}
	Let $\Sigma_n = \{1,\dots, n\}$. For every $n$, the objective 
	\[W_n = \{w\in \SS_n^\oo \mid w \text{ contains two different letters infinitely often}\} \] 
	 has memory $2$ over games where Eve controls all vertices and memory $n$ over arbitrary~games.
\end{restatable}
\begin{proof}\textcolor{red}{TOPROVE 3}\end{proof}
In his PhD thesis, Vandenhove conjectures that an assymetric lift for chromatic memory holds for $\oo$-regular objectives~\cite[Conjecture~9.1.2]{Vandenhove23Thesis}. This question remains open.

\begin{question}
	Is there an $\oo$-regular objective  with chromatic memory $k$ over games where Eve controls all vertices and chromatic memory $k'>k$ over arbitrary~games?
\end{question}


\subparagraph*{Further decidability results for memory.} As mentioned in the introduction, many extensions of $\oo$-automata (including deterministic $\oo$-Turing machines and unambiguous $\oo$-petri nets~\cite{FSJLS22}) compute languages that are in $\BCSigma$.
We believe that our characterisation may lead to decidability results regarding the memory of objectives represented by these models.

\subparagraph*{Objectives in $\Delta_0^3$.} Some of the questions answered in this work in the case of $\BCSigma$ objectives are open in full generality, for instance, the generalised Kopczy\'nski's conjecture. A reasonable next step would be to consider the class $\bdelta 3 = \bsigma 3 \cap \bpi 3$.
Objectives in $\bdelta 3$ are those recognised by max-parity automata using infinitely many priorities~\cite{Skrzypczak13Colorings}.
Our methods seem appropriate to tackle this class, however, we have been unable to extend the extraction lemma (Lemma~\ref{lem:everywhere_cofinal}) used in the proof of Section~\ref{sec:existence_automata}.



 


\bibliography{bib}

\newpage
\appendix


\section{Proof of Theorem~\ref{thm:structuration} (Structuration result)}\label{app:structuration}

We now give a proof of Theorem~\ref{thm:structuration}.

\structuration*

The idea is to use choice arenas, which were introduced in Ohlmann's PhD thesis~\cite[Section 3.2 in Chapter 3]{Ohlmann21PhD} for positionality, and then adapted to memory in~\cite{CO24Positional}.

\begin{proof}\textcolor{red}{TOPROVE 4}\end{proof} 

\section{Full proof of (\ref{item:memory-small-games}) $\implies$ (\ref{item:existence-automata}) in Theorem~\ref{thm:main-charac} (Existence of $k$-wise $\eps$-complete automata)}\label{app:existence-eps-automata}
We prove the following proposition.

\existenceEpsComplete*

We let $\kappa=2^{\aleph_0}$ and let $S$ denote $S_d^\kappa$.
Our goal is to define a (chromatic) $k$-blowup $\B$ of $\A$ which is $k$-wise $\eps$-complete and recognises $W$.
For now, we discuss general memory, and explain below how the proof is (very easily) adapted to the chromatic case.

We start with the extraction lemma mentioned in the overview, then we will present the definition of $\B$ and then prove its correctness.

\subsection{A combinatorial lemma: Extracting homogeneous subtrees}

As an important part of our proof, we will take the graph $S$, whose set of vertices is $\kappa^{\d_\odd}$, where $\d_\odd=\{1,3,\dots,d-1\}$, and extract from it a large enough subgraph which is homogeneous.
This requires a combinatorial lemma which we now describe.

By a slight abuse of terminology (since this is not consistent with the definition of trees from the preliminaries), we use the terminology ``signature trees'' to refer to subsets of $\kappa^{\d_\odd}$.
Elements of the subsets should be thought of as leaves of the tree, while their (non-proper) prefixes correspond to nodes.
More precisely, a node of level $x$, where $x$ is an even priority from $\d$, in a signature tree $T$ is a tuple $s_{< x} \in \kappa^{x_\odd}$ such that there exists $s_{> x}$ satisfying $s_{< x} s_{> x} \in T$.

The subtree rooted at a node $s_{<x}$ of level $x$ is defined to be the tree $\{s_{>x} \mid s_{<x}s_{>x} \in T\}$.
We say that a tree $T$ is everywhere cofinal if for each node $s_{< x}$, the subtree rooted at $x$ is cofinal in $\kappa^{(d-x)_\odd}$.
An inner labelling of a tree $T$ by $L$ is a map $\lambda$ assigning a label in $L$ to every node in $T$.
We are now ready to state the extraction lemma
We let $\kappa=2^{\aleph_0}$.

\begin{lemma}\label{lem:everywhere_cofinal}
Let $\lambda$ be an inner labelling of $\kappa^{d_\odd}$ by $L$, where $L$ is countable.
There is an everywhere cofinal tree $T$ such that at every level $x$, $\lambda$ is constant over nodes of level $x$.
\end{lemma}
We say that a labelling as in the conclusion of the lemma is constant per level.

\begin{proof}\textcolor{red}{TOPROVE 5}\end{proof}

\subsection{Definition of $\B$}

Consider the cascade product $\A \casc S$. 
By Lemma~\ref{lem:cascade_products}, it satisfies $W$ and is well-founded.
Moreover it has size $\leq \kappa$, so by our assumption on $W$, we may apply Theorem~\ref{thm:structuration}.
This yields a $k$-blowup $G$ of ${\A \casc S}$ which is $k$-wise $\eps$-complete.
Let us write $V(G) = V(\A) \times \{1,\dots,k\} \times V(S)$.
We close $G$ by transitivity, meaning that we add transitions $(q,m,s) \re c (q',m',s')$, for $c \in \Sigmaeps$, whenever $(q,m,s) \re{\eps^* c \eps^*} (q',m,s')$; the obtained graph $\overline{G}$ still satisfies $W$.

Here comes the important definition: say that $(q,m)$ strongly $c$-dominates $(q',m')$ at node $s_{< x}$ if
\[
    \exists s_{> x} \forall s'_{> x} \qquad (q,m,s_{< x} s_{> x}) \re c (q',m',s_{< x} s'_{> x}) \tin \overline{G},
\]
and that $(q,m)$ weakly $c$-dominates $(q',m')$ at $s_{< x}$ if
\[
    \forall s'_{> x} \exists s_{> x} \qquad (q,m,s_{< x} s_{> x}) \re c (q',m',s_{< x} s'_{> x}) \tin \overline{G},
\]
where $c \in \Sigmaeps$.
Note that strong domination implies weak domination.
The type of a node $s_{< x}$ is the information, for each $q,q',m,m'$ and $c$, of whether $(q,m)$ strongly or weakly (or not at all) $c$-dominates $(q',m')$.
This gives finitely-many possibilities for fixed $q,q',m,m'$ and $c$, and therefore there are in total a countable number of possible types.
Thus Lemma~\ref{lem:everywhere_cofinal} yields a tree $T \subseteq \kappa^{\alpha}$ which is everywhere cofinal and such that for all $x$, nodes at level $x$ in $T$ all have the same type $t_x$.

We are now ready to define $\B$.
We put $V(\B) = V(\A) \times \{1,\dots,k\}$, and for each even $x \in \d$ and $c \in \Sigmaeps$, we define transitions by
\[
    \begin{array}{lcrl}
    (q,m) &\re{c:x}& (q',m') & \tif (q,m) \text{ strongly $c$-dominates } (q',m') \tin t_x \\
    (q,m) &\re{c:x+1}& (q',m') & \tif (q,m) \text{ weakly $c$-dominates } (q',m')\tin t_x.
    \end{array}
\]
Here is the main lemma, which proves the direct implication in Theorem~\ref{thm:main-charac}.

\begin{lemma}\label{lem:main_lemma_charac}
Automaton $\B$ is a $k$-blowup of $\A$, it is $k$-wise $\eps$-complete and it recognises $W$.
\end{lemma}
The remainder of the section is devoted to proving Lemma~\ref{lem:main_lemma_charac}.

\subsection{Correctness of $\B$: Proof of Lemma~\ref{lem:main_lemma_charac}}

There are a few things to show.
The interesting argument is the one that shows that $\B$ recognises $W$ (Lemma~\ref{lem:language-containement} below).
We should also prove there is no accepting run over words in $\Sigma^*\eps^\omega$, which will be done below as part of Lemma~\ref{lem:language-containement}.

\subparagraph*{$\B$ is a $k$-blowup of $\A$.}

We should prove the following.

\begin{claim}
    For all transitions $q \re{a:y} q'$ in $\A$, and any $m \in \{1,\dots,k\}$ there is some $m' \in\{1,\dots,k\}$ such that $(q,m) \re{a:y} (q',m')$ in $\B$.
\end{claim}

\begin{claimproof}
Let $q \re{a:y} q'$ be a transition in $\A$ and let $m \in \{1,\dots,k\}$.
Since $G$ is a $k$-blowup of $\A \times S$, for all edges $s\re y s'$ in $S$ there is $m' \in \{1,\dots,k\}$ such that $(q,m,s) \re a (q',m',s') \tin G$.
Although both proofs are similar, we distinguish two cases.
\begin{itemize}
    \item If $y=x$ is even.
    We prove that for all nodes $s_{< x}$ at level $x$, $(q,m)$ strongly $c$-dominates $(q',m')$ for some $m'$.
    Therefore the same is true in $t_x$ which implies the wanted result.
    We let $s_{> x}=0_{> x}$, the zero sequence in $\kappa^{(d-x)_\odd}$.
    Now for all $s'_{>x} \in \kappa^{(d-x)_\odd}$, it holds that $s_{< x} s_{> x} = s_{< x} 0_{> x} \re x s_{< x} s'_{>x}$ in $S$, so there is $m'$ such that $(q,m,s_{< x} s_{>x}) \re a (q,m',s_{< x} s'_{>x})$ in $G$ and thus also in $\overline G$; in this case say that $m'$ is good for $s'_{> x}$.
    
    Now we claim that if $m'$ is good for $\tilde s'_{> x} \geq s'_{> x}$, then it is also good for $s'_{> x}$.
    Indeed, we have $(q',s_{<x}\tilde s'_{> x}) \re \eps (q',s_{<x}s'_{> x})$ in $\A \times S$ therefore since $\eps$-transitions preserve the memory state in $G$ (Theorem~\ref{thm:structuration}) we have $(q',m',s_{<x} \tilde s'_{> x}) \re \eps (q',m',s_{<x} s'_{>x})$ in $G$ thus $(q,m,s_{<x}s_{> x}) \re a (q',m',s_{<x}s'_{> x})$ in $\overline{G}$.

    Therefore the sets $M'_{s'_{> x}}$ of $m'$ which are good for $s'_{> x}$ form a decreasing chain of non-empty subsets of $\{1,\dots,k\}$ and thus their intersection is non-empty: there is some $m'$ which is good for all $s'_{> x}$, as required.

    \item If $y=x+1$ is odd. 
    We now prove that for all nodes $s_{< x}$ at level $x$, $(q,m)$ weakly $c$-dominates $(q',m')$ for some $m'$.
    Let $s'_{> x} \in \kappa^{(d-x)_\odd}$.
    Then for any $s_{> x}$ such that $s_x > s'_x$, it holds that $s_{< x} s_{> x} \re {x+1} s'_{< x} s_{>x}$ in $S$, so there is some $m'$ such that $(q,m,s_{< x} s_{>x}) \re a (q,m',s_{< x} s'_{>x})$ in $G$.
    Hence there is some $m'$ such that, for cofinitely many $s_{> x}$, $(q,m,s_{< x} s_{>x}) \re a (q,m',s_{< x} s'_{>x})$ is an edge in $G$ and thus also in $\overline G$; say that such an $m'$ is good for $s'_{> x}$.

    Now, we claim that if $m'$ is good for $\tilde s'_{> x} \geq s'_{> x}$, then it is also good for $s'_{> x}$.
    Indeed, as in the first case, we have $(q',m',\tilde s_{<x}s'_{> x}) \re \eps (q',m',s_{<x}s'_{> x})$ in $G$ thus for cofinitely many $s_{> x}$ we have $(q,m,s_{< x}s_{>x}) \re a (q',m',s_{<x} s'_{> x})$ in $\overline{G}$.

    We conclude just as above. \claimqedhere 
\end{itemize}
\end{claimproof}


\subparagraph*{$\B$ is $k$-wise $\eps$-complete.}

We should prove the following claim.

\begin{claim}
    For every even $x$, memory state $m$ and states $q,q'$, either $(q,m) \re{\eps:x} (q',m)$ or $(q',m) \re{\eps:x+1} (q,m)$.
\end{claim}

\begin{claimproof}
    Assume that $(q,m) \re{\eps:x+1} (q',m)$ is not a transition in $\B$.
    Consider a node $s_{<x}$ at level $x$ in $T$: it has type $t_x$ and thus $(q,m)$ does not weakly $\eps$-dominate $(q',m)$ at $s_{<x}$.
    This rewrites as
    \[
        \exists s'_{> x} \forall s_{> x} \qquad (q,m,s_{< x} s_{>x}) \re \eps (q',m,s_{< x} s'_{>x}) \text{ is not an edge in } \overline G.
    \]
    Now since $\overline G$ is $\eps$-complete, we get 
    \[
        \exists s'_{> x} \forall s_{> x} \qquad (q',m,s_{< x} s'_{>x}) \re \eps (q,m,s_{< x} s_{>x}) \tin \overline G,
    \]
    therefore $(q',m)$ strongly $\eps$-dominates $(q,m)$ at $s_{<x}$, which concludes.
\end{claimproof}


\subparagraph*{$\B$ recognises $W$.}

We now turn to the more involved part.
We start by proving the following two technical lemmas.

\begin{lemma}\label{lem:technical1}
Assume that $(q,m) \re{c:y} (q',m') \tin \B$ for some $y$.
There is a map $f:T \to T$ such that for all $s \in T$, 
\begin{itemize}
    \item there is an edge $(q,m,f(s)) \re c (q',m',s)$ in $\overline G$; and
    \item it holds that $f(s)_{< y} = s_{< y}$.
\end{itemize}
\end{lemma}

\begin{proof}\textcolor{red}{TOPROVE 6}\end{proof}

\begin{lemma}\label{lem:technical2}
    Assume that $(q,m) \re{c:x} (q',m') \tin \B$ for some even $x$, and let $s_{< x}$ be a node at level $x$ in $T$.
    There is $s_{> x} \in \kappa^{(d-x)_\odd}$ such that $s_{< x}s_{> x} \in T$ and for any $s'_{> x} \in \kappa^{(d-x)_\odd}$, $(q,m,s_{< x}s_{> x}) \re c (q',m',s_{<x}s'_{> x})$ in $\overline G$.
\end{lemma}

\begin{proof}\textcolor{red}{TOPROVE 7}\end{proof}

We are now ready for the main argument.

\begin{lemma}\label{lem:language-containement}
    The language of $\B$ is contained in $W$.
    Moreover, there is no accepting run labelled by words in $\Sigma^* \eps^\omega$.
\end{lemma}

\begin{proof}\textcolor{red}{TOPROVE 8}\end{proof}

\po{todo: figure} 

\section{Proof of Theorem~\ref{thm:union} (Generalised Kopczyński's conjecture)}\label{app:unon}
We let $\A^\eps$ be the automaton defined in Section~\ref{sec:union} (i.e. a version of the cascade product of $\A_{1}^\eps \times \A_{2}^\eps$ with~$\T$). In this appendix we prove:
\AepsCompleteUnion*

First, we need a few remarks on the structure of $\eps$-complete automata.
We write $q \rer{\eps:x+1} q'$ to denote the conjunction of $q \re{\eps:x+1} q' $ and $q' \re{\eps:x+1} q$.
Given two states $q,q'$ in the same part of a $k$-wise $\eps$-complete automaton, we call breakpoint priority of $q$ and $q'$ the least even $\xbreak$ such that $q \rer{\eps:\xbreak+1} q'$ does not hold.
Note that this is a property of the unordered pair $\{q,q'\}$.
Observe also that by the definition of $\eps$-completeness, we have either $q \re{\eps:\xbreak} q'$ or $q' \re{\eps:\xbreak} q$. Moreover, assuming that $q \re{\eps:\xbreak} q'$, we also get that there can be no $q' \re{\eps:x} q$, for even $x$, otherwise we would accept some run labelled by $\Sigma^* \eps^\omega$; therefore for even $x \geq \xbreak$ we also have $q \re{\eps:x} q'$.
To sum up, if $\xbreak$ is the breakpoint priority of $\{q,q'\}$ and $q \re{\eps:\xbreak} q'$, then: 
\begin{itemize}
    \item $q\rer{\eps:y} q'$ for all odd $y<\xbreak$;
    \item $q\re{\eps:x} q'$ for all $x\geq \xbreak$; and
    \item there is no transition $q'\re{\eps:x} q$ for even $x$.
\end{itemize}
Finally, observe that in $\A_2^\eps$, since there are no transitions with priority $0$ (and therefore $\re{\eps:1}$ connects every ordered pair of states), breakpoint priorities are always $\geq 2$.


We are now ready to prove Lemma~\ref{lem:A-eps-complete-union}

\begin{proof}\textcolor{red}{TOPROVE 9}\end{proof}

This concludes the proof of Theorem~\ref{thm:union}. 
\section{Proof of Proposition~\ref{prop:1-2-player-counterexample} (No 1-to-2 player lift)}\label{app:no-lift}
\counterexampleLift*

\begin{proof}\textcolor{red}{TOPROVE 10}\end{proof} 
\end{document}