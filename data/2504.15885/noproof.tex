\documentclass[a4paper,UKenglish,cleveref, autoref, thm-restate, pdfa]{lipics-v2021}


\pdfoutput=1 \hideLIPIcs  

\usepackage{cases}
\usepackage{bm}
\usepackage{algorithm}
\usepackage{algpseudocode}


\newcommand{\mon}[1]{\textcolor{magenta}{\textbf{Mon:} #1}}
\newcommand{\ele}[1]{\textcolor{orange}{\textbf{Ele:} #1}}
\newcommand{\koppany}[1]{\textcolor{blue}{\textbf{Koppany:} #1}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\rev}[1]{\textcolor{blue}{#1}}

\bibliographystyle{plainurl}

\theoremstyle{plain}
\newtheorem{thm}{Theorem}  \newtheorem{lmm}{Lemma}  \newtheorem{prop}{Proposition}  


\title{Branch-and-Bound Algorithms as Polynomial-time Approximation Schemes}

\author{{Koppány István} Encz\footnote{Corresponding author}}{Faculty of Informatics, Università della Svizzera italiana, [CH 6962 Lugano, Switzerland] \and Istituto Dalle Molle di studi sull'intelligenza artificiale (IDSIA USI-SUPSI), [CH 6962 Lugano, Switzerland]}{enczk@usi.ch}{}{Supported by the Swiss National Science Foundation project n. $200021\_212929$ / 1 "Computational methods for integrality gaps analysis". Project code: 36RAGAP}

\author{Monaldo Mastrolilli}{Dipartimento Tecnologie innovative, Scuola universitaria professionale della Svizzera italiana [CH 6962 Lugano, Switzerland] \and Istituto Dalle Molle di studi sull'intelligenza artificiale (IDSIA USI-SUPSI) [CH 6962 Lugano, Switzerland]}{monaldo.mastrolilli@supsi.ch}{}{Supported by the Swiss National Science Foundation project n. $200021\_212929$ / 1 "Computational methods for integrality gaps analysis". Project code: 36RAGAP}

\author{Eleonora Vercesi}{Faculty of Informatics, Università della Svizzera italiana, [CH 6962 Lugano, Switzerland] \and Istituto Dalle Molle di studi sull'intelligenza artificiale (IDSIA USI-SUPSI), [CH 6962 Lugano, Switzerland] \and \url{https://eleonoravercesi.github.io/}}{eleonora.vercesi@usi.ch}{}{Supported by the Swiss National Science Foundation project n. $200021\_212929$ / 1 "Computational methods for integrality gaps analysis". Project code: 36RAGAP}

\authorrunning{K. I. Encz and M. Mastrolilli and E. Vercesi} 

\Copyright{Koppány István Encz, Monaldo Mastrolilli, and Eleonora Vercesi} 

\ccsdesc[500]{Theory of computation~Branch-and-bound}
\ccsdesc[500]{Theory of computation~Numeric approximation algorithms}
\ccsdesc[500]{Theory of computation~Scheduling algorithms} 

\keywords{Branch-and-bound algorithm, Polynomial-time approximation scheme, Parallel machine scheduling problem, Knapsack problem} 

\category{} 

\relatedversion{} 

\supplementdetails[subcategory = {Source code}]{Software}{https://github.com/eleonoravercesi/branch_and_bound_as_PTAS} 





\nolinenumbers 



\EventEditors{John Q. Open and Joan R. Access}
\EventNoEds{2}
\EventLongTitle{42nd Conference on Very Important Topics (CVIT 2016)}
\EventShortTitle{CVIT 2016}
\EventAcronym{CVIT}
\EventYear{2016}
\EventDate{December 24--27, 2016}
\EventLocation{Little Whinging, United Kingdom}
\EventLogo{}
\SeriesVolume{42}
\ArticleNo{23}


\begin{document}

\maketitle

\begin{abstract}
Branch-and-bound algorithms (B\&B) and polynomial-time approximation schemes (PTAS) are two seemingly distant areas of combinatorial optimization. We intend to (partially) bridge the gap between them while expanding the boundary of theoretical knowledge on the B\&B framework. Branch-and-bound algorithms typically guarantee that an optimal solution is eventually found. However, we show that the standard implementation of branch-and-bound for certain knapsack and scheduling problems also exhibits PTAS-like behavior, yielding increasingly better solutions within polynomial time. Our findings are supported by computational experiments and comparisons with benchmark methods. This paper is an extended version of a paper accepted at ICALP 2025.

\end{abstract}

\section{Introduction}\label{sec:intro}

Branch-and-bound algorithms (B\&B) have been a central part of combinatorial optimization for quite some time. They serve as a go-to method for several optimization problems both in theory and in practice, and many $\mathsf{NP}$-hard optimization problems are still frequently attacked by a variant of the branch-and-bound method or solvers having it at their core. Their great success could partially be attributed to the generality of the framework, which allows it to be applied to fundamentally different problems; along with the flexibility of choosing parameters such as the branching rule or the search strategy, resulting in an extensive list of options for exploiting the same underlying principles of the framework. For an introduction of the idea and a thorough description, we refer to Kohler and Steiglitz \cite{kohler_steiglitz}; whereas a survey of recent advancements regarding best practices of parameter tuning can be found in \cite{bnb_survey}.

Broadly speaking, the B\&B framework consists of iteratively refining a partition of the search space of a given combinatorial optimization problem. The process is commonly depicted with the help of an auxiliary tree, where the leaves of the tree (often called \emph{active nodes}) correspond to the currently considered partition classes. In this context, refining the partition by further dividing one class equates to creating child nodes for the corresponding leaf in the tree. In addition, each node in the tree has an attributed lower or upper bound (depending on the type of problem in consideration) on the objective value of the best solution in that particular partition class; typically they are calculated from a linear relaxation of the integer formulation of the problem. Occasionally, a node can be discarded from the search process if its attributed bound is worse than an already found solution. 

It is evident from the above description that the framework has various degrees of freedom. The key components that can be adjusted independently are the following:
\begin{itemize}
    \item \textbf{Branching}: The process of dividing a problem into sub-problems.
    \item \textbf{Bounding}: Calculating lower/upper bounds to limit the search space.
    \item \textbf{Selection/Search}: Choosing the next sub-problem (active node) to explore based on some kind of ranking.
\end{itemize}

The choice of these parameters could of course be influenced by the underlying problem. However, when it comes to traversing the branching tree, certain heuristics might be fixed a priori that do not depend on the nature of the problem. Common search heuristics include breadth-first search (BFS), depth-first search (DFS), or best-first search; the latter of which chooses the active subproblem with the best attributed lower/upper bound. 

Several attempts have been made to analyze the effect of preferring one selection strategy to another. For instance, Greenberg and Hegerich \cite{greenberg_hegerich} compare the DFS and the best-first methods applied to the knapsack problem. 
Moreover, with the increasing interest in applications of artificial intelligence, researchers have even deployed machine learning-based methods that try to \emph{learn} an optimal selection strategy (See, e.g., \cite{gp_learning}. See \cite{scavuzzo24} for a recent survey). However, these comparisons either rely on empirical evidence and rank strategies according to their practical performance or argue intuitively about the dominance of one method from a certain aspect: DFS is often considered the memory-efficient alternative, whereas the best-first search is often regarded as the ``intuitive'' best choice. To the best of our knowledge, little to no effort has been made to justify why one strategy outperforms another, given the problem type.

Meaningful applications of the B\&B framework mostly revolve around $\mathsf{NP}$-hard optimization problems, where a standard worst-case analysis does not illuminate the true power of algorithms. Instead, recent results focus on the average-case analysis, and study the performance on randomly sampled instances. Pataki, Tural, and Wong \cite{pataki_basis_2010} analyze the complexity of B\&B for the integer feasibility problem. They show that if the magnitude of the coefficients in the constraint matrix is sufficiently large, then, up to a reformulation technique, almost all instances can be solved directly at the root node. Recently, \cite{dey_branch-and-bound_2023} show that, with any branching rule and best-first as node selection, B\&B reaches the optimum with a polynomial number of nodes on randomly sampled instances, for a fixed number of constraints.
Some ``negative'' results have also been proposed. Dash \cite{dash_exponential_2005} showed an exponential lower bound on Branch and Cut for 0-1 integer programming when just a few families of cuts are enforced. Bell and Frieze \cite{bell_solving_2023} show that any B\&B method for the Asymmetric Traveling Salesman Problem via the assignment problem relaxation has an exponential number of nodes.
More recently, \cite{full_strong_branching} showed that for the Vertex Cover problem, choosing full strong branching as the variable selection rule can either perform exceptionally well or be exponentially worse than any other rule, depending on the class of instances considered.


The efficiency of B\&B appears to be strongly dependent on the problem at hand, as well as the choice of lower bound, branching rule, and node selection strategy. This makes it particularly interesting to investigate for which problems, under what conditions, and with which specific choices B\&B can be made to run in polynomial time.

In our current work, we endeavor to explain the intuitive advantage of the best-first search strategy by giving a worst-case theoretical analysis from a slightly unusual point of view. Namely, we will show that for the makespan minimization of unrelated parallel machine scheduling problem with a fixed number of machines (denoted by $Rm||C_{max}$ following standard three-field representation; see \cite{machine_scheduling_review}) and the multiple knapsack problem, the best-first strategy paired with other natural linear programming-based branching and bounding strategies yields a polynomial-time approximation scheme. Thus, practical observations regarding its superiority are strengthened by the guarantee that it is able to find fast a solution \emph{arbitrarily close to the optimum}.

\bigskip

Our contributions are as follows: first (Section \ref{sec:knap}), we consider a family of branch-and-bound algorithms with the best-first tree traversal rule for the multiple knapsack problem, and show that they form a polynomial-time approximation scheme, in which their execution time is constrained to be within polynomial bounds for any fixed approximation ratio. The family $A^{\text{knap}}_{\alpha}$ (parametrized by the approximation parameter $\alpha$) relies on a linear programming-based upper bound and a branching rule that exploits the specific structure of the linear program (see Proposition \ref{2_approx_knapsack}).

\begin{restatable*}{thm}{mk}\label{multi_knapsack_ptas}
    For every fixed $0< \alpha < 1$, the algorithm $A^{\text{knap}}_{\alpha}$ returns an $\alpha$-approximate solution to the multiple knapsack problem, after processing $O(n^{c_{\alpha}+1} \cdot m^{c_{\alpha}})$-many nodes in the branching tree for some constant $c_{\alpha}$ that depends on $\alpha$.
\end{restatable*}

Based on the same underlying idea, we provide (Section \ref{sec:js_ptas}) similar results for the makespan minimization of unrelated parallel machine scheduling problem \emph{with a fixed number of machines}, in which we prove that a certain B\&B algorithm is an efficient polynomial-time approximation scheme (EPTAS\footnote{A scheme is called EPTAS when, for an arbitrary $\epsilon$ and inputs of size $n$, its running time is $O(n^c \cdot f(1/\epsilon))$ for some constant $c$ that is independent from $\epsilon$.}) for $Rm||C_{max}$. The family $A^{\text{unrel}}_{\epsilon}$ again relies on a linear programming relaxation and its approximation property. The following theorem  easily implies a polynomial running time for any fixed error.

\begin{restatable*}{thm}{unrel}\label{unrel_machine_ptas}
    For every fixed $\epsilon > 0$, the algorithm $A^{\text{unrel}}_{\epsilon}$ returns a $(1+\epsilon)$-approximate solution to the unrelated machine scheduling problem, after processing at most $m^{\lfloor\frac{m^2}{\epsilon}\rfloor}$-many nodes in the branching tree.
\end{restatable*}

Exploration and exploitation are two fundamental concepts in search and optimization algorithms. Exploration searches diverse areas, while exploitation refines known good solutions for efficiency. A balance is crucial in algorithms like branch-and-bound to prevent slow convergence. In many branching points of the algorithm, there are decision ambiguities; meaning that (almost) indistinguishable subproblems keep reappearing in the process. In Section \ref{sec:js:fptas}, we demonstrate that if two nodes represent ``similar'' situations, it is not necessary to explore all such ``similar'' cases to obtain an approximate solution. Given a polynomial bound on the running time, we can put on hold the exploration of some nodes while prioritizing others that are not similar to already explored sub-problems, resulting in an improved exploration of the search space. Standard rounding techniques can be used to identify ``similar nodes''. For the special case of the \emph{uniform machine scheduling} problem with fixed number $m$ of machines (denoted by $Qm||C_{max}$, see \cite{machine_scheduling_review}), this enhances the diversification of the best-first search, resulting in better exploration of the search space and the achievement of a fully polynomial-time approximation scheme (FPTAS). We will denote the enhanced algorithm by $A^{\text{sim-prof}}_{\epsilon}$ and prove the following theorem:

\begin{restatable*}{thm}{fptas}\label{fptas}
    For every fixed $\epsilon > 0$, the algorithm $A^{\text{sim-prof}}_{\epsilon}$ returns a $(1+\epsilon)$ - approximate solution to the uniform machine scheduling problem, after processing at most $n\cdot \left(\frac{5n}{\epsilon}\right)^m$ nodes in the branching tree.
\end{restatable*}

To prove these results, we provide structural properties of the vertices of the corresponding polyhedra in Lemma \ref{vertex_char}. We continue with computational experiments supporting our theoretical results in Section \ref{sec:comp_exp}. Finally, with the hope of fueling future research, we list (Section \ref{sec:conc}) the fundamental properties of optimization problems that made our approach applicable. 
\section{A B\&B PTAS for the Multi-Knapsack Problem}\label{sec:knap}

In the one-dimensional $0-1$ multiple knapsack problem (referred to as \emph{multiple knapsack problem} or \emph{multi-knapsack problem}), we are given $n$ items characterized by weights $\bm{w} = (w_1, \ldots, w_n)$ and  profits $\bm{p} = (p_1, \ldots, p_n)$. Additionally, we have \( m \) knapsacks with capacities \( \bm{C} = (C_1, \ldots, C_m) \), where \( m \) is a fixed constant. The goal is to select a subset of items to maximize the total profit while ensuring that the weight constraints of each knapsack are satisfied. We assume that all weights, profits, and knapsack capacities are nonnegative integers.  

When $m$ is fixed, that is the case we are considering in this paper, the problem is weakly $\mathsf{NP}$-hard and admits an FPTAS \cite{ibarra_kim,lawler_fptas}.
For reference on a comprehensive theory of the problem, we mention the Martello-Toth book \cite{martello_toth}. A survey on recent improvements can be found in \cite{survey,MastrolilliH06}. 

The use of branch-and-bound algorithms for the knapsack problem is well-established. Notable early contributions include those by Kolesar \cite{kolesar}, Greenberg and Hegerich \cite{greenberg_hegerich}, and Horowitz and Sahni \cite{horowitz_sahni_knapsack} for the single-knapsack case. These approaches often leverage various heuristics, such as DFS or BFS search strategies and fractional-item pivot selection rules. However, most of these results focus primarily on computational experiments, with little emphasis on formal theoretical guarantees.

In this work, we demonstrate that a ``standard'' branch-and-bound implementation for the multi-knapsack problem naturally yields a PTAS. Specifically, at each node, the \textbf{bounding} step involves solving the linear programming relaxation known as \emph{surrogate relaxation} (see \cite{martello_toth}, Chapter 6). 
We then apply the standard \textbf{rounding} technique of Dantzig \cite{dantzig} to obtain an $(m+1)$-approximate feasible solution, and \textbf{branch} according to the most profitable fractional item. The \textbf{selection} strategy is the best-first rule, where we choose the node to be processed next whose upper bound (the fractional optimum) is the greatest; we will denote this strategy by GUB in the experimental section. We terminate whenever the ratio between the global upper bound and the best integer solution reaches or goes above a fixed constant $\alpha\in (0,1)$. The full details of our algorithm, \( A^{\text{knap}}_{\alpha} \), are provided in Section \ref{sec:knap_proof}, along with a proof of the following result:

\mk

For completeness, we sketch a pseudocode of $A^{\text{knap}}_{\alpha}$ in Appendix \ref{app:pseudocode}.

\subsection{Proof of Theorem \ref{multi_knapsack_ptas}}\label{sec:knap_proof}

In this section, we demonstrate that a ``standard'' branch-and-bound implementation for the multi-knapsack problem naturally yields a PTAS. We will need the standard integer programming formulation of the problem:
\begin{equation}\label{knapsack_ip}
    MK_m(\bm{C}, \bm{w}, \bm{p}) : \max \sum\limits_{j=1}^n \sum\limits_{i = 1}^m p_j \cdot x_{j,i} \,\,\, \text{ s.t.}
\end{equation}

\begin{subnumcases}{}
    \sum\limits_{j=1}^n w_j \cdot x_{j,i} \le C_i, & $i \in [m]$, \label{cap_const}\\
    \sum\limits_{i=1}^m x_{j,i} \le 1, & $j \in [n]$, \label{ass_const}\\
    x_{j,i} \in \mathbb{N}, & $j \in [n],\, i \in [m]$, \label{nonneg_const}
  \end{subnumcases}

For the \textbf{Branching} and \textbf{Bounding} components, we will rely on a relaxation of \eqref{knapsack_ip}--\eqref{nonneg_const}. Several such relaxations are discussed in \cite{martello_toth}; the ones that are relevant to us are the \emph{linear programming} relaxation (\eqref{knapsack_ip}, \eqref{cap_const}, \eqref{ass_const}, and non-negativity constraints instead of \eqref{nonneg_const}) and the \emph{surrogate relaxation}, which in essence merges the $m$ knapsacks into one single knapsack with capacity $\sum\limits_{i=1}^m C_i$:

\begin{align}\label{knapsack_surrogate}
    S\text{\rev{-}}MK_m(\bm{C}, \bm{w}, \bm{p})= MK_1 \left(\sum\limits_{i = 1}^m C_i, \bm{w}, \bm{p}\right)
\end{align}

Martello and Toth show in \cite{martello_toth} that the optimum of the linear relaxation of the surrogate relaxation coincides with the optimum of the linear relaxation of the original problem. Using this relationship and the well-known observation of George Dantzig \cite{dantzig}, they describe an algorithm that returns an $(m+1)$-approximate solution. They sort the $n$ items decreasingly by their unit profit $\frac{p_i}{w_i}$, and greedily fill each knapsack in this order until an item no longer fits inside entirely. Then they cut the excessive part and assign it to the next knapsack, and resume the process on this new knapsack with the next item in the queue. Let $\bm{x}^*$ denote this optimal solution to the linear relaxation, and let $s_1, \ldots, s_m$ denote the (at most) $m$ items that are fractionally assigned in the process. Item $s_k$ is referred to as the \emph{critical item relative to knapsack $k$}, and is obtained as $s_k = \min \left\{j: \sum \limits_{l=1}^j w_l > \sum\limits_{i=1}^k C_i\right\}$.

They conclude that the (at most) $m$ individual critical items and the collection of integrally assigned items yield (at most) $m+1$ feasible solutions to the integer program, the best of which has a profit of at least $\frac{1}{m+1}$ times the fractional optimum. Let $\bm{x'}$ denote the most profitable of these $m+1$ assignments. With these notations, we have that
\begin{prop}[Martello, Toth; \cite{martello_toth}]\label{2_approx_knapsack}
    \[
    \bm{p}\cdot \bm{x}' = \max\left\{p_{s_1}, \ldots, p_{s_m}, \sum\limits_{k=1}^{m}\sum\limits_{j = s_{k-1}+1}^{s_k -1} p_j\right\} \ge \frac{1}{m+1}(\bm{p} \cdot \bm{x}^*).
    \]
\end{prop}

In the analysis of the algorithm $A^{\text{knap}}_{\alpha}$, we are going to need the following simple observation, which connects the profit of the best critical item $j^*$ (i.e. the critical item with the highest profit) with the gap of $\bm{x}'$ with respect to the optimal $\bm{x}^*$:

\begin{lmm}\label{knapsack_gap_and_critical_element}
     \[
     \frac{p_{j^*}}{\bm{p}\cdot \bm{x}^*} \ge \min\left\{\frac{1}{m+1}, \,\frac{1}{m} \cdot \left(1-\frac{\bm{p}\cdot \bm{x}'}{\bm{p}\cdot \bm{x}^*}\right)\right\}.
    \]
\end{lmm}

For a fixed $0 <\alpha < 1$,  the specifications of algorithm $A^{\text{knap}}_{\alpha}$ are as follows: as input, we have a triple $(\bm{C}, \bm{w}, \bm{p})$ defining an instance of the multi-knapsack problem. At each step, we select a node $v$ (the \emph{branching node}) among the leaves (the \emph{active nodes}) of a tree we build step-by-step; each node corresponds to a subproblem in which we fix some variables that are given by the unique path from the root to the node. The \textbf{selection} in our case occurs according to the best-first strategy, where we select the node to be processed next whose attributed upper bound (described later) is the largest of all active nodes. For convenience, let us introduce the ``dummy'' variables $\bar{x}_j = 1-\sum\limits_{i=1}^m x_{j,i}$ for $j = 1, \ldots, n$. Suppose that in the unique path from the root of the tree to $v$, we have fixed $x_{j_1, i_1} = x_{j_2, i_2} = \ldots = x_{j_k, i_k} = 1$, and $\bar{x}_{e_1} = \bar{x}_{e_2} = \ldots = \bar{x}_{e_l}=1$. In other words, items $j_1, \ldots, j_k$ are set to be included in knapsack $i_1, \ldots, i_k$ respectively; whereas items $e_1, \ldots, e_l$ are completely disposed of. Consequently, node $v$ encodes the knapsack sub-problem on the ground set $S=[n]\setminus \left(\bigcup_{z=1}^k j_z \cup \bigcup_{z=1}^l e_z\right)$ given by $(\bm{C}_v, \bm{w}_v, \bm{p}_v)$ with $\bm{C}_v = (C'_1, \ldots, C'_m)$ where $C'_i = C_i-\sum\limits_{z:\, i_z = i} w_{j_z}, \,\,\, i \in [m]$; $\bm{w}_v=\bm{w}|_{S}$ and $\bm{p}_v=\bm{p}|_{S}$. 

In the \textbf{bounding} component, a local upper and lower bound $U(v)$ and $L(v)$ is determined in the following manner: we consider the appropriate integer program in \eqref{knapsack_ip} with parameters $(\bm{C}_v, \bm{w}_v, \bm{p}_v)$, and its surrogate relaxation. The linear relaxation of \eqref{knapsack_surrogate} is solved to optimality by Dantzig's method giving $\bm{x}^*$, and the $(m+1)$-approximate integer solution $\bm{x}'$ is obtained according to Proposition \ref{2_approx_knapsack}. We save the subproblem optimum and feasible solution into the variables $SU(v)=\bm{p}_v\cdot \bm{x}^*$ and $SL(v)=\bm{p}_v\cdot \bm{x}'$; and from these, we create a feasible solution and a local upper bound \emph{to the original problem} by putting items $j_1, \ldots, j_k$ back in knapsacks $i_1, \ldots, i_k$, respectively. Therefore, we set $U(v) = \bm{p}_v\cdot \bm{x}^* + (p_{j_1}+\ldots p_{j_k})$, and $L(v)= \bm{p}_v\cdot \bm{x}' + (p_{j_1}+\ldots p_{j_k})$.

Next, we expand the current tree by creating $m+1$ new subproblems that are going to be represented by the children of $v$; this \textbf{branching} rule is determined by setting the best critical item $j^*$ as the pivot element. For $i<m+1$, the $i$-th new branch is identified by fixing $x_{j^*, i} = 1$, and corresponds to putting the best critical item $j^*$ in knapsack $i$ while reducing its capacity by $w_{j^*}$. The $(m+1)$-th new branch corresponds to setting $\bar{x}_{j^*} =1$ (and $x_{j^*, 1} = \ldots = x_{j^*, m}=0$ at the same time), and means that we exclude item $j^*$ from all of the knapsacks. We calculate the pertaining upper and lower bounds of all $m+1$ sub-problems. If any of them are infeasible (including the case when item $j^*$ does not fit into knapsack $i$ for some $i$), or their upper bounds are lower than the value of an already found feasible integer solution, we prune the corresponding branch. Otherwise, we add them to the set of active nodes while removing $v$. We update the highest local upper bound ($GU$) and the best integer solution found so far ($GL$). Finally, we terminate whenever the multiplicative gap between the global upper bound and the best solution found so far reaches or goes above $\alpha$; i.e. $\frac{GL}{GU} \ge \alpha$.

Since processing a node $v$ clearly takes polynomial time, a polynomial upper bound on the number of visited nodes in the branching tree means that the above scheme is a PTAS. 

\begin{proof}\textcolor{red}{TOPROVE 0}\end{proof}

\begin{proof}\textcolor{red}{TOPROVE 1}\end{proof}

\begin{remark}
    With a more careful analysis, we can determine an exact bound on $l(\alpha, n)$ that depends on alpha, just like we did in Theorems \ref{unrel_machine_ptas} and \ref{fptas}. In particular, observe that \eqref{alpha} implies $\alpha > f_{l(\alpha, n)}$, and assume that in Lemma \ref{f}, the minimum is obtained by $\frac{1}{m}\cdot (1-\alpha)$. It follows that 
    \[
    \alpha > \frac{(l(\alpha,n)-1)\cdot \frac{1}{m}\cdot (1-\alpha)}{1+(l(\alpha,n)-1)\cdot \frac{1}{m}\cdot (1-\alpha)} = 1- \frac{1}{1+(l(\alpha,n)-1)\cdot \frac{1}{m}\cdot (1-\alpha)},
    \]
    and 
    \[
    1+(l(\alpha,n)-1)\cdot \frac{1}{m}\cdot (1-\alpha) < \frac{1}{1-\alpha},
    \]
    so
    \[
    l(\alpha,n) < \left(\frac{1}{1-\alpha} -1\right)\cdot \frac{m}{1-\alpha} + 1= \frac{m\alpha}{(1-\alpha)^2}+1.
    \]
    On the other hand, if the minimum in Lemma \ref{f} is achieved by $\frac{1}{m+1}$, a similar analysis shows that 
    \[
    l(\alpha, n) < \frac{m+1}{1-\alpha} + 1.
    \]
\end{remark}
 
\section{A B\&B EPTAS for the Unrelated Machine Scheduling Problem}\label{sec:js_ptas}

In the \emph{unrelated parallel machine scheduling} problem, $n$ jobs are assigned to $m$ machines to minimize the \emph{makespan} $\max\{C_S(i) : i = 1, \dots, m\}$, where $C_S(i)$ is the completion time of machine $i$ according to the schedule $S$. Each job $j$ has a machine-dependent processing time $p_{j, i} \in \mathbb{N}$. The problem is strongly $\mathsf{NP}$-complete when $m$ is part of the input \cite{garey_johnson_1}, ruling out an FPTAS unless $\mathsf{P} = \mathsf{NP}$. Furthermore, even a PTAS would imply $\mathsf{P} = \mathsf{NP}$ \cite{lenstra_shmoys_tardos}. When $m$ is a fixed constant, the problem is denoted by $Rm || C_{\max}$ (following \cite{machine_scheduling_review}), and an FPTAS is possible \cite{horowitz_sahni_job_fptas,grouping_1}. See, e.g. \cite{job_survey} for a survey of more recent advances. Several applications of the B\&B framework were developed for machine scheduling problems, with diverse lower bound strategies ranging from surrogate relaxations \cite{van_de_velde} to lagrangian relaxations \cite{martello_soumis_toth}.  

In this section, we study a B\&B implementation for $Rm || C_{\max}$.
Again, we follow a simple and standard implementation of the B\&B framework. At a given node, the corresponding sub-problem is modeled as an integer program. Its LP-based relaxation is solved by a Binary Search (BS) subroutine in the \textbf{bounding} component, followed by a common rounding technique to determine an $(m+1)$-approximate schedule (see \cite{vazirani}, Chapter $17.3$). Then, we \textbf{branch} according to the fractional job that maximizes the minimal processing time $\min \{p_{j, i}: i \in [m]\}$. The \textbf{selection} strategy is again the best-first rule, where the active node with the lowest fractional optimum (denoted by the acronym LLB in the experiments) is picked for processing. We stop whenever the ratio between the global lower bound and the best integer schedule discovered so far reaches or goes below $(1+\epsilon)$, where $\epsilon>0$ is a fixed constant. We provide exact details of the algorithm $A^{\text{unrel}}_{\epsilon}$ in Subsection \ref{sec:js_ptas_proof}, where we show the following:

\unrel
 
\subsection{Proof of Theorem \ref{unrel_machine_ptas}}\label{sec:js_ptas_proof}

In this section, we consider the problem $Rm||C_{max}$. Our analysis will rely on a \emph{self-similarity} property of the problem; that is, each node of the branching tree must correspond to the same class of LP-formulations. For the sake of exploiting this property, we need that fixing a job to any of the machines should yield another machine scheduling problem. This is not true for the default description, so we introduce the concept of \emph{overheads} denoting the earliest time machines can start completing jobs. Fixing a job $j$ to machine $i$ now corresponds to increasing the overhead of machine $i$ with $p_{j, i}$. Machine $i$ with an overhead $t_i \in \N$ has a completion time $C_S'(i)= t_i + C_S(i)$.

Again, the \textbf{bounding} and \textbf{branching} components will heavily rely on an integer programming formulation of the problem and its linear relaxation. The most straightforward formulation, however, gives rise to some concerns. Instead, we opt to follow in the footsteps of Vazirani \cite{vazirani} and Lenstra, Shmoys, and Tardos \cite{lenstra_shmoys_tardos}. Their proofs rely on a technique called \emph{parametric pruning}, which consists of a binary search for a ``guess'' on the optimal integer makespan while disregarding job-machine pairings that immediately exceed the current guess. For this purpose, they define the following modification of the ``standard'' program: for a given $\bm{P} \in \mathbb{N}^{n\times m}$ and ``guess'' $T \in \mathbb{N}$, let $S_T$ be the set of (job, machine) pairings which do not immediately violate the time limit $T$.
\[
S_T := \{(j,i): p_{j,i} \le T\}.
\]

\begin{definition}
    For $\bm{P} \in \mathbb{N}^{n\times m}, \,\, T \in \mathbb{N}$ and $\bm{t} \in \mathbb{N}^m$, let $PARTIAL\text{-}LP\text{-}MS_m(\bm{P}, \bm{t}, T)$ be the polyhedron determined by the following set of inequalities:
    \begin{align}\label{partial_machine_scheduling_lp}
        \begin{cases}
            \sum\limits_{i: (j,i) \in S_T} x_{j,i} = 1, & j\in [n], \\
            \sum\limits_{j: (j,i) \in S_T} p_{j,i} \cdot  x_{j,i} \le T-t_i, & i \in [m], \\
            x_{j,i} \ge 0, & (j,i) \in S_T.
        \end{cases}
    \end{align}
    \label{def:binary_search_poly}
\end{definition}

The key properties described in \cite{lenstra_shmoys_tardos} and \cite{vazirani} are easily transcribed to our version with overheads with little to no modification, since they are only dependent on the constraint matrix describing \eqref{partial_machine_scheduling_lp} and not on the right-hand side of the inequalities. Let us recall that for a feasible solution $\bm{x}$, job $j$ is called \emph{fractional} if there exists $i$ such that $x_{j,i}$ does not equal $0$ or $1$ (and therefore has a fractional value); otherwise job $j$ is called \emph{integral}.

\begin{lmm}[Lenstra, Shmoys, Tardos; \cite{lenstra_shmoys_tardos}]\label{rounding}
    If the linear program described in \eqref{partial_machine_scheduling_lp} is feasible, then each vertex $\bm{x}^*$ has at most $m$ fractional jobs. Furthermore, there exists an injection from fractional jobs to the $m$ machines such that each fractional job $j$ is matched to a machine $i$ where $x_{j,i}\ne 0$. Moreover, the schedule we get by keeping integral jobs in $\bm{x}^*$ and reassigning fractional jobs to machines according to the injection has a makespan of at most $2T$.
\end{lmm}

Lenstra et al. designed a binary search procedure (starting from an arbitrary integer schedule) for the smallest integer value of $T$ for which the program in \eqref{partial_machine_scheduling_lp} is feasible. They prove that their procedure runs in polynomial time.

\begin{prop}[Lenstra, Shmoys, Tardos; \cite{lenstra_shmoys_tardos}]\label{2_approx_machine}
    Let $T'$ be the result of the binary search; i.e. the smallest integer $T$ for which \eqref{partial_machine_scheduling_lp} is feasible. Furthermore, let $T_{opt}$ be the fractional optimum. Then the rounding procedure from Lemma \ref{rounding} applied to a schedule with makespan $T'$ yields an integer schedule with makespan at most $2T_{opt}$.
\end{prop}

For a fixed $\epsilon > 0$, the specifications of algorithm $A^{\text{unrel}}_{\epsilon}$ are as follows: as input, we have a matrix $\bm{P} \in \N^{n \times m}$ defining an instance of the unrelated machine scheduling problem. The overhead at the beginning is $\bm{t}\equiv 0$. At each step, we select a node $v$ (the \emph{branching node}) among the leaves (the \emph{active nodes}) of a tree we build step-by-step; each node corresponds to a subproblem in which we fix some job-machine pairings identified by the unique path from the root to the node. The \textbf{selection} in our case occurs according to the best-first selection rule, where we select the node to be processed next whose attributed lower bound (described later) is the smallest of all active nodes. Suppose that in the unique path of length $k$ from the root of the tree to $v$, we have fixed $x_{j_1, i_1} = x_{j_2, i_2} = \ldots = x_{j_k, i_k} = 1$. In other words, job $j_1$ is fixed to machine $i_1$, job $j_2$ is fixed to machine $i_2$, and so on. Consequently, node $v$ encodes the sub-problem with jobs $S=[n]\setminus \bigcup_{z=1}^k j_z$ given by processing times $\bm{P}_v=\bm{P}|_{S \times [m]}$ and overhead vector determined by the already fixed job-machine pairings: $\bm{t}_v= (t_1, \ldots, t_m)$ with $t_i= \sum\limits_{z: i_z = i} p_{j_z, i_z}, \,\, i \in [m]$. With these, the \textbf{bounding} takes place: a local lower and upper bound $L(v)$ and $U(v)$ is determined by applying the binary search of Lenstra et al. to find the smallest integer $T$ (denoted by $T'$) for which \eqref{partial_machine_scheduling_lp} is feasible, and by rounding a vertex of the corresponding polyhedron $PARTIAL\text{-}LP\text{-}MS_m(\bm{P}_v, \bm{t}_v, T')$ to an integer assignment with makespan at most $(m+1)\cdot T'$. The rounding consists of assigning each fractional job to the machine where its processing time is minimal. We then \textbf{branch} according to the fractional job $j$ whose minimal processing time ($\min\{p_{j,i}: i\in [m]\}$) is maximal. Branch $i$ out of the $m$ new branches fixes job $j$ to machine $i$ and increases its overhead by $p_{j, i}$. 

We calculate the local lower and upper bounds of all $m$ new subproblems. If their lower bounds are greater than the makespan of an already found integer solution, we \textbf{prune} them. Otherwise, we add them to the set of active nodes while removing $v$. We update the global lower and upper bounds $GL$ and $GU$: at a given step, they are defined as the minimal local lower bound of the active nodes and the best makespan of an integer solution found so far, respectively. Finally, we terminate whenever the multiplicative gap between the global lower bound and the current champion makespan, $\frac{GU}{GL}$, reaches or goes below $1+\epsilon$.







\begin{proof}\textcolor{red}{TOPROVE 2}\end{proof}
 
\section{A B\&B FPTAS for the Uniform Machine Scheduling Problem}\label{sec:js:fptas}

In the \emph{identical machine scheduling} setup, every job takes the same amount of time to complete on each of the machines, and so job $j$ is associated with a single processing time $p_j$. The \emph{uniform machine scheduling} setup extends this by associating a speed $s_i$ with each machine $i \in [m]$, and thus rendering the processing time of job $j$ on machine $i$ to be $p_{j, i} = \frac{p_j}{s_i}$. In this paper, we assume that the vector of processing times $\bm{p}$ and the vector of speeds $\bm{s}$ are such that $p_{j,i} \in \mathbb{N}, \forall j \in [n], \forall i \in [m]$. The problem is frequently denoted by $Qm||C_{max}$ when $m$ is a constant (following \cite{machine_scheduling_review}); it is weakly $\mathsf{NP}$-hard and, being a special case of $Rm||C_{max}$, it admits an FPTAS.

In this section, we enhance $A^{\text{unrel}}_{\epsilon}$ for the special case $Qm||C_{max}$ by exploiting a simple observation. During the course of the algorithm, certain repetitive patterns can be identified based on the jobs that are fixed at a given node. Situations may arise where two or more nodes encode sub-problems that could be classified as similar; provided that the fixed job-machine pairings yield schedules that are sufficiently close to each other (the coordinate-wise distance of schedules required to declare them similar is based on the error tolerance factor $\epsilon$). When aiming for an approximate solution, the processing of these similar nodes can be delayed indefinitely. When the modified B\&B runs its full course, the returned schedule is either optimal or it is within a range of $\epsilon$ of the real optimal solution that we put on hold due to similarity.

As we will see, this modified scheme $A^{\text{sim-prof}}_{\epsilon}$ (which, apart from the enhanced node selection rule, is equivalent with $A^{\text{unrel}}_{\epsilon}$) reduces the search space by such a great factor that the running time will be polynomial in $1/\epsilon$ as well.

\fptas

Apart from its increased efficiency, the scheme offers the additional advantage of adaptability to changing requirements and time constraints. If the algorithm completes before its designated time limit or a higher-quality solution is needed, it can resume processing the delayed nodes, further refining the search space using a more precise similarity measure with a smaller $\epsilon$.

 
\subsection{Proof of Theorem \ref{fptas}}\label{sec:js_fptas_proof}

In this section, we consider the problem $Qm||C_{\text{max}}$, and we will enhance the previous algorithm $A^{\text{unrel}}_{\epsilon}$ by exploiting common input-modifying techniques that are frequently used for obtaining fully polynomial-time approximation schemes. In general, these techniques consist of applying a series of transformations on the input instance, while keeping the objective value sufficiently close to the optimum. Most often, the modifications are a mixture of rounding down processing times to the nearest value of some finite sequence, and grouping small jobs together to reduce the number of jobs in the input. The rounding of processing times allows for greater control on feasible solutions and gives way to create \emph{profiles} that collect equivalent schedules. On the other hand, grouping small jobs together results in a smaller instance for which even a complete enumeration of schedules would be feasible. If the parameters of the modification are chosen carefully, the combination of these two steps guarantees an algorithm that runs in polynomial time in both $n$ and $\frac{1}{\epsilon}$. For a detailed background, we refer to \cite{grouping_1} and \cite{grouping_2}.

Let us fix $\epsilon > 0$, and consider an input $\bm{P} \in \mathbb{N}^{n \times m}$ to the uniform machine scheduling problem with $m$ fixed machines and $n$ jobs. For the sake of a simpler analysis, let us divide each processing time with the global fractional optimum of the ``standard'' LP-relaxation. This step does not affect the optimal integer assignment, and its new makespan $T_{\text{opt}}$ satisfies that 
\[
1 \le T_{\text{opt}} \le 2.
\]

In our current investigation, we will solely rely on the first type of modification: rounding down processing times to the nearest value of some sequence. But, instead of directly modifying the input, we will design a scheme that allows us to obtain the same effect without touching the input first, thus guaranteeing a more ``natural'' approach. Our method builds on the concept of  
\emph{profiles}: for a (partial) assignment $S$ of some jobs, the profile of $S$ is the $m$-tuple $(C_S(1), \ldots, C_S(m))$ of completion times. We call two profiles $\Pi(S_1)$ and $\Pi(S_2)$ similar if $|\Pi(S_1)_i - \Pi(S_2)_i| \le \frac{\epsilon}{n}, \,\,\,\forall i \in [m]$. The key observation is the following: let $\epsilon < 1$, and note that a $(1+\epsilon)$-approximate solution has a makespan of at most $2(1+\epsilon)= 2 + 2\epsilon$. Consider the $m$-dimensional cube $[0,2+2\epsilon]^m$, and consider its partition given by the set of points $[0,\frac{\epsilon}{n}, \frac{2\epsilon}{n}, \ldots, \frac{n(2+2\epsilon)}{\epsilon}\cdot \frac{\epsilon}{n}]^m$. If we have two profiles falling into the same partition class, then they are similar. Conversely, any set of profiles with makespan at most $2+2\epsilon$ that does not have $2$ similar profiles has at most $\left(1+\frac{n(2+2\epsilon)}{\epsilon}\right)^m \le \left(\frac{5n}{\epsilon}\right)^m$ elements.

For a node $v$ in the branching tree, its profile $\Pi(v)$ is defined as the profile of the partial schedule made up of the jobs fixed at $v$. In other words, the profile is simply the overhead vector associated with the integer programming formulation corresponding to the sub-problem at $v$: $\Pi(v) = \bm{t}_v$. The concept of similar profiles allows us to consider nodes of the branching tree ``equivalent'' if they have similar profiles, and they have the same set of jobs fixed so far. Note that we need \emph{both} the same profiles and the same fixed jobs in order to declare two nodes equivalent, as shown by the following identical instance with $2$ machines given by processing time $(n,n,1,\ldots, 1)$ with $n$-many $1$-jobs. We can have two partial assignments with the same profile $(n,n)$, but one of them is made up of one $n$-job and $n$ $1$-jobs while the other is made up of two $n$-jobs. It is not justified to deem them equivalent as the best extension of the first profile has a makespan of $2n$, while the latter can be extended to a schedule with makespan $\frac{3}{2}n$.

However, the following Lemma gives a natural way to ensure that all nodes at a given level have the same fixed jobs in the uniform setup.

\begin{lmm}\label{longest_frac}
   Let $({\bm{P}}, \bm{t}) \in (\mathbb{N}^{(n\times m)}, \mathbb{N}^m)$ be an instance of the uniform machine scheduling problem with $n$ jobs where $\bm{P}$ is given by processing times $\bm{p} \in \mathbb{N}^n$ and machine speeds $\bm{s}\in \mathbb{N}^m$, and let $n$ be the job whose processing time is maximal. Let $T'$ denote the smallest integer $T$ for which $PARTIAL-LP-MS_m (\bm{P}, \bm{t}, T)$ is feasible. If there exists a schedule $\bm{x}^*$ with at least one fractional job such that $\bm{x^*}$ is a vertex of $PARTIAL-LP-MS_m (\bm{P}, \bm{t}, T')$, then there exists a schedule $\bm{\hat{x}}$ in which job $n$ is fractional and $\bm{\hat{x}}$ is a vertex of the same polyhedron.
\end{lmm}

In the proof of Lemma \ref{longest_frac}, we will exploit useful properties of vertices of the polytope described in \eqref{partial_machine_scheduling_lp}. Namely, in the uniform machine scheduling model, we can extend the result of Lemma \ref{rounding} and characterize vertices of the polyhedra. The basic idea of the lemma is the following: for a feasible solution $\bm{x}$, they construct a bipartite auxiliary graph $G(\bm{x})$ with the $2$ classes corresponding to the $m$ machines and the at most $m$ fractional jobs, and they add an edge between $ i \in [m]$ and $j \in [n]$ if $x_{j, i} >0$ and is fractional. They conclude that $G(\bm{x})$ must be a \emph{pseudo-forest}, and use this fact to construct a matching between machines and fractional jobs. 

We can strengthen their observation in the uniform model, and use it to our advantage for characterizing vertices of the corresponding polyhedra. Let $(\bm{p}, \bm{s}) \in (\mathbb{N}^{n}, \mathbb{N}^{m})$ denote an input to the uniform machine scheduling problem with $\bm{p}$ being the vector of processing times, and $\bm{s}$ being the vector of machine speeds. The corresponding input matrix is $\bm{P} = (p_{j,i})_{i,j = 1, 1} ^{m, n}$ with $p_{j,i} = \frac{p_j}{s_i}$. Recall that $\bm{P} \in \mathbb{N}^{n \times m}$ is assumed, although it is not explicitly used in the proof. Let us recall that a machine's completion time according to some schedule $S$ is denoted by $C_S'(i)$ when taking into account overhead $t_i$ as well. With a little abuse of notation, a fractional solution $\bm{x}$ of the linear program can be interpreted as a fractional schedule, where the completion time at machine $i$ is denoted by $C_{\bm{x}}'(i)$.

\begin{lmm}\label{vertex_char}
    Let $(\bm{P}, \bm{t})$ be an input to the uniform machine scheduling problem, and let $T'$ denote the smallest integer $T$ for which \eqref{partial_machine_scheduling_lp} is feasible; let $\bm{x}$ be a feasible solution of $PARTIAL-LP-MS_m (\bm{P}, \bm{t}, T')$. Then $\bm{x}$ is a vertex if and only if these two conditions hold: (i) $G(\bm{x})$ is a forest, and (ii) each connected component of $G(\bm{x})$ contains at most one machine-node $i$ for which $C_{\bm{x}}'(i)<T'$.
\end{lmm}

\begin{proof}\textcolor{red}{TOPROVE 3}\end{proof}














\begin{proof}\textcolor{red}{TOPROVE 4}\end{proof}

With this, we are ready to define our final enhanced algorithm $A^{\text{sim-prof}}_{\epsilon}$. It takes as input an instance of the uniform machine scheduling problem $(\bm{P}, \bm{0})$ where $\bm{P}$ is given by $(\bm{p}, \bm{s}) \in \mathbb{N}^{n+m}$. It rearranges the jobs such that $p_1 \ge \ldots \ge p_n$, then creates an equivalent instance $\bm{P}'$ by dividing $\bm{P}$ with the global fractional optimum. Then it proceeds as a branch-and-bound algorithm with the following specifications: when processing a node $v$, it first finds a vertex of the corresponding relaxation of the sub-problem \eqref{partial_machine_scheduling_lp} with the smallest $T$ for which the program is feasible. If the vertex is  integer, the algorithm stops, as it has found a globally optimal schedule (due to the best-first selection criterion). If the vertex is fractional and the longest unfixed job is not fractional, then it follows Lemma \ref{longest_frac} to arrive at another optimal vertex in which the longest unfixed job is fractional. Then, it rounds up this vertex to find an integer solution, according to an arbitrary matching between machines and fractional jobs. The pivot element at node $v$ will be the longest fractional job, which by now coincides with the longest unfixed job. $m$ new branches are created, labelled by the machine on which the longest unfixed job is fixed at the next level. For each new node $u$, the algorithm checks whether it has already found a schedule with a makespan better than $L(u)$, in which case $u$ is discarded. Next, $\Pi(u) = \bm{t}_u$ is compared with all previous profiles at the same depth. If $\max\{\Pi(u)_i: i \in [m]\} > 2+2\epsilon$, or there already exists a node at the same depth whose profile is similar to $\Pi(u)$, $u$ is discarded. The remaining of the $m$ new nodes are added to the list of active nodes, the list of profiles is appended with the new ones, and the next node to process is selected according to the best-first tree traversal rule.

The process terminates when the ratio between the makespan of the best discovered schedule and the lowest lower bound satisfies $\frac{GU}{GL} \le 1+ \epsilon$, at which point it returns the best schedule found so far.

\begin{lmm}\label{fptas_size}
    Let $F$ be the final branching tree traversed by algorithm $A^{\text{sim-prof}}_{\epsilon}$. The number of nodes in $F$ is at most $n\cdot \left(\frac{5n}{\epsilon}\right)^m$.
\end{lmm}

\begin{proof}\textcolor{red}{TOPROVE 5}\end{proof}

\begin{proof}\textcolor{red}{TOPROVE 6}\end{proof}

In what follows we show how the algorithm $A^{\text{sim-prof}}_{\epsilon}$ generalizes a dynamic programming approach for the machine scheduling problem. The latter consists of constructing a matrix $\bm{M}$, where the $n$ rows are labeled by the $n$ jobs in some fixed order. Column $i$ pertains to a representative profile $\Pi(i)$ of the partition classes of the cube $[0,2+2\epsilon]^m$ given by points $[0, \frac{\epsilon}{n}, \ldots, \frac{n(2+2\epsilon)}{\epsilon}\cdot \frac{\epsilon}{n}]^m$. The entry at column $i$ and row $j$ is $1$ if there exists a partial assignment with the first $j$ jobs whose profile is similar to $\Pi(i)$; otherwise, the entry is $0$. The algorithm fills in the entries of the matrix by the best-first principle, then checks all $ 1$-entries of the last row and determines the best makespan of the corresponding profiles. By our above reasoning, the returned schedule will be a $(1+\epsilon)$-approximate solution.

The embedding of the dynamic programming in $A^{\text{sim-prof}}_{\epsilon}$ can be described as follows: each level of the branching tree $F$ has the same job fixed at every node, therefore level $j$ contains nodes where the fixed jobs are $1,\ldots, j$. Moreover, we only prune a node when either its profile is similar to another one already found (implying that at most one profile is considered from each partition class), or its lower bound is worse than an already found integer solution. Therefore, nodes at depth $j$ in $F$ have a one-to-one correspondence with cells of the $j$-th row of $M$ whose entry is $1$, except for some profiles that were discarded for having a too-high lower bound. In other words, $A^{\text{sim-prof}}_{\epsilon}$ can be seen as the dynamic programming algorithm embedded in the branch-and-bound framework, where $\bm{M}$ is traversed according to the best-first logic, and some entries are disregarded when even the best possible extension of their profile is worse than an already found feasible solution. In the worst case, each cell of $\bm{M}$ is visited before a $(1+\epsilon)$-approximate solution is found; but it happens no later than the processing of the last entry, according to Theorem \ref{fptas}.

The embedding becomes even more evident if we replace the best-first node selection rule with BFS. Then, traversing level $j$ of the tree is nothing else but processing the $j$-th row of $\bm{M}$ except for some entries that are stepped over because of their lower bound.

To conclude our work, we point out the infeasibility of repeating our results for the unrelated machine scheduling problem and the multiple knapsack problem. The notion of profiles and the $\frac{\epsilon}{n}$-partition of their space can be extended without changing anything. The difficulty lies in guaranteeing the highly structured property of the branching tree, in which the same job is fixed at all nodes of a given level. Of course, one can simply hard-code this into the algorithm, but giving a pivot rule that achieves this naturally seems infeasible. In particular, the following example shows that the ``maximal shortest processing time'' selection rule does not have this guarantee: let $m = 3$ and consider the following input with $n=2k+2$ jobs: $p_{1,1} = p_{1,2}=p_{1,3} = 3k+2$, $p_{j,2} = p_{j,3} = 3, \,\,\, j= 2, \ldots, n-1$ and $p_{n,2} = p_{n,3} = 2$. The rest of the processing times are chosen such that $p_{j,1} \le 3k+1, \,\,\, j = 2, \ldots, n$. It is easy to check that in the second iteration, there is no vertex in the polyhedron where the job with the maximal shortest processing time is fractional. This instance also serves as a counterexample for a bunch of other pivot selection strategies, such as ``maximal average completion time'' or ``maximal longest processing time''.

A similar phenomenon takes place in the case of the (multiple) knapsack problem, with the exception that we \emph{know} the infeasibility of having a structure where each node in the same level has the same job fixed. In particular, for the single knapsack problem, the two children of a given node have different pivot elements (provided that they are both feasible).

\begin{lemma}
    Let $(C, \bm{w}, \bm{p})$ denote an input to the single knapsack problem. Assume that the items are such that $\frac{p_1}{w_1} > \ldots > \frac{p_n}{w_n}$, and the pivot element is $j^*$. Let $(C-w_{j^*}, \bm{w}', \bm{p}')$ and $(C, \bm{w}', \bm{p}')$ denote the two subproblems corresponding to including and excluding item $j^*$ from the knapsack, with $\bm{w}' = \bm{w}|_{[n]-j^*}$ and $\bm{p}' = \bm{p}|_{[n]-j^*}$. Assume that both subproblems are feasible, and the corresponding pivot elements are $j_1$ and $j_2$. Then $j_1 < j^* < j_2$.
\end{lemma}

\begin{proof}\textcolor{red}{TOPROVE 7}\end{proof} 
\section{Computational Experiments}\label{sec:comp_exp}

In this section, we aim to assess the performance of our proposed algorithm on some randomly generated instances. Specifically, we compare our proposed strategies, which gave us theoretical guarantees, with other ones commonly used. The goal is to assess whether our theoretical guarantees are also observable in practice. We also provide a detailed runtime analysis. For the instances under study, a carefully optimized B\&B implementation, such as SCIP \cite{scip}, outperforms our naive implementation. However, we chose to reimplement everything from scratch, focusing on simplicity rather than efficiency. \\

\textbf{Experimental Setting.} All experiments were conducted on a Linux computer equipped with Intel Xeon E5-2650 v3 CPUs, each running at 2.3 GHz, and 64 GB of RAM. Our main code was implemented in Python 3.10.14, and all optimization routines were carried out using SCIP \cite{scip}. The code is provided as supplementary material.

\subsection{Multiple knapsack problem}

To test our algorithm, we generate 30 random instances for each pair $(n, m) \in \{(5, 2), (10, 2), \\(10, 5), (50, 2), (50, 5), (50, 15), (100, 2), (100, 5), (100, 10), (100, 15)\}$. Capacities are uniformly sampled integers from the range $[c_{\min}, c_{\max}]$, where $c_{\min} = \min_{j}{w_j}$ and $c_{max} = \left\lceil \frac{\sum w_j}{n} \right\rceil - c_{\min}$. The lower bound ensures that each item fits inside at least one of the knapsacks, while the upper bound ensures that (on average) half of the items fit in the union of the knapsacks, as discussed in \cite{chvatal}.

As baselines for node selection, we test DFS and BFS alongside the proposed GUB rule. For branching rules, we evaluate two approaches in addition to the previously introduced ``critical element'' (CE) strategy. In one strategy, we branch on the items among the \emph{fractional} ones with the largest profit-to-weight ratio (PPW). In the other strategy, as suggested by \cite{kolesar}, we branch on the item among the \emph{unfixed} ones with the largest profit-to-weight ratio (K).
We test these strategies for different values of $\alpha$ and collect various metrics, including the number of nodes explored, the gap to the optimum, the maximum depth reached, the number of nodes after finding the optimum, and the number of left turns.
Here, we report partial results, while a more extensive set of experiments is available in the interactive notebook. 

Figure \ref{fig:mk_nodes} shows the number of nodes explored to get an $\alpha = 0.97$ approximation. Since our implementation is a proof of concept and not fully optimized, we encountered memory issues. To address this, we imposed a threshold of $10^4$ nodes explored, beyond which we return the best solution found so far.
We observe that, in terms of number of nodes explored, the Greatest Upper Bound (GUB) strategy consistently outperforms the others. This is particularly evident in the ``hard'' instances $(100, 10), (100, 15), (50, 15)$, where all successful methods in at least one instance involve the GUB strategy.
Our proposed strategy (yellow box) frequently achieves the best overall performance.
Interestingly, in several cases, branching using the PPW rule yields better results compared to branching based on the Critical Element (CE) criterion.

In our analysis, we also record the optimality gap of the returned solution, defined as 
\[\frac{|z - z^*|}{\max(z,z^*)} \]
where $z$ is the solution as returned by our algorithms and $z^*$ is the optimal solution we computed using state-of-art Google OR-Tools \cite{ortools} with SCIP \cite{scip} as a linear solver. 


Figure \ref{fig:mk_gap} presents this information, clearly showing that GUB is often a winning strategy in terms of producing high-quality solutions. In this case, we do not observe any significant difference between CE and PPW.


\begin{figure}
\centering
\begin{subfigure}{0.5\textwidth}
\centering
    \includegraphics[width=1\linewidth]{multi_knapsack_0.97_nodes_explored.pdf}
    \caption{Multi knapsack, $\alpha = 0.97$}
    \label{fig:mk_nodes}
\end{subfigure}\begin{subfigure}{0.5\textwidth}
\centering
    \includegraphics[width=1.015\linewidth]{unrelated_job_scheduling_0.01_nodes_explored.pdf}
    \caption{Unrelated Machine Scheduling, $\epsilon = 0.01$}
    \label{fig:ujs_nodes}
\end{subfigure}
\caption{Performance of different strategies in the branch-and-bound method for the Multi-Knapsack and Unrelated Machine Scheduling problems. The number of nodes explored before termination (or reaching the stopping condition) is reported on a logarithmic scale.}
\label{fig:nodes}
\end{figure}

\begin{figure}
\centering
\begin{subfigure}{0.5\textwidth}
\centering
    \includegraphics[width=1\linewidth]{multi_knapsack_0.97_opt_gap.pdf}
    \caption{Multi knapsack, $\alpha = 0.97$}
    \label{fig:mk_gap}
\end{subfigure}\begin{subfigure}{0.5\textwidth}
\centering
    \includegraphics[width=1.015\linewidth]{unrelated_job_scheduling_0.01_opt_gap.pdf}
    \caption{Unrelated Machine Scheduling, $\epsilon = 0.01$}
    \label{fig:ujs_gap}
\end{subfigure}
\caption{Performance of different strategies in the branch-and-bound method for the Multi-Knapsack and Unrelated Machine Scheduling problems. The optimality gap is measured before termination (either upon reaching the stopping condition or exiting). The $y$-axis has been limited to highlight the most relevant portion of the plot.}
\label{fig:nodes_2}
\end{figure}





\subsection{Unrelated machine scheduling problem}
In this case, we generate 30 random instances for each pair $(n, m) \in \{(5, 2), (10, 2), (10, 5), \\(50, 2), (50, 5), (50, 10), (100, 2)\}$. Job lengths are uniformly sampled integers from the range $[1, 100]$. 
Note that, in this case, the analysis of the $(50, 5)$ instance could not be completed within our 48-hour time frame. Hence, we report only the average over the instances that were successfully solved.  
We attempt to understand why this occurred, given that the Multi-Knapsack framework initially seemed more tractable.  
In this case, the binary search involves repeatedly solving LPs, significantly increasing computational overhead. We have 12 different B\&B-like algorithms to evaluate, whereas, in the Multi-Knapsack setting, there were only 9. Lastly, unlike Multi-Knapsack, there is no pruning by infeasibility, making it harder to discard unpromising nodes quickly.


As baselines for node selection, we test DFS and BFS along with the proposed rule LLB.
For the lower bound, we chose both the proposed BS scheme and the Linear Relaxation (LR) of Integer Linear Programming minimizing the makespan that is commonly used in unrelated parallel machine scheduling.  
As the branching rule, we test only the one we propose: branching on the variable with the largest minimum processing time across machines (MMP).  
Both BS and LR return a solution that may contain fractional components, which we need to round to obtain an upper bound on the optimal solution.  
We compare two different rounding strategies (i) The one we prove leads to a PTAS, which assigns \emph{A}ll fractional jobs to the machine where their processing time is \emph{S}hortest (AS); (ii) An alternative approach based on Best Matching (BM) of the at most $m$ fractional jobs, where we find a matching that minimizes the total makespan. 
Even in this case, we observe a similar trend: LLB results in fewer nodes explored. However, on average, BM appears to be a better rounding strategy compared to AS.  
Regarding the optimality gap, interestingly, BFS slightly outperforms LLB in some instances (e.g., $n = 100, m = 5$). This is not surprising, as our theoretical results (Proposition \ref{bfs}) suggest that BFS also guarantees a PTAS.
Overall, we observe a significant difference in the order of magnitude of the average gap between the two problems. In the first case, a gap of 0 is rarely achieved, whereas, in the Unrelated Job Scheduling problem, the algorithm reaches the optimal solution for $n \in \{5, 10\}$ and for the instance $(100, 2)$, regardless of the choice of branching, bounding, and selection strategy.
 
\subsection{Analysis of the runtime of the proposed 
algorithm}\label{app:comp_exp}

\begin{figure}
    \centering
    \includegraphics[width=0.49\linewidth]{multi_knapsack_0.97_runtime.pdf}
    \includegraphics[width=0.49\linewidth]{unrelated_job_scheduling_0.01_runtime.pdf}
    \caption{Performance of different strategies in the branch-and-bound method for the Multi-Knapsack and Unrelated Machine Scheduling problems. The runtime (in seconds) is reported on a logarithmic scale.}
    \label{fig:runtime}
\end{figure}

In Figure \ref{fig:runtime}, we report the runtime of our proposed algorithm for the Multi-Knapsack problem (left) and the Unrelated Machine Job Scheduling problem (right).  

\subsubsection{Multiple Knapsack}  
First, we observe that the runtime is quite long, even for relatively small instances in both cases.  
As previously mentioned, our implementation is not highly optimized.  
To put this into perspective, solving the benchmark instance $(100, 15)$ in the Multi-Knapsack framework typically requires $7.82 \pm 4.40$ seconds with the B\&B implemented in SCIP\footnote{In this case, for fairness, we disable presolve, cutting plane, heuristics, restarts, and propagation, and set a branching nodes limit equal to $10^4$}, which is significantly shorter than our results.

However, an interesting pattern emerges from the analysis of our strategies.  
For the Multi-Knapsack problem, we observe that when the number of knapsacks is $\geq 10$, DFS outperforms GUB in terms of speed.  
We suspect this is because, in the Multi-Knapsack setting, the DFS strategy consistently reaches the node exploration limit, set to $10^4$.  
As a result, it likely prunes many nodes quickly due to infeasibility or bounding, avoiding complex computations on the explored nodes. Hence, we arrive at the node limit faster. 

\subsubsection{Unrelated machine scheduling problem}
As in the previous section, we compare our approach with SCIP for the case $(50, 10)$, where we observe a runtime of $10.051\pm 1.63$, which is again shorter than the observed runtime of our algorithm.  

In this case, we find that the LLB strategy results in faster solutions.  
This outcome is expected: since all nodes are explored and pruning by infeasibility is not possible (as every subproblem remains feasible), the time required per node remains roughly the same.  

Since the LLB strategy achieves better solutions with fewer node explorations, we can conclude that our theoretical expectations align with the experimental results. 
\section{Concluding Remarks}\label{sec:conc}

Let us collect general observations about our methods. Most importantly, we note that the best-first rule can be replaced by BFS in $A^{\text{unrel}}_{\epsilon}$ without losing the theoretical worst-case guarantee (while keeping the other parameters fixed). The key element of our proof was to limit the depth of the final tree of the algorithm ($F$) at $\lfloor\frac{m^2}{\epsilon}\rfloor$. Let $F'$ be the B\&B tree of the alternative with BFS, limited to depth $\lfloor\frac{m^2}{\epsilon}\rfloor$. Since $F \subseteq F'$, the lowest lower bound in $F'$ (denoted by $GL'$) is greater or equal than the lowest lower bound in $F$ (denoted by $GL$), and the best integer solution found in $F'$ (denoted by $GU'$) is better than the best integer solution we found with $A^{\text{unrel}}_{\epsilon}$ (denoted by $GU$). Hence,
\[
\frac{GU'}{GL'} \le \frac{GU}{GL}\le 1+\epsilon,
\]
and it follows that with BFS as the search strategy (denoted by $A^{\text{BFS-unrel}}_{\epsilon}$), we terminate no later than processing $F'$. Since $|F'|\le m^{\lfloor\frac{m^2}{\epsilon}\rfloor}$, we have that

\begin{prop}\label{bfs}
    For every fixed $\epsilon > 0$, the algorithm $A^{\text{BFS-unrel}}_{\epsilon}$ returns a $(1+\epsilon)$-approximate solution to the unrelated machine scheduling problem, after processing at most $m^{\lfloor\frac{m^2}{\epsilon}\rfloor}$-many nodes in the branching tree.
\end{prop}

However, according to our experiments, best-first seems empirically better in terms both of optimality gap and number of nodes explored. For $A^{\text{knap}}_{\alpha}$, we do not have similar guarantees as there we only have bound on the number of left-turns in the branching tree and the depth of $F$ can potentially be as large as $n$. For the algorithm $A^{\text{sim-prof}}_{\epsilon}$, the bound on the number of visited nodes was independent of the tree traversal strategy, since our proof only relies on the limited number of different nodes at each level. Therefore, any alternative strategy can be used to replace the best-first one with the same worst-case guarantee.

Next, we note that for the machine scheduling problem, the results and algorithms can be modified to work with the ``standard'' LP-formulation lower bound instead of the binary search one, but the lower bound itself is trivially worse. Last, we mention that for the special case of identical parallel machines, $A_{\epsilon}^{\text{unrel}}$ can be improved with a slight change in the rounding method. This version visits $m^{\lfloor \frac{m}{\epsilon}\rfloor}$ nodes in the worst case, by keeping the exact same argument. Furthermore, there are fast and intuitive heuristics for finding vertices of the polyhedra, thus the time spent in individual nodes can also be reduced.

We conclude by collecting the most essential properties of the knapsack and machine scheduling problems that were exploited during the investigation, intending to set the ground for generalizing the results to a larger class of problems. 

Perhaps the most paramount property the two problems have in common is the notion of \emph{self-similarity}. To repeatedly apply the same argument for each node of a path in the branching tree, we needed the sub-problems encoded by these nodes to fall into the same category as the original problem. In other words, fixing one variable to $1$ or $0$ should result in a problem that is in the same class as the original one. This property was by default true for the knapsack problem: setting $x_{j, i} = 1$ in $MK_m(\bm{C}, \bm{w}, \bm{p})$ yields the sub-problem $MK_m(\bm{C'}, \bm{w}|_{[n]-j}, \bm{p}|_{[n]-j})$ with $\bm{C'} = (C_1, \ldots, C_{i-1}, C_i - w_j, C_{i+1}, \ldots, C_m)$, while the rightmost branch corresponds to $MK_m(\bm{C}, \bm{w}|_{[n]-j}, \bm{p}|_{[n]-j})$. For the machine scheduling problem, on the other hand, the default description was not sufficient. If we fix a binary variable to $1$ in the standard linear programming formulation, the resulting LP will not correspond to a machine scheduling problem of the same type. However, introducing overheads ensures the desired property, since now setting a variable to $1$ corresponds to increasing the appropriate machine's overhead by the processing time of the fixed-job.

Strongly related to this property, we relied on the monotonicity of subproblems: for maximization problems, the local upper bound of a node is greater than the local upper bound of any of its children (for minimization problems, a similar property holds). However, this is a direct consequence of using the same objective function on subsequently smaller sets.

We also exploited that there was a quantifiable relationship between a node's lower/upper bounds and the job/item that was fixed at the node. For the knapsack problem, this relationship is guaranteed by Lemma \ref{knapsack_gap_and_critical_element}, whereas for the machine scheduling problem, the inequality
\[
\frac{p'_j}{L(v)} \le \frac{m}{k}
\]
provided the connection.

Finally, the least demanding requirement we need to pose is that of approximability. When rounding a fractional solution of a sub-problem at a node, an $(m+1)$-approximation algorithm was used for both the knapsack problem and the machine scheduling problem. However, for the machine scheduling problem, the proof did not rely upon this local rounding guarantee, and hence the necessity of a constant-factor approximation rounding is unclear.

\bigskip

It is important to acknowledge the limitations and drawbacks of our approach. During the last couple of decades, several approximation schemes have been described for both the knapsack and the machine scheduling problem. In fact, both problems are known to admit a fully polynomial-time approximation scheme (FPTAS), which is far superior to the PTAS framework in which the desired proximity ratio ($\epsilon$ or $\alpha$) appears in the exponent of the running time. Furthermore, as we see in Subsection \ref{sec:js_fptas_proof}, our arguments are not directly repeatable or extendable for some of the cases. Nevertheless, we believe that the connection between B\&B and approximation algorithms explored in the paper adds a surprising flavor to the theory of branch-and-bound algorithms, and sheds some light on their good behavior observed in practice.

For future research directions, we mention the possibility of a B\&B yielding an FPTAS for the unrelated machine scheduling problem (or even more complex scheduling paradigms such as the job shop problem), with a possibly different choice of parameters and additional rounding tricks. 





\bibliography{references}

\section{Pseudocodes}\label{app:pseudocode}

\subsection{The branch-and-bound framework}
As already discussed in Section \ref{sec:intro}, any B\&B methods run some basic functions that can be highly customized. 
Algorithm \ref{alg:branch_and_bound} details a general B\&B framework for a minimization problem, but a similar argument can occur with a maximization one. 

\begin{algorithm}
\caption{Branch and Bound Algorithm. The steps denoted with $*$ must be changed when switching from minimization to maximization. }
\label{alg:branch_and_bound}
\begin{algorithmic}[1]
    \State \textbf{Input:} Problem instance, a threshold that we wish to guarantee to our solution quality
    \State \textbf{Output:} High-quality solution
    \State Do any necessary preprocessing
    \State Initialize global lower bound (GLB) and global upper bound (GUB) (best feasible solution)
    \State Compute initial lower bound using a relaxation method
    \If{is integer}
        \textbf{return}
    \EndIf
    \State Initialize priority queue (heap) with root node
    \While{queue is not empty}
        \State Extract the most promising node from the queue
        \If{node is integral}
            \State Update GUB$^*$ if a better solution is found
            \State \textbf{continue}
        \EndIf
        \State Select a branching item/job
        \For{each possible branch (child node)}
            \State Apply feasibility check
            \State Compute new upper bound and lower bound
            \If{new lower bound $<$ GUB$^*$}
                \State Add new node to the queue
            \EndIf
        \EndFor

        \State Update GLB$^*$ as max remaining lower bound in queue
        \If{A relation between GUB, GLB and the threshold is satisfied}
            \State \textbf{return}
        \EndIf
    \EndWhile
\end{algorithmic}
\end{algorithm}














\subsection{A specific implementation}

Now we describe how Algorithm \ref{alg:branch_and_bound} is specified to obtain $A^{\text{knap}}_{\alpha}$. The other algorithms can be derived similarly.

Let Solve-LP denote the subroutine that on input $(\bm{C}, \bm{w}, \bm{p})$, returns the triple $(\bm{x}^*, \bm{x'}, j^*)$ as described in Proposition \ref{2_approx_knapsack} and Lemma \ref{knapsack_gap_and_critical_element}: 
$\bm{x^*}$ is the fractional optimum of the knapsack instance, $\bm{x}'$ is the best assignment among $\lfloor\bm{x}^*\rfloor$ and the critical elements, and $j^*$ is the most profitable critical element.

We describe the branch-and-bound algorithm $A^{\text{knap}}_{\alpha}$ in detail below. Each node $v$ will be identified by the unique sets $(I, E)$ with $I = \{(j_1,i_1), \ldots, (j_k, i_k)\}$ being the (item, knapsack) inclusions fixed so far, and $E=\{(e_1,m+1), \ldots, (e_l,m+1)\}$ being the excluded items.

\begin{algorithm}
\caption{$A^{\text{knap}}_{\alpha}$}
\label{alg:knapsack}
\begin{algorithmic}[1]
    \State \textbf{Input:} A knapsack instance $(\bm{C}, \bm{w}, \bm{p})$ with a fixed number of knapsacks.
    \State \textbf{Output:} An integer assignment whose profit is at least $\alpha$ times the optimum.
    \State $r := (\emptyset, \emptyset)$
    \State $(\bm{x}^* _r, \bm{x}_r ', j^* _r) := \text{Solve-LP}(\bm{C, \bm{w}, \bm{p}})$
    \State $U(r):= \bm{p}\cdot \bm{x}^* _r$, $L(r):= \bm{p}\cdot \bm{x}' _r$
    \State $GU := U(r)$, $GL := L(r)$
    \State $queue :=  \{r\}$
    \While{$\frac{GL}{GU} < \alpha$}
        \State $v := \arg\max \{U(node): node \in queue\}$
        \State $(I, E) \leftarrow v$, $\{(j_1, i_1, )\ldots, (j_k, i_k)\} \leftarrow I$, $\{(e_1, m+1), \ldots, (e_l, m+1)\} \leftarrow E$
        \State $queue := queue \setminus \{v\}$
        \State $S := [n]\setminus \left(\bigcup_{z=1}^k j_z \cup \bigcup_{z=1}^l e_z\right)$
        \State $\bm{w}_v := \bm{w}|_{S}$, $\bm{p}_v := \bm{p}|_{S}$
        \For{$i=1, \ldots, m$}
            \State $C^v_i := C_i - \sum\limits_{z: i_z = i} w_z$
        \EndFor
        \State $\bm{C}_v := (C^v _1, \ldots, C^v _m)$
        \State $(\bm{x^*}, \bm{x}', j^*):= \text{Solve-LP}(\bm{C}_v, \bm{w}_v, \bm{p}_v)$
        \For{$i=1, \ldots, m$}
            \State $v_i := (I\cup (j^*, i), E)$
        \EndFor
        \State $v_{m+1} := (I, E\cup (j^*, m+1))$
        \For{$i=1, \ldots, m$}
            \State $\bm{C}_{v_i} := (C^v _1, \ldots, C^v _{i-1}, C^v_i - w_{j^*}, C^v_{i+1}, \ldots, C^v_m)$
        \EndFor
        \State $\bm{C}_{v_{m+1}} := \bm{C}_v$, 
        \For{$i=1, \ldots, m+1$}
            \State $\bm{w}_{v_i} := \bm{w} |_{S \cup \{j^*\}}$, $\bm{p}_{v_i} := \bm{p} |_{S \cup \{j^*\}}$
            \State $(\bm{x}^* _{v_i}, \bm{x}' _{v_i}, j^*_{v_i}) := \text{Solve-LP}(\bm{C}_{v_i}, \bm{w}_{v_i}, \bm{p}_{v_i})$
            \State $SU(v_i) := \bm{p}_{v_i} \cdot \bm{x}^* _{v_i}$, $SL(v_i) := \bm{p}_{v_i} \cdot \bm{x}' _{v_i}$
            \State $U(v_i) := SU(v_i) + \sum\limits_{(j,t) \in I \cup \{(j^*, i)\}, t \ne m+1} p_j$
            \State $L(v_i) := SL(v_i) + \sum\limits_{(j,t) \in I \cup \{(j^*, i)\}, t \ne m+1} p_j$
            \If{$U(v_i) > GL$}
                \State $queue := queue \,\cup \{v_i\}$
            \EndIf
        \EndFor
        \State $GU = \max\{U(node): node \in queue\}$, $GL = \max\{L(node): node \in queue\}$
    \EndWhile
    \State $v := \arg\max \{U(node): node \in queue\}$
    \State $(I,E) \leftarrow v$
    \State \textbf{return} $I \cup \{(j,i): (\bm{x}'_v)_{j,i}=1\}$.
\end{algorithmic}
\end{algorithm}

 
\end{document}