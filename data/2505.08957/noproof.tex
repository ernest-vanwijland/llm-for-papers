

\documentclass[11pt]{article}
\usepackage{amsmath,amssymb,amsthm}

\DeclareMathOperator*{\E}{\mathbb{E}}
\let\Pr\relax
\DeclareMathOperator*{\Pr}{\mathbb{P}}
\newcommand{\rsmpl}{\xleftarrow{\$}}

\newcommand{\eps}{\varepsilon}
\newcommand{\med}{\mathsf{med}}
\newcommand{\inprod}[1]{\left\langle #1 \right\rangle}

\iffalse
\topmargin 6pt
\advance \topmargin by -\headheight
\advance \topmargin by -\headsep
\textheight 8.9in
\oddsidemargin 0pt
\evensidemargin \oddsidemargin
\marginparwidth 1.5in
\textwidth 6.5in
\parindent 0in
\parskip 1.5ex
\linespread{1.1}
\fi

\iffalse
\usepackage[ruled,vlined,linesnumbered]{algorithm2e}
\usepackage{lmodern}        \usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage{framed}
\usepackage{titling}
\usepackage{verbatim}
\usepackage{float}
\usepackage{amssymb,amsfonts,amsmath}
\usepackage{enumerate}
\usepackage{amsthm}
\usepackage{graphicx,lipsum}
\usepackage{algpseudocode}
\usepackage{dsfont}
\usepackage{color}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{amsthm}
\usepackage{hyperref}
\usepackage{caption}
\usepackage{subfigure}
\usepackage{microtype}
\usepackage{mathtools}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
\DeclarePairedDelimiter\card{\lvert}{\rvert}
\fi

\usepackage[framemethod=TikZ]{mdframed}
\newcounter{Frame}
\newenvironment{Frame}[1][htb]{\refstepcounter{Frame}
    \begin{mdframed}[frametitle={#1},
        skipabove=\baselineskip plus 2pt minus 1pt,
        skipbelow=\baselineskip plus 2pt minus 1pt,
        linewidth=1.0pt,
        frametitlerule=true,
        nobreak=true
    ]}{\end{mdframed}
}


\newenvironment{customizedFrame}[1]
  {\mdfsetup{
    frametitle={\colorbox{white}{\space#1\space}},
    skipabove=\baselineskip plus 2pt minus 1pt,
    skipbelow=\baselineskip plus 2pt minus 1pt,
    frametitleaboveskip=-0.7\ht\strutbox,
    frametitlealignment=\center
    }
  \begin{mdframed}}
  {\end{mdframed}}

\usepackage{tikz}
\usetikzlibrary{arrows}

\makeatletter
\newcommand\@erelb@r[1]{\mathrel{\tikz[baseline=-.5ex]\draw[#1] (0,0)--(0.3,0);}
}
\newcommand{\erelbar}[1]{\@erelbar#1}
\def\@erelbar#1#2{\ifcase\numexpr#1*4+#2\relax
    \@erelb@r{-}\or     \@erelb@r{->}\or    \@erelb@r{-|}\or    \@erelb@r{->|}\or   \@erelb@r{<-}\or    \@erelb@r{<->}\or   \@erelb@r{<-|}\or   \@erelb@r{<->}\or   \@erelb@r{|-}\or    \@erelb@r{|->}\or   \@erelb@r{|-|}\or   \@erelb@r{|<->|}\or \@erelb@r{|<-}\or   \@erelb@r{|<->}\or  \@erelb@r{|<-|}\or  \@erelb@r{|<->|}    \else
    \@wrong
  \fi
}
\makeatother




\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}

\newtheorem{lem}[theorem]{Lemma}



\newtheorem{defn}[theorem]{Definition}
\newtheorem{asm}[theorem]{Assumption}
\newtheorem{rem}[theorem]{Remark}
\newtheorem{clm}[theorem]{Claim}
\newtheorem{cond}[theorem]{Condition}
\newtheorem{fact}[theorem]{Fact}


\newcommand{\problem}[1]{\section{#1}}		\newcommand{\new}[1]{{\em #1\/}}		

\newcommand{\setof}[2]{\{\,{#1}|~{#2}\,\}}	\newcommand{\C}{\mathbb{C}}	                \newcommand{\poly}{{\rm poly}}
\newcommand{\N}{\mathbb{N}}   
\newcommand{\calP}{\mathcal{P}} \newcommand{\calD}{\mathcal{D}} 
\newcommand{\calQ}{\mathcal{Q}} 

\newcommand{\LL}{\mathcal{L}}  
\newcommand{\OO}{\mathcal{O}} 

\newcommand{\compl}[1]{\overline{#1}}	

\newcommand{\pr}[1]{\text{\bf Pr}\normalfont\bigl[ #1 \bigr]}
\newcommand{\npr}{\text{\bf Pr}}
\newcommand{\ppr}[2]{\underset{#1}{\text{\bf Pr}} \normalfont\bigl[ #2 \bigr]}
\newcommand{\bpr}[1]{\text{\bf Pr}\normalfont\Bigl[ #1 \Bigr]}
\newcommand{\ex}[1]{\mathbb{E}\normalfont\lbrack #1 \rbrack}

\newcommand{\bex}[1]{\mathbb{E}\normalfont \Big[#1 \Big]}
\newcommand{\ang}[1]{\langle #1 \rangle }

\newcommand{\m}{\mathfrak{m}}
\newcommand{\p}{\mathfrak{p}} 
\newcommand{\R}{\mathbb{R}} 
\newcommand{\PP}{\mathbb{P}} 
\newcommand{\G}{\mathcal{G}}
\newcommand{\MWB}{\text{MWB}}
\newcommand{\I}{\mathcal{I}}
\newcommand{\tx}[1]{\text{#1}}
\newcommand{\ttx}[1]{\texttt{#1}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}



\newcommand{\var}[1]{\text{\bf Var}\normalfont\lbrack #1 \rbrack} 
\newcommand{\e}[1]{\text{\bf E}\normalfont\lbrack #1 \rbrack} 
\newcommand{\ind}[1]{\mathds{1}\normalfont\lbrack #1 \rbrack} 

\newcommand{\ying}[1]{{\color{blue!40!teal} [{\bf Ying:} #1]}}
\newcommand{\piotr}[1]{{\color{red} [{\bf Piotr:} #1]}}

\newcommand{\opt}{\mathsf{opt}}
\newcommand{\Tournament}{\texttt{Tournament}}
\newcommand{\QuadTree}{\texttt{QuadTree}} \usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage[utf8]{inputenc}
\usepackage{booktabs} 

\usepackage{hyperref}


\newcommand{\theHalgorithm}{\arabic{algorithm}}

\providecommand{\email}[1]{\href{mailto:#1}{\nolinkurl{#1}\xspace}}


\hypersetup{
     colorlinks   = true,
     citecolor    = {blue!50!teal},
     linkcolor		= purple
} \usepackage{fullpage}


\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}




\usepackage[capitalize,noabbrev]{cleveref}



\usepackage[textsize=tiny]{todonotes}

\usepackage{subfiles}
\usepackage{url,color}
\usepackage{footnote}
\usepackage{threeparttable}
\usepackage{mdframed}
\usepackage{xspace}
\usepackage{pifont}
\usepackage{stmaryrd}
\makesavenoteenv{tabular}
\makesavenoteenv{table}
\newtheorem*{theorem*}{Theorem}
\newcommand{\lwe}{\epsilon}

\usepackage{booktabs}
\usepackage{tablefootnote}

\usepackage{array}
\newcolumntype{L}[1]{>{\raggedright\arraybackslash}p{#1}} \newcolumntype{C}[1]{>{\centering\arraybackslash}p{#1}}    \newcolumntype{R}[1]{>{\raggedleft\arraybackslash}p{#1}}   



\begin{document}

\newcommand{\tikzxmark}{\tikz[scale=0.23] {
    \draw[line width=0.7,line cap=round] (0,0) to [bend left=6] (1,1);
    \draw[line width=0.7,line cap=round] (0.2,0.95) to [bend right=3] (0.8,0.05);
}}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}


\renewcommand{\subparagraph}[1]{\medskip\noindent\underline{\textit{#1}}}

\title{Even Faster Algorithm for the Chamfer
Distance}
\author{Ying Feng\thanks{MIT. 
E-mail: \email{yingfeng@mit.edu}}
\and
Piotr Indyk\thanks{MIT. 
E-mail: \email{indyk@mit.edu}}
}
\date{\today}
\maketitle









\begin{abstract}
For two $d$-dimensional point sets $A,B$  of size up to $n$, the Chamfer distance from $A$ to $B$ is defined as $CH(A,B)=\sum_{a \in A} \min_{b \in B} \|a-b\|$.
 The Chamfer distance is a widely used measure for quantifying dissimilarity between sets of points, used in many machine learning and computer vision applications. A recent work of Bakshi et al, NeuriPS'23, gave the first near-linear time $(1+\eps)$-approximate algorithm, with a running time of  $\OO(nd \log (n)/\eps^2)$. In this paper we improve the running time further,  to $\OO(nd(\log\log n+\log\frac{1}{\eps})/\eps^2)$. 
  When $\eps$ is a constant, this reduces 
  the gap between the upper bound and the trivial $\Omega(dn)$ lower bound significantly, from $\OO(\log n)$ to $\OO(\log\log n)$. 
\end{abstract} 
\newpage

\section{Introduction}

For any two $d$-dimensional point sets $A,B$ of sizes up to $n$, the Chamfer distance from $A$ to $B$ is defined as 
\newcommand{\CH}{\mathsf{CH}}
\begin{equation*}
\CH(A,B) = \sum_{a \in A} \min_{b \in B} \|a-b\|
\end{equation*}

where $\|\cdot\|$ is the underlying norm defining the distance between the points. Chamfer distance and its variant, the Relaxed Earth Mover Distance~\cite{kusner2015word,atasu19a}, are widely used metrics for quantifying the distance between two {\em sets} of points. These measures are especially popular in fields such as machine learning (e.g.,\cite{kusner2015word,wan2019transductive}) and computer vision (e.g.,\cite{athitsos2003estimating,sudderth2004visual,fan2017point,jiang2018gal}). A closely related notion of ``the sum of maximum similarities'', where $\min_{b \in B} \|a-b\|$ is replaced by $\max_{b \in B} a \cdot b$, has been recently popularized by the ColBERT system~\cite{khattab2020colbert}. Efficient subroutines for computing Chamfer distances are provided in prominent libraries including Pytorch~\cite{pytorch3d}, PDAL~\cite{pdal} and Tensorflow~\cite{tensorflow}. In many applications (e.g., see ~\cite{kusner2015word}), Chamfer distance is favored as a faster alternative to the more computationally intensive Earth-Mover Distance or Wasserstein Distance. 



Despite the popularity of Chamfer distance, efficient algorithms for computing it haven't attracted as much attention as algorithms for, say, the Earth-Mover Distance. The first improvement to the naive  $\OO(dn^2)$-time algorithm was obtained in~\cite{sudderth2004visual}, who utilized the fact that $\CH(A,B)$ can be computed by performing $|A|$ nearest neighbor queries in a data structure storing $B$. However, even when  the state of the art approximate nearest neighbor algorithms are used, this leads to an $(1+\epsilon)$-approximate estimator with only slightly sub-quadratic running time of $\OO \left(dn^{1+\frac{1}{2 (1+\epsilon)^2 -1}}\right)$ in high dimensions~\cite{andoni2015optimal}\footnote{All algorithms considered in this paper are randomized, and return $(1+\eps)$-approximate answers with a constant probability}. The first near-linear-time algorithm for any dimension was proposed only recently in ~\cite{BIJ24}, who gave a $(1+\epsilon)$-approximation algorithm with a running time of $\OO(dn \log(n)/\eps^2)$, for $\ell_1$ and $\ell_2$ norms.
Since any algorithm for approximating the distance must run in at least $\Omega(dn)$ time\footnote{The Chamfer Distance could be dominated by the distance from a single point $a \in A$ to $B$.}, the the upper and lower running time bounds differed by a factor of $\log(n)/\eps^2$.
 \ \\
{\bf Our result:}
In this paper we make a substantial progress towards reducing the gap between the upper and lower bounds for this problem. In particular, we show the following theorem. Assume a Word RAM model where both the input coordinates and the memory/processor word size is $\OO(\log n)$ bits.\footnote{In the Appendix, we adopt the reduction of~\cite{BIJ24} to extend the result to coordinates of arbitrary finite precision.} Then:

\begin{theorem}
\label{t:main}
    There is an algorithm that, given two sets $A, B$ of $d$-dimensional points with coordinates in 
    $\{1 \ldots \mbox{\poly}(n) \}$ and a parameter $\eps>0$, computes a $(1+\eps)$-approximation to the Chamfer distance from $A$ to $B$ under the $\ell_1$ metric,  in time  \[ \OO(nd(\log\log n+\log\frac{1}{\eps})/\eps^2)) .\]
   The algorithm is randomized and is correct with a constant probability.
\end{theorem}

Thus, we reduce the gap between upper and lower bounds from $\OO(\log (n)/\eps^2)$ to \\ $\OO(\log\log n +\log\frac{1}{\eps})/\eps^2)$.


\subsection{Our techniques}
Our result is obtained by identifying and overcoming the bottlenecks in the previous algorithm ~\cite{BIJ24}. On a high level, that algorithm consists of two steps, described below. For the sake of exposition, in what follows we assume that the target approximation factor $1+\eps$ is some constant.


\ \\
{\bf Outline of the prior algorithm:} In the first step, for each point $a \in A$, the algorithm computes an estimate $\calD_a$ of the distance $\opt_a$ from $a$ to its nearest neighbor in $B$. The estimate is $\OO(\log n)$-approximate, meaning that we  $\opt_a \le \calD_a \le \OO(\log n) \opt_a$. This is achieved as follows. First,  the algorithm imposes $\OO(\log n)$ grids of sidelength $1,2, 4, \ldots$, and maps each point in $B$ to the corresponding cells. Then, for each $a$, it identifies the finest grid cell containing both $a$ and some point $b \in B$.
Finally, it uses the distance between $a$ and $b$ as an estimate $\calD_a$. To ensure that this process yields an $\OO(\log n)$-approximation, each grid needs to be {\em independently} shifted at random. We emphasize that this independence between the shifts of different grids is {\em crucial} to ensure the $\OO(\log n)$-approximation guarantee - the more natural approach of using ``nested grids'' does not work. The whole process takes $\OO(nd)$ time per grid, or $\OO(nd \log n)$ time overall.

In the second step, the algorithm estimates the Chamfer distance via {\em importance sampling.} Specifically, the algorithm samples $T$ points from $A$, such that the probability of sampling $a$ is proportional to the estimate $\calD_a$. For each sampled point $a$, the distance $\opt_a$ from $a$ to its nearest neighbor in $B$ is computed directly in $\OO(nd)$ time. The final estimate of the Chamfer distance is equal to the weighted average the $T$ values $\opt_a$ . It can be shown that if the number of samples $T$ is equal to the distortion $\OO(\log n)$ of the estimates $\calD_a$, this yields a constant factor approximation to the Chamfer distance from $A$ to $B$. The overall cost of the second step is $\OO(T nd) = \OO(nd \log n)$, i.e., asymptotically the same as the cost of the first step. 

\ \\
{\bf Intuitions behind the new algorithm:}  To improve the running time, we need to reduce the cost of each of the two steps. In what follows we outline the obstacles to this task and how they can be overcome. 

\ \\
{\em Step 1: } The main difficulty in reducing the cost of the first step is that, for each grid,  the point-to-cell assignment takes $\OO(nd)$ time to compute, so computing these $T$ assignments separately for each grid takes $\OO(nd T)$ time. And,  since each grid is independently translated by a different random vector,  the grids are not nested, i.e., a  (smaller) cell of side length $2^i$ might contain points from many  (larger) cells of side length $2^{i+1}$.
As a result,  is unclear how to reuse the point-to-cell assignment in one grid to speedup the assignment in another grid, while computing them separately takes $\OO(ndT)$ time. 

To overcome this difficulty, we abandon independent shifts and resort to $\OO(\log n)$ {\em nested} grids. 
Such grids can be viewed as forming a {\em quadtree} with $\OO(\log n)$ levels, where any cell $C$ at level $i+1$ (i.e., of side length $2^{i+1}$) is connected to $2^d$ cells at level $i$ contained in $C$.
(Note that the root node of the quadtree has the highest level $\OO(\log n)$).
Although using a single quadtree increases the approximation error, we show that using {\em two} independently shifted quadtrees retains the $\OO(\log n)$ approximation factor. That is, we repeat the process of finding the finest grid cell containing both $a$ and some point from $B$ twice, and return the point in $B$ that is closer to $a$. This amplifies the probability of finding a point from $B$ that is ``close'' to $a$, which translates into a better approximation factor compared to using a single quadtree.

We still need show that the point-to-cell assignments can be computed efficiently. To this end, we observe that for each point $a$, its assignment to all $\OO(\log n)$ nested grids can be encoded as $d$ words of length $\OO(\log n)$, or a $d \times \OO(\log n)$ bit matrix $M$. Each row corresponds to one of the $d$ coordinates, and the most significant bit of a row indicates the assignment to cells at the highest level (i.e. cells with the largest side length) with respect to that coordinate. In other words, the most significant bits of all coordinates are packed into the first column, etc. We observe that two points $a$ and $b$  lie in the same cell of side length $2^{i}$ if and only if their matrices agree in all but the last $i$ columns.
If we {\em transpose} $M$ and read the resulting matrix in the row-major order, then finding a point $b \in B$ in the finest grid cell containing $a$ is equivalent to finding $b$ that shares the longest common prefix with $a$. We show that this transposition can be done using $\OO( \log n \cdot \log \log n)$ simple operations on words, yielding $\OO(n \log n \cdot \log \log n) = \OO(n d \cdot \log \log n)$ time overall.

As an aside, we note that quadtree computation is a common task in many geometric algorithms~\cite{har2011geometric}. Although an $\OO(n)$ algorithm for this task was known for constant dimension $d$ ~\cite{C08}\footnote{Assuming that each coordinate can be represented using $\log n$ bits.},  to the best of our knowledge our algorithm is the first to achieve $\OO(n d \cdot \log \log n)$ time for arbitrary dimension. 

\ \\
{\em Step 2:} At this point we computed estimates $\calD_a$ such that $\opt_a \le \calD_a \le \OO(\log n) \opt_a$. Given these estimates, importance sampling still requires sampling $\Omega(\log n)$ points. Therefore, we improve the running time by {\em approximating} (up to a constant factor) the values $\opt_a$, as opposed to computing them exactly. This is achieved by computing $\OO(\log \log n)$ random projections of the input points, which ensures that that the distance between any fixed pair of points is well-approximated with probability $1-1/\mbox{poly}(\log n)$. We then employ these projections in a variant of the tournament algorithm of~\cite{K97} which computes $\OO(1)$-approximate estimates of $\opt_a$ for $\OO(\log n)$ sampled points $a$ in $\OO(nd \log \log n)$ time. Since the algorithm of \cite{K97} works for the $\ell_2$ metric as opposed to the $\ell_1$ metric, we replace Gaussian random projections with Cauchy random projections, and re-analyze the algorithm. 

This completes the overview of an $\OO(nd \log \log n)$-time algorithm for estimate the Chamfer distance up to a {\em constant} factor. To achieve a $(1+\eps)$-approximation guarantee for any $\eps>0$, we proceed as follows. 
 First, instead of sampling $\OO(\log n)$ points as before, we sample $\OO(\log(n)/\eps^2)$ points $a$. Then, we use the tournament algorithm to compute $\OO(1)$-approximations to $\opt_a$, as before. \footnote{Note that we could use the tournament algorithm to report $(1+\eps)$-approximate answers, but then the dependence of the running time on  $1/\eps$ would become {\em quartic}, as  the $1/\eps^2$ term in the sample size would be multiplied by another $1/\eps^2$ term in the bound for the number of projections needed to guarantee that the tournament algorithm returns $(1+\eps)$-approximate answers. } Then we use a technique called {\em rejection sampling} to simulate the process of sampling $\OO(1/\eps^2)$ points $a$ with probability proportional to $\Theta(\opt_a)$. For each such point, we compute $\opt_a$ exactly in $\OO(nd)$ time. Finally, we use the $\OO(1/\eps^2)$ sampled points $a$ and the exact values of $\opt_a$ in importance sampling to estimate the Chamfer distance up to a factor of $1+\eps$.

This concludes the overview of our algorithm for the Chamfer distance under the $\ell_1$ metric. We remark that \cite{BIJ24} also extends their result from the $\ell_1$ metric to the $\ell_2$ metric by first embedding points from $\ell_2$ to $\ell_1$ using random projections. This takes $\OO(nd \cdot \log n)$ time, which exceeds the runtime of our algorithm, eliminating our improvement. However, a faster embedding method would yield an improved runtime for the Chamfer distance under the $\ell_2$ metric. We leave finding a faster embedding algorithm as an open problem.


 \section{Preliminaries}

In this paper, we consider the regime where the approximation factor $\eps \geq \sqrt{\frac{\log n}{n}}$. Note that otherwise, an $\OO(nd/\eps^2)$ time bound would be close to the runtime of a naive exact computation.

In the proof of Theorem~\ref{t:main}, we assume a Word RAM model where both the input coordinates and the memory/processor word size is $\OO(\log n)$ bits.
This model is particularly important in procedures $\texttt{Concatenate}$ and $\texttt{Transpose}$, where we rely on the fact that we can shift bits and perform bit-wise AND, ADD and OR operations in constant time. 


\ \\
{\bf Notation:} For any integers $a \geq 1$, we use $[n]$ to denote the set of all integers from $1$ to $n$. For any two real numbers $a, b$ such that $a \leq b$, we use $[a, b]$ to denote the set of all reals from $a$ to $b$. Let $d$ be the dimension of points. 

For any $q \in \R^d$, define $\opt^P_q := \min_{p \in P}\lVert q-p\rVert_1$ for some subset $P$ of $\R^d$. We will omit the superscript $P$ when it is clear in the context.  \section{Quadtree}


In Figure \ref{Figure:QuadTree}, we show an algorithm $\QuadTree$ that outputs crude estimations of the nearest distances simulatenously for a set of points. The estimation guarantee is the same as the $\mathtt{CrudeNN}$ algorithm in \cite{BIJ24}. While \cite{BIJ24} achieves this using a quadtree with $\log n$ {\it independent} levels, which naturally introduce a $\log n$ runtime overhead, we show that two compressed quadtrees with dependent levels suffice. Our construction of compressed quadtrees is a generalization of \cite{C08} to high dimensions.



\begin{figure}[ht!]
	\begin{customizedFrame}{\QuadTree}
		
		\begin{flushleft}
			\noindent {\bf Input:} Two size-$n$ subsets $Q := \{q_i\}_{i \in [n]}$ and $P := \{p_i\}_{i \in [n]}$ of a metric space $(\R^d, \lVert \cdot \rVert_1)$, such that $Q, P \subset [0, \alpha]^d$ for some bound $\alpha = \poly(n)$.
			
			\noindent {\bf Output:} A set of $n$ values $\{\calD_i\}_{i \in [n]}$, such that every $\calD_i \in \R$ satisfies $\calD_i \geq \opt^P_{q_i}$.

            \begin{enumerate}
				\item\label{Line:draw} Let $t = \lceil\log (\alpha)\rceil+1$. Sample two uniformly random points $z, z' \sim [0, 2^{t-1}]^d$. For any point $x \in [0, \alpha]^d$, define \[h(x) := (\lceil\vec{x}_1+\vec{z}_1\rceil, \lceil\vec{x}_2+\vec{z}_2\rceil, \cdots, \lceil\vec{x}_d+\vec{z}_d\rceil),\]
                \[h'(x) := (\lceil\vec{x}_1+\vec{z'}_1\rceil, \lceil\vec{x}_2+\vec{z'}_2\rceil, \cdots, \lceil\vec{x}_d+\vec{z'}_d\rceil),\]

                where $\vec{x}_i, \vec{z}_i, \vec{z'}_i$ are the $i$-th coordinates of $x, z, z'$, respectively.

                \item\label{Line:transpose} For each $x \in Q \cup P$:

                \begin{itemize}
                    \item Compute $h(x)$ and write each element of $h(x)$ as a $t$-bit binary string. Then $h(x)$ can be viewed as a $d$-by-$t$ binary matrix stored in the row-major order, whose $(i, j)$-th entry is the $j$-th significant bit of the $i$-th element of $h(x)$. 
                Transpose this matrix and concatenate the rows of the transpose. Denote the resulting binary string as $h(x)^\top$. 

                \item Similarly, compute $h'(x)^\top$.
                \end{itemize}

                \item Use $h(x)^\top$ as keys to sort all $x \in Q \cup P$. Also, use $h'(x)^\top$ as keys to sort all $x \in Q \cup P$.
                
                \item For each $q_i \in Q$:
                
                \begin{itemize}
                    \item Use the sort to find a $p \in P$ that maximizes the length $l$ of the longest common prefix of $h(q_i)^\top$ and $h(p)^\top$. Similarly, find a $p' \in P$ that maximizes the length $l'$ of the longest common prefix of $h'(q_i)^\top$ and $h'(p')^\top$.

                    \item If $l \geq l'$ then output $\mathcal{D}_i := \lVert q_i - p\rVert_1$; otherwise, output $\mathcal{D}_i := \lVert q_i - p'\rVert_1$.
                \end{itemize}
            \end{enumerate}

		\end{flushleft}
	\end{customizedFrame}
	\caption{The $\QuadTree$ Algorithm.}\label{Figure:QuadTree}
\end{figure}

\ \\
{\bf Correctness:} 
For any $x \in [0, \alpha]^d$ and any integer $k$ such that $0 \leq k \leq t$, let $h_k(x) := (\lceil\frac{\vec{x}_1+\vec{z}_1}{2^k}\rceil, \lceil\frac{\vec{x}_2+\vec{z}_2}{2^k}\rceil, $ $\cdots, \lceil\frac{\vec{x}_d+\vec{z}_d}{2^k}\rceil)$, where $z$ is the random point drawn on Line \ref{Line:draw} in Figure \ref{Figure:QuadTree}. 
Observe that $h_k(x)$ is related to the prefix of $h(x)^\top$.

\begin{clm}\label{Claim:prefix}
    Let $q, p \in [0, \alpha]^d$ be arbitrary. For any integer $k$ such that $0 \leq k \leq t$, $h_k(q) = h_k(p)$ if and only if $h(q)^\top$ and $h(p)^\top$ share a common prefix of length at least $d(t-k)$.
\end{clm}

\begin{proof}\textcolor{red}{TOPROVE 0}\end{proof}

Claim \ref{Claim:prefix} justifies using $h_k(\cdot)$'s as an alternative representation of the binary string $h(\cdot)^\top$. \cite{BIJ24} shows that $h_k$ has a locality-sensitive property, which will help us bound the distance between points.












\begin{clm}[Lemma A.4 of \cite{BIJ24}]\label{Claim:cite}
    For any fixed integer $k$ such that $0 \leq k \leq t$ and any two points $q, p \in [0, \alpha]^d$,
\[
\pr{h_k(q) \neq h_k(p)} \leq \frac{\lVert q- p \rVert_1}{2^k},
\]\[
\pr{h_k(q) = h_k(p)} \leq \exp{(-\frac{\lVert q- p \rVert_1}{2^k})},
\]
where the probabilities are over the random choice of $z$.
\end{clm}


We now show that if two points have the same hash $h_k$, then their distance is likely not too much greater than $2^k$. A straight-forward bound follows from the diameter of the $d$-dimensional cube.

\begin{lem}\label{Lemma:cube}
    For all $q \in Q$, $p \in P$, and $0 \leq k \leq t$, the following always holds: If $h_k(q) = h_k(p)$ then $\lVert  q- p \rVert_1  \leq 2^k\cdot d$.
\end{lem}

\begin{proof}\textcolor{red}{TOPROVE 1}\end{proof}

Moreover, using Claim \ref{Claim:cite}, we can bound this ratio with respect to $n$.

\begin{lem}\label{Lemma:logn}
With probability at least $1-\OO(1/n)$, the following holds simultaneously for all $q \in Q$, $p \in P$, and $0 \leq k \leq t$: If $h_k(q) = h_k(p)$ then $\lVert  q- p \rVert_1  \leq 2^k\cdot 3\log n$.
\end{lem}

\begin{proof}\textcolor{red}{TOPROVE 2}\end{proof}




Symmetrically, if we define $h'_k(x) := (\lceil\frac{\vec{x}_1+\vec{z'}_1}{2^k}\rceil, \lceil\frac{\vec{x}_2+\vec{z'}_2}{2^k}\rceil, \cdots, \lceil\frac{\vec{x}_d+\vec{z'}_d}{2^k}\rceil)$, the  claims and lemmas above also hold for $h'_k$.
Using these, we show that the expected outputs of the $\QuadTree$ algorithm are (crude) estimations of the nearest neighbor distances. 

\begin{theorem}\label{Theorem:quadtree}
    With probability at least $1 - \OO(1/n)$, it holds for all $q_i \in Q$ that $\e{\mathcal{D}_i} \leq 5 \min (d, 3\log n) \cdot \opt_{q_i}^P$.
\end{theorem}

\begin{proof}\textcolor{red}{TOPROVE 3}\end{proof}
\ \\
{\bf Runtime analysis:}  




\begin{lem}[Line \ref{Line:transpose}]
    For any $x \in Q\cup P$, $h(x)^\top$ (and $h'(x)^\top$) can be computed in $\OO(d \log \log n)$ time.
    
\end{lem}

\begin{proof}\textcolor{red}{TOPROVE 4}\end{proof}

\begin{theorem}
    The $\QuadTree$ algorithm runs in $\OO(nd \log\log n)$ time.
\end{theorem}

\begin{proof}\textcolor{red}{TOPROVE 5}\end{proof} \section{Tournament}
\label{s:tour}
In this section, we compute the $2$-approximation of the nearest neighbor distances for logarithmically many queries. We do so using a depth-$2$ tournament: we first partition input points into random groups, project them to a lower dimensional space, and collect the nearest neighbor in the projected space in every group as a set $\tilde{S}$. Then the final output of the tournament is the nearest neighbor among points in $\tilde{S}$ in the original space. Intuitively, because each random group is small,  the true nearest neighbor could only lose to another near neighbor in the first step. Then in the second step, $\tilde{S}$ should contain at least one near neighbor.

\ \\
{\bf Notation:} We use the same notation ${{D}} := \min{(d, 3\log n)}$ as in the previous section. For any finite subset $T \subset \R$, let $\med T \in \R$ denote the median of $T$.

When working under the $\ell_1$ norm, we use Cauchy random variables to project points. We first recall a standard bound on the median of projections, which will be useful for our analysis. (The following lemma essentially follows from Claim 2 and Lemma 2 in~\cite{indyk2006stable}; we reprove it in the appendix for completeness.)

\begin{lem}\label{Lemma:comparison}
    Let $x, y \in \R^d$ and $0 < c <1/2$. Sample $r$ random vectors $v_1, v_2, \cdots, v_r \sim (\mathsf{Cauchy}(0, 1))^d$. With probability at least $1 -2e^{-rc^2/50}$, $\med\{|v_i \cdot (x -y) | : i \in [r]\} \in (1\pm c) \lVert x - y\rVert_1$.
\end{lem}


In Figure \ref{Figure:Tournament}, we describe how to construct a data structure to find $2$-approximate nearest neighbors. The construction borrows ideas from the second algorithm of \cite{K97}, but using a tournament of depth $2$ instead of $\OO(\log n)$.

\begin{figure}[ht!]
	\begin{customizedFrame}{\Tournament}
		
		\begin{flushleft}
			\noindent {\bf Input:} A set of $t$ queries $\{q_i\}_{i \in [t]}$ and a set of $n$ points $P$, which are both subsets of a metric space $(\R^d, \|\cdot\|_1)$.
			
			\noindent {\bf Output:} A set of $t$ values $\{\calD_i\}_{i \in [t]}$, such that every $\calD_i \in \R$ satisfies $\calD_i \geq \opt^P_{q_i}$.

            \paragraph{Building the Data Structure.}

            \begin{enumerate}
				\item Let $r \geq 800(2\log t + \log\log n)$.
                
                \item For each $j \in [r]$, draw  $v_j \sim (\mathsf{Cauchy}(0, 1))^d$, compute $v_j \cdot p$ for all points $p \in P$, and store all $v_j$ and $v_j \cdot p$.

                \item\label{Line:partition} Randomly partition $P$ into $n/ \log n $ subsets $P_1, P_2, \cdots, P_{n/ \log n}$, each of size $\log n$.
            \end{enumerate}

            

            \paragraph{Processing the  Queries.} For each query $q := q_i$ for $i \in [t]$: 



            
            \begin{enumerate}
            \item Compute $v_j \cdot q$ for all $j \in [r]$. 




\item\label{Line:add} Let $\tilde{S}$ be an empty set. For each $k \in [n/\log n]$:

\begin{itemize}
    \item Compute $\med_{p} := \med\{|v_j \cdot (q -p) | : j \in [r]\}$ for every $p \in P_k$.

    \item Find $p := \argmin_{p \in P_k}\{\med_{p}\}$ and add it into $\tilde{S}$.
\end{itemize}


\item \label{Line:output} 
Output $\calD_i := \min_{p \in \tilde{S}} \lVert q- p \rVert_1$ by computing and comparing all {\it exact} distances $\lVert q - p\rVert_1$ for $p \in \tilde{S}$.



\end{enumerate}
			
		\end{flushleft}
	\end{customizedFrame}
	\caption{The $\Tournament$ Algorithm.}\label{Figure:Tournament}
\end{figure}


\ \\
{\bf Correctness:}

We fix a query $q := q_i$. 








\begin{lem}\label{Lemma:small}
   With probability at least $1- \frac{1}{10t}$, $\calD_i \leq 2\opt_q$. 
\end{lem}

We let $S$ denote the set of all $2$-approximate nearest neighbors to $q$, i.e., $S := \{p \in P : \lVert q-p \rVert_1 \leq 2 \opt_{q} \}$, and let $p^*\in P$ denote a nearest neighbor of $q$, i.e. $\lVert q-p^*\rVert_1 = \opt_q$. 
To prove Lemma \ref{Lemma:small}, we first make the following observation: 

\begin{lem}\label{Lemma:unique}
   Let $P'$ be an  arbitrary subset of $P \setminus S$. The probability that there exists $p \in P'$ such that $\med_p \leq \med_{p^*}$, where $\med_{p} := \med\{|v_j \cdot (q -p) | : j \in [r]\}$ and $\med_{p^*} := \med\{|v_j \cdot (q -p^*) | : j \in [r]\}$, is at most  $\frac{2(|P'|+1)}{t^2\log n}$. 
\end{lem}

\begin{proof}\textcolor{red}{TOPROVE 6}\end{proof}

In Line $\ref{Line:partition}$ of the data structure building procedure, the point $p^*$ is assigned to one of the subsets $P^* \in \{P_1, P_2, $ $\cdots, P_{n/\log n}\}$. Focusing on this subset $P^*$, we can show that with high probability, either $p^*$ is added into $\tilde{S}$, or $p^*$ loses to another 2-approximate nearest neighbor. In both cases, the data structure is guaranteed to output an 2-approximation.
















\begin{proof}\textcolor{red}{TOPROVE 7}\end{proof}

Applying a union bound on Lemma \ref{Lemma:small}, we get the correctness guarantee:

\begin{theorem}
    Given $t$ queries $\{q_i\}_{i \in [t]}$, with probability at least $9/10$, the $\Tournament$ algorithm outputs $2$-approximate nearest neighbors simulataneously for all $t$ queries.
\end{theorem}

Finally, we state the runtime guarantee as follows:

\begin{theorem}\label{Theorem:tournamentTime}
    The $\Tournament$ algorithm runs in $\OO(n (d+ t) (\log t + \log \log n) + dt^2 \log t \log n) $ time.
\end{theorem}


\begin{proof}\textcolor{red}{TOPROVE 8}\end{proof}

For our purpose of estimating the Chamfer distance, we will apply the $\Tournament$ algorithm with a number of queries $t = \Theta({{D}}/\eps^2)$ for ${{D}} = \min{(d, 3\log n)}$ and some $\eps > 0$ satisfying $\eps^{-2} = \OO(\frac{n}{\log n})$. Under this setting, the runtime is dominated by the first additive term of Theorem \ref{Theorem:tournamentTime}, which is at most $\OO(n d(\log\log n+\log\frac{1}{\eps})/\eps^2)$.





 \section{Rejection Sampling}
\ \\
{\bf Notation:} All occurrences of $\opt$ in this section are with respect to the set $B$. Let $\eps > 0$ be our target approximation factor. We call the distribution $\calP$ an $f$-Chamfer distribution for some $f = f(n, d, \eps)$, if it is supported on $A$ and for every $a \in A$,
\[f\frac{\opt_a}{\mathsf{CH}(A, B)} \leq \calP(a), \text{where we denote } \calP(a) := \ppr{x \sim \calP}{x = a}.\] 

We first show a general bound for estimating the Chamfer distance using samples from a Chamfer distribution. This follows from a standard analysis of importance sampling. 
\begin{lem}\label{Lemma:general}
    Let $X := \{x_i\}_{i \in [t]}$ be a set of $t$ samples drawn from a $f$-chamfer distribution $\calP$. Fix $h = h(n, d, \eps) \geq 1$. Given an arbitrarily $\tilde{\opt}_{x_i}$ for every $x_i$ that satisfies $\opt_{x_i} \leq \tilde{\opt}_{x_i} \leq h\cdot \opt_{x_i}$, then for any $0 < \kappa < 1$,
    \[\bpr{\tilde{\mathsf{CH}}(A, B) \leq (1-\kappa) \mathsf{CH}(A, B) } +  \bpr{\tilde{\mathsf{CH}}(A, B) \geq (1+\kappa) \cdot h \cdot \mathsf{CH}(A, B)} \leq \frac{\frac{h^2}{f}-1}{t \cdot \kappa^2}, \] where
    $\tilde{\mathsf{CH}}(A, B) := \frac{\sum_{i \in [t]}\tilde{\opt}_{x_i} / \calP(x_i) }{t }.$
\end{lem}

\begin{proof}\textcolor{red}{TOPROVE 9}\end{proof}


In this section, we aim to construct a set of samples $S = \{s_j\}_{j \in [s]}$ for some large enough $s$, such that
each $s_j$ is drawn from a fixed $\OO(1)$-Chamfer distribution. Once we have $S$, we can compute a weighted sum of the nearest neighbor distances for $s_j \in S$, and invoke Lemma \ref{Lemma:general} to show that it is likely an $(1+\eps)$-estimation of $\mathsf{CH}(A, B)$.

We will construct such $S$ via a two-step sampling procedure: in the first step, we sample $\Theta({{D}}/\eps^2)$ points from $A$ using a distribution defined by the estimations from the $\QuadTree$ algorithm. In the second step, we subsample these $\Theta({{D}}/\eps^2)$ points, using an acceptance probability defined by the estimations from the $\Tournament$ algorithm. We describe our \textbf{Chamfer-Estimate} algorithm in Figure \ref{Figure:Chamfer}.

\begin{figure}[ht!]
	\begin{customizedFrame}{Chamfer-Estimate}
		
		\begin{flushleft}
			\noindent {\bf Input:} Two subsets $A,B$ of a metric space $(\R^d, \|\cdot\|_1)$ of size $n$, a parameter $\eps > 0$, and a parameter $q \in \N$.
			
			\noindent {\bf Output:} An estimated value  $\tilde{\mathsf{CH}}(A, B) \in \R$.
			
			\begin{enumerate}
				\item\label{Line:QuadTree} Execute the algorithm $\QuadTree(A, B)$, and let the output be a set of values $\{ \calD_a \}_{a \in A}$ which always satisfy $\calD_a \geq \opt_a$. Let $\calD := \sum_{a \in A} \calD_a $. 
                
				\item\label{Line:distribution} Construct a probability distribution $\calP$ supported on $A$ such that for every $a \in A$, $\calP(a) = \frac{\calD_a}{\calD}$. For $i \in [q]$, sample $x_i \sim \calP$.
                
				\item\label{Line:Tournament} Execute the algorithm $\Tournament(\{x_i\}_{i \in [q]}, B)$, and let the output be a set of values $\{ \calD'_{x_i} \}_{i \in [q]}$ which always satisfy $\calD'_{x_i} \geq \opt_{x_i}$. Let $\calD' := \sum_{i \in [q]} \frac{\calD'_{x_i}}{\calP(x_i)} / q$ and denote $\calP'(a) := \frac{\calD'_{a}}{\calD'}$ (which is well-defined only if $a = x_i$ for some $i \in [q]$).
                
				\item\label{Line:sample} Define
                \[M := \max_{i \in [q]} \frac{\calP'(x_i)}{\calP(x_i)}.\]
                For each $i \in [q]$, mark $x_i$ as \textsc{accepted} with probability $\frac{\calP'(x_i)}{M \cdot \calP(x_i)}$. 
                
                If the number of \textsc{accepted} $x_i$ is less than $s=10/\eps^2$ then output \textbf{Fail} and exit the algorithm. Otherwise, collect the first $s$ \textsc{accepted} $x_i$ as a set $S := \{s_j\}_{j \in [s]}$.

                \item\label{Line:exact} Compute $\opt_{s_j}$ for each $j$. Output \[\tilde{\mathsf{CH}}(A, B) := \sum_{j \in [s]} \frac{\opt_{s_j}}{\calP'(s_j)} / s.\]
			\end{enumerate}
		\end{flushleft}
	\end{customizedFrame}
	\caption{The \textbf{Chamfer-Estimate} Algorithm.}\label{Figure:Chamfer}
\end{figure}

The \textbf{Chamfer-Estimate} algorithm applies the $\QuadTree$ algorithm and the $\Tournament$ algorithm as subroutines. If they are executed successfully, their outputs should satisfy the following conditions:


\begin{cond}\label{Condition:QuadTree}
    We say the $\QuadTree$ algorithm succeeds if for every $a \in A$, $\e{\calD_a} \leq 5{{D}} \cdot \opt_{a}$.
\end{cond}

\begin{cond}\label{Condition:Tournament}
    We say the $\Tournament$ algorithm succeeds if for every $x_i$ for $i \in [q]$, $\calD'_{x_i} \leq 2\opt_{x_i}$. 
\end{cond}

That is,  as described in the introduction, we need $\QuadTree$ to provide $\OO(\log n)$-approximation (to ensure that the sample size $q$ can be at most logarithmic in $n$), and that $\Tournament$ provide $\OO(1)$-approximation (to ensure that the final estimator using $s$ samples has variance bounded by a constant). 

We state some facts about the \textbf{Chamfer-Estimate} algorithm, which will be useful for our analysis. 



\begin{clm}[Line \ref{Line:distribution}]\label{Claim:P}
    Under Condition \ref{Condition:QuadTree}, with probability at least $9/10$, $\calP$ is a $(1/50{{D}})$-Chamfer Distribution.
\end{clm}

\begin{proof}\textcolor{red}{TOPROVE 10}\end{proof}

\begin{clm}[Line \ref{Line:Tournament}]\label{Claim:D'}       Let $q \geq 10^4 {{D}}$. Under Condition \ref{Condition:QuadTree} and \ref{Condition:Tournament}, with probability at least $4/5$, $\calD' \geq \mathsf{CH}(A, B)/2$. 
\end{clm}

\begin{proof}\textcolor{red}{TOPROVE 11}\end{proof}

{\bf Analysis of $S$:} We now show that the set $S$ on Line \ref{Line:sample} collects enough samples (thus the algorithm does not fail) and is equivalent to sampling from a $\OO(1)$-Chamfer distribution $\calQ$. We note that the algorithm, in fact, only knows a $(1/50{{D}})$-Chamfer distribution $\calP$ and probabilities $\calP'(x_i)$ for $\{x_i\}_{i \in [q]}$, so it cannot explicitly sample from such $\calQ$. Nevertheless, by a standard analysis of rejection sampling, we show that $S$ ``simulates'' sampling from $\calQ$.

\begin{lem}\label{Lemma:size}
    Let $q \geq 10^4{{D}}/\eps^2$. Under Condition \ref{Condition:QuadTree} and \ref{Condition:Tournament}, with probability at least $3/5$, the number of \textsc{accepted} $x_i$ is at least $s$, so the algorithm does not fail.
\end{lem}

\begin{proof}\textcolor{red}{TOPROVE 12}\end{proof}



\begin{lem}\label{Lemma:equivalent}
    Each $s_j$ is independently and identically distributed, and under Condition \ref{Condition:Tournament}, $\pr{s_j = a} \geq \frac{\opt_a}{2\mathsf{CH}(A, B)}$ for any $a \in A$.
\end{lem}

\begin{proof}\textcolor{red}{TOPROVE 13}\end{proof}


Lemma \ref{Lemma:size} and \ref{Lemma:equivalent} together say that $S$ can be viewed as a set of $s$ samples from a $\frac{1}{2}$-Chamfer Distribution, thus we can invoke another importance sampling analysis. In the final step of the algorithm, we compute the exact nearest neighbor distance for all $s_j$ and then compute a weighted sum over them. With high probability, this gives an $(1\pm\eps)$-estimation of $\mathsf{CH}(A, B)$.

\begin{theorem}
    Under Condition \ref{Condition:QuadTree} and \ref{Condition:Tournament}, {\normalfont \textbf{Chamfer-Estimate}($A, B, \eps, q \geq 10^4{{D}}/\eps^2$)} outputs $\tilde{\mathsf{CH}}(A, B)$ that satisfies $(1-\eps)\mathsf{CH}(A, B) \leq \tilde{\mathsf{CH}}(A, B) \leq (1+\eps)\mathsf{CH}(A, B)$ with probability at least $1/2$.
\end{theorem}

\begin{proof}\textcolor{red}{TOPROVE 14}\end{proof}


\begin{theorem}
    {\normalfont \textbf{Chamfer-Estimate}($A, B, \eps, q = 10^4{{D}}/\eps^2$)} runs in time $\OO(nd(\log\log n + \log \frac{1}{\eps})/\eps^2))$.
\end{theorem}

\begin{proof}\textcolor{red}{TOPROVE 15}\end{proof} 


\nocite{langley00}


\section*{Acknowledgements}
Ying Feng was supported by an MIT Akamai Presidential Fellowship. Piotr Indyk was supported in part by the NSF TRIPODS program (award DMS-2022448).The authors would like to thank Anders Aamand for helpful comments that helped simplify the algorithm in Section~\ref{s:tour}.

\def\shortbib{0}
\bibliographystyle{alpha}
\bibliography{ref}




\appendix

\section{Reducing the Bit Precision of Inputs.}

In our algorithm, we assumed that all points in input sets $A, B$ are integers in $\{1,2 , \cdots, $ $\poly(n)\}^d$. Here, we show that this is without loss of generality, as long as all coordinates of the original input are $w$-bit integers for arbitrary $w \geq \log n$ in a unit-cost RAM with a word length
of $w$ bits.

Section $A.3$ of \cite{BIJ24} gives an efficient reduction from real inputs to the case that \[1 \leq \min_{a \in A, b \in B} \lVert a - b\rVert_1  \leq \max_{a \in A, b \in B} \lVert a - b\rVert_1 \leq \poly(n),\] i.e., the input has a $\poly(n)$-bounded aspect ratio. Their reduction can be adapted to our case as follows: 


\begin{clm}[Lemma $A.3$ of \cite{BIJ24}]
    Given an $\mathsf{est}$ such that $\mathsf{CH}(A, B) \leq  \mathsf{est} \leq \poly(n) \cdot \mathsf{CH}(A, B)$, if there exists an algorithm that computes an $(1+\eps)$-approximation to $\mathsf{CH}(A, B)$ in $\OO(nd(\log\log n+\log\frac{1}{\eps})/\eps^2))$ time under the assumption that $A, B$ contain points from 
    $\{1 \ldots \mbox{\poly}(n) \}^d$, then there exists an algorithm that computes an $(1+\eps)$-approximation to $\mathsf{CH}(A, B)$ for any 
 integer-coordinate $A, B$ in asymptotically same time.
\end{clm}

It remains to show how to obtain a $\poly(n)$-approximation.

\begin{lem}
    There exists an $\OO(nd + n\log\log n)$-time algorithm that computes $\mathsf{est}$ which satisfies $\mathsf{CH}(A, B) \leq  \mathsf{est} \leq \poly(n)\cdot \mathsf{CH}(A, B)$ with $1-\frac{1}{n}$ probability.
\end{lem}

\begin{proof}\textcolor{red}{TOPROVE 16}\end{proof}

 \section{Proof of Lemma \ref{Lemma:comparison}}

\begin{proof}\textcolor{red}{TOPROVE 17}\end{proof} 
\end{document}
