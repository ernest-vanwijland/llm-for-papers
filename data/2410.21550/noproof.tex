\documentclass{article}
\usepackage{bm} \usepackage{amsmath} \usepackage{amssymb} \usepackage{amsthm} \usepackage[boxed]{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{pifont} \usepackage{braket}


\newcommand{\labs}{\left|}
\newcommand{\rabs}{\right|}
\newcommand{\lnorm}{\left\|}
\newcommand{\rnorm}{\right\|}
\newcommand{\lcurly}{\left\{}
\newcommand{\rcurly}{\right\}}
\newcommand{\lbrac}{\left[}
\newcommand{\rbrac}{\right]}
\newcommand{\lpar}{\left(}
\newcommand{\rpar}{\right)}


\algnewcommand\algorithmicinput{\textbf{Input:}}
\algnewcommand\Input{\item[\algorithmicinput]}
\algnewcommand\algorithmicoutput{\textbf{Output:}}
\algnewcommand\Output{\item[\algorithmicoutput]}
\algnewcommand\algorithmicalgorithm{\textbf{Algorithm:}}
\algnewcommand\Algorithm{\item[\algorithmicalgorithm]}
\renewcommand\algorithmicthen{{\bf:}}
\renewcommand\algorithmicdo{{\bf:}}
\renewcommand{\algorithmicrequire}{\textbf{Requires:}}
\renewcommand{\algorithmicensure}{\textbf{Ensures:}}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{claim}{Claim}[section]
\newtheorem{fact}{Fact}[section]
\newtheorem{remark}{Remark}[section]
\newtheorem{definition}{Definition}[section]
\newtheorem{problem}{Problem}[section]
\newtheorem{conjecture}{Conjecture}[section]
\newtheorem{question}{Question}[section]
\newtheorem{assumption}{Assumption}[section]

\DeclareMathOperator{\nnz}{nnz}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\trace}{trace}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\range}{range}
\DeclareMathOperator{\im}{im}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\poly}{poly}
\DeclareMathOperator{\polylog}{polylog}
\DeclareMathOperator{\vecspan}{span}
\DeclareMathOperator{\vol}{vol}
\DeclareMathOperator{\gap}{\Delta}
\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\erf}{erf}
\DeclareMathOperator{\round}{round}
\DeclareMathOperator{\grid}{grid}
\DeclareMathOperator{\length}{length}

\newcommand\veca{\boldsymbol{\mathrm{a}}}
\newcommand\vecb{\boldsymbol{\mathrm{b}}}
\newcommand\vecc{\boldsymbol{\mathrm{c}}}
\newcommand\vecd{\boldsymbol{\mathrm{d}}}
\newcommand\vece{\boldsymbol{\mathrm{e}}}
\newcommand\vecf{\boldsymbol{\mathrm{f}}}
\newcommand\vecg{\boldsymbol{\mathrm{g}}}
\newcommand\vech{\boldsymbol{\mathrm{h}}}
\newcommand\veci{\boldsymbol{\mathrm{i}}}
\newcommand\vecj{\boldsymbol{\mathrm{j}}}
\newcommand\veck{\boldsymbol{\mathrm{k}}}
\newcommand\vecl{\boldsymbol{\mathrm{l}}}
\newcommand\vecm{\boldsymbol{\mathrm{m}}}
\newcommand\vecn{\boldsymbol{\mathrm{n}}}
\newcommand\veco{\boldsymbol{\mathrm{o}}}
\newcommand\vecp{\boldsymbol{\mathrm{p}}}
\newcommand\vecq{\boldsymbol{\mathrm{q}}}
\newcommand\vecr{\boldsymbol{\mathrm{r}}}
\newcommand\vecs{\boldsymbol{\mathrm{s}}}
\newcommand\vect{\boldsymbol{\mathrm{t}}}
\newcommand\vecu{\boldsymbol{\mathrm{u}}}
\newcommand\vecv{\boldsymbol{\mathrm{v}}}
\newcommand\vecw{\boldsymbol{\mathrm{w}}}
\newcommand\vecx{\boldsymbol{\mathrm{x}}}
\newcommand\vecy{\boldsymbol{\mathrm{y}}}
\newcommand\vecz{\boldsymbol{\mathrm{z}}}

\newcommand\vecatilde{\widetilde{\boldsymbol{\mathrm{a}}}}
\newcommand\vecbtilde{\widetilde{\boldsymbol{\mathrm{b}}}}
\newcommand\vecctilde{\widetilde{\boldsymbol{\mathrm{c}}}}
\newcommand\vecdtilde{\widetilde{\boldsymbol{\mathrm{d}}}}
\newcommand\vecetilde{\widetilde{\boldsymbol{\mathrm{e}}}}
\newcommand\vecftilde{\widetilde{\boldsymbol{\mathrm{f}}}}
\newcommand\vecgtilde{\widetilde{\boldsymbol{\mathrm{g}}}}
\newcommand\vechtilde{\widetilde{\boldsymbol{\mathrm{h}}}}
\newcommand\vecitilde{\widetilde{\boldsymbol{\mathrm{i}}}}
\newcommand\vecjtilde{\widetilde{\boldsymbol{\mathrm{j}}}}
\newcommand\vecktilde{\widetilde{\boldsymbol{\mathrm{k}}}}
\newcommand\vecltilde{\widetilde{\boldsymbol{\mathrm{l}}}}
\newcommand\vecmtilde{\widetilde{\boldsymbol{\mathrm{m}}}}
\newcommand\vecntilde{\widetilde{\boldsymbol{\mathrm{n}}}}
\newcommand\vecotilde{\widetilde{\boldsymbol{\mathrm{o}}}}
\newcommand\vecptilde{\widetilde{\boldsymbol{\mathrm{p}}}}
\newcommand\vecqtilde{\widetilde{\boldsymbol{\mathrm{q}}}}
\newcommand\vecrtilde{\widetilde{\boldsymbol{\mathrm{r}}}}
\newcommand\vecstilde{\widetilde{\boldsymbol{\mathrm{s}}}}
\newcommand\vecttilde{\widetilde{\boldsymbol{\mathrm{t}}}}
\newcommand\vecutilde{\widetilde{\boldsymbol{\mathrm{u}}}}
\newcommand\vecvtilde{\widetilde{\boldsymbol{\mathrm{v}}}}
\newcommand\vecwtilde{\widetilde{\boldsymbol{\mathrm{w}}}}
\newcommand\vecxtilde{\widetilde{\boldsymbol{\mathrm{x}}}}
\newcommand\vecytilde{\widetilde{\boldsymbol{\mathrm{y}}}}
\newcommand\vecztilde{\widetilde{\boldsymbol{\mathrm{z}}}}




\newcommand\vecahat{\widehat{\boldsymbol{\mathrm{a}}}}
\newcommand\vecbhat{\widehat{\boldsymbol{\mathrm{b}}}}
\newcommand\vecchat{\widehat{\boldsymbol{\mathrm{c}}}}
\newcommand\vecdhat{\widehat{\boldsymbol{\mathrm{d}}}}
\newcommand\vecehat{\widehat{\boldsymbol{\mathrm{e}}}}
\newcommand\vecfhat{\widehat{\boldsymbol{\mathrm{f}}}}
\newcommand\vecghat{\widehat{\boldsymbol{\mathrm{g}}}}
\newcommand\vechhat{\widehat{\boldsymbol{\mathrm{h}}}}
\newcommand\vecihat{\widehat{\boldsymbol{\mathrm{i}}}}
\newcommand\vecjhat{\widehat{\boldsymbol{\mathrm{j}}}}
\newcommand\veckhat{\widehat{\boldsymbol{\mathrm{k}}}}
\newcommand\veclhat{\widehat{\boldsymbol{\mathrm{l}}}}
\newcommand\vecmhat{\widehat{\boldsymbol{\mathrm{m}}}}
\newcommand\vecnhat{\widehat{\boldsymbol{\mathrm{n}}}}
\newcommand\vecohat{\widehat{\boldsymbol{\mathrm{o}}}}
\newcommand\vecphat{\widehat{\boldsymbol{\mathrm{p}}}}
\newcommand\vecqhat{\widehat{\boldsymbol{\mathrm{q}}}}
\newcommand\vecrhat{\widehat{\boldsymbol{\mathrm{r}}}}
\newcommand\vecshat{\widehat{\boldsymbol{\mathrm{s}}}}
\newcommand\vecthat{\widehat{\boldsymbol{\mathrm{t}}}}
\newcommand\vecuhat{\widehat{\boldsymbol{\mathrm{u}}}}
\newcommand\vecvhat{\widehat{\boldsymbol{\mathrm{v}}}}
\newcommand\vecwhat{\widehat{\boldsymbol{\mathrm{w}}}}
\newcommand\vecxhat{\widehat{\boldsymbol{\mathrm{x}}}}
\newcommand\vecyhat{\widehat{\boldsymbol{\mathrm{y}}}}
\newcommand\veczhat{\widehat{\boldsymbol{\mathrm{z}}}}




\newcommand\vecat{\boldsymbol{\mathrm{a}}^\top}
\newcommand\vecbt{\boldsymbol{\mathrm{b}}^\top}
\newcommand\vecct{\boldsymbol{\mathrm{c}}^\top}
\newcommand\vecdt{\boldsymbol{\mathrm{d}}^\top}
\newcommand\vecet{\boldsymbol{\mathrm{e}}^\top}
\newcommand\vecft{\boldsymbol{\mathrm{f}}^\top}
\newcommand\vecgt{\boldsymbol{\mathrm{g}}^\top}
\newcommand\vecht{\boldsymbol{\mathrm{h}}^\top}
\newcommand\vecit{\boldsymbol{\mathrm{i}}^\top}
\newcommand\vecjt{\boldsymbol{\mathrm{j}}^\top}
\newcommand\veckt{\boldsymbol{\mathrm{k}}^\top}
\newcommand\veclt{\boldsymbol{\mathrm{l}}^\top}
\newcommand\vecmt{\boldsymbol{\mathrm{m}}^\top}
\newcommand\vecnt{\boldsymbol{\mathrm{n}}^\top}
\newcommand\vecot{\boldsymbol{\mathrm{o}}^\top}
\newcommand\vecpt{\boldsymbol{\mathrm{p}}^\top}
\newcommand\vecqt{\boldsymbol{\mathrm{q}}^\top}
\newcommand\vecrt{\boldsymbol{\mathrm{r}}^\top}
\newcommand\vecst{\boldsymbol{\mathrm{s}}^\top}
\newcommand\vectt{\boldsymbol{\mathrm{t}}^\top}
\newcommand\vecut{\boldsymbol{\mathrm{u}}^\top}
\newcommand\vecvt{\boldsymbol{\mathrm{v}}^\top}
\newcommand\vecwt{\boldsymbol{\mathrm{w}}^\top}
\newcommand\vecxt{\boldsymbol{\mathrm{x}}^\top}
\newcommand\vecyt{\boldsymbol{\mathrm{y}}^\top}
\newcommand\veczt{\boldsymbol{\mathrm{z}}^\top}

\newcommand\matA{\boldsymbol{\mathrm{A}}}
\newcommand\matB{\boldsymbol{\mathrm{B}}}
\newcommand\matC{\boldsymbol{\mathrm{C}}}
\newcommand\matD{\boldsymbol{\mathrm{D}}}
\newcommand\matE{\boldsymbol{\mathrm{E}}}
\newcommand\matF{\boldsymbol{\mathrm{F}}}
\newcommand\matG{\boldsymbol{\mathrm{G}}}
\newcommand\matH{\boldsymbol{\mathrm{H}}}
\newcommand\matI{\boldsymbol{\mathrm{I}}}
\newcommand\matJ{\boldsymbol{\mathrm{J}}}
\newcommand\matK{\boldsymbol{\mathrm{K}}}
\newcommand\matL{\boldsymbol{\mathrm{L}}}
\newcommand\matM{\boldsymbol{\mathrm{M}}}
\newcommand\matN{\boldsymbol{\mathrm{N}}}
\newcommand\matO{\boldsymbol{\mathrm{O}}}
\newcommand\matP{\boldsymbol{\mathrm{P}}}
\newcommand\matQ{\boldsymbol{\mathrm{Q}}}
\newcommand\matR{\boldsymbol{\mathrm{R}}}
\newcommand\matS{\boldsymbol{\mathrm{S}}}
\newcommand\matT{\boldsymbol{\mathrm{T}}}
\newcommand\matU{\boldsymbol{\mathrm{U}}}
\newcommand\matV{\boldsymbol{\mathrm{V}}}
\newcommand\matW{\boldsymbol{\mathrm{W}}}
\newcommand\matX{\boldsymbol{\mathrm{X}}}
\newcommand\matY{\boldsymbol{\mathrm{Y}}}
\newcommand\matZ{\boldsymbol{\mathrm{Z}}}

\newcommand\matGamma{\boldsymbol{\mathrm{\Gamma}}}
\newcommand\matDelta{\boldsymbol{\mathrm{\Delta}}}
\newcommand\matEta{\boldsymbol{\mathrm{\Eta}}}
\newcommand\matTheta{\boldsymbol{\mathrm{\Theta}}}
\newcommand\matLambda{\boldsymbol{\mathrm{\Lambda}}}
\newcommand\matXi{\boldsymbol{\mathrm{\Xi}}}
\newcommand\matPi{\boldsymbol{\mathrm{\Pi}}}
\newcommand\matSigma{\boldsymbol{\mathrm{\Sigma}}}
\newcommand\matPhi{\boldsymbol{\mathrm{\Phi}}}
\newcommand\matPsi{\boldsymbol{\mathrm{\Psi}}}
\newcommand\matOmega{\boldsymbol{\mathrm{\Omega}}}

\newcommand\matAt{\boldsymbol{\mathrm{A}}^\top}
\newcommand\matBt{\boldsymbol{\mathrm{B}}^\top}
\newcommand\matCt{\boldsymbol{\mathrm{C}}^\top}
\newcommand\matDt{\boldsymbol{\mathrm{D}}^\top}
\newcommand\matEt{\boldsymbol{\mathrm{E}}^\top}
\newcommand\matFt{\boldsymbol{\mathrm{F}}^\top}
\newcommand\matGt{\boldsymbol{\mathrm{G}}^\top}
\newcommand\matHt{\boldsymbol{\mathrm{H}}^\top}
\newcommand\matIt{\boldsymbol{\mathrm{I}}^\top}
\newcommand\matJt{\boldsymbol{\mathrm{J}}^\top}
\newcommand\matKt{\boldsymbol{\mathrm{K}}^\top}
\newcommand\matLt{\boldsymbol{\mathrm{L}}^\top}
\newcommand\matMt{\boldsymbol{\mathrm{M}}^\top}
\newcommand\matNt{\boldsymbol{\mathrm{N}}^\top}
\newcommand\matOt{\boldsymbol{\mathrm{O}}^\top}
\newcommand\matPt{\boldsymbol{\mathrm{P}}^\top}
\newcommand\matQt{\boldsymbol{\mathrm{Q}}^\top}
\newcommand\matRt{\boldsymbol{\mathrm{R}}^\top}
\newcommand\matSt{\boldsymbol{\mathrm{S}}^\top}
\newcommand\matTt{\boldsymbol{\mathrm{T}}^\top}
\newcommand\matUt{\boldsymbol{\mathrm{U}}^\top}
\newcommand\matVt{\boldsymbol{\mathrm{V}}^\top}
\newcommand\matWt{\boldsymbol{\mathrm{W}}^\top}
\newcommand\matXt{\boldsymbol{\mathrm{X}}^\top}
\newcommand\matYt{\boldsymbol{\mathrm{Y}}^\top}
\newcommand\matZt{\boldsymbol{\mathrm{Z}}^\top}

\newcommand\matGammat{\boldsymbol{\mathrm{\Gamma}}^\top}
\newcommand\matDeltat{\boldsymbol{\mathrm{\Delta}}^\top}
\newcommand\matEtat{\boldsymbol{\mathrm{\Eta}}^\top}
\newcommand\matThetat{\boldsymbol{\mathrm{\Theta}}^\top}
\newcommand\matLambdat{\boldsymbol{\mathrm{\Lambda}}^\top}
\newcommand\matXit{\boldsymbol{\mathrm{\Xi}}^\top}
\newcommand\matPit{\boldsymbol{\mathrm{\Pi}}^\top}
\newcommand\matSigmat{\boldsymbol{\mathrm{\Sigma}}^\top}
\newcommand\matPhit{\boldsymbol{\mathrm{\Phi}}^\top}
\newcommand\matPsit{\boldsymbol{\mathrm{\Psi}}^\top}
\newcommand\matOmegat{\boldsymbol{\mathrm{\Omega}}^\top}

\newcommand\matAtilde{\widetilde{\boldsymbol{\mathrm{A}}}}
\newcommand\matBtilde{\widetilde{\boldsymbol{\mathrm{B}}}}
\newcommand\matCtilde{\widetilde{\boldsymbol{\mathrm{C}}}}
\newcommand\matDtilde{\widetilde{\boldsymbol{\mathrm{D}}}}
\newcommand\matEtilde{\widetilde{\boldsymbol{\mathrm{E}}}}
\newcommand\matFtilde{\widetilde{\boldsymbol{\mathrm{F}}}}
\newcommand\matGtilde{\widetilde{\boldsymbol{\mathrm{G}}}}
\newcommand\matHtilde{\widetilde{\boldsymbol{\mathrm{H}}}}
\newcommand\matItilde{\widetilde{\boldsymbol{\mathrm{I}}}}
\newcommand\matJtilde{\widetilde{\boldsymbol{\mathrm{J}}}}
\newcommand\matKtilde{\widetilde{\boldsymbol{\mathrm{K}}}}
\newcommand\matLtilde{\widetilde{\boldsymbol{\mathrm{L}}}}
\newcommand\matMtilde{\widetilde{\boldsymbol{\mathrm{M}}}}
\newcommand\matNtilde{\widetilde{\boldsymbol{\mathrm{N}}}}
\newcommand\matOtilde{\widetilde{\boldsymbol{\mathrm{O}}}}
\newcommand\matPtilde{\widetilde{\boldsymbol{\mathrm{P}}}}
\newcommand\matQtilde{\widetilde{\boldsymbol{\mathrm{Q}}}}
\newcommand\matRtilde{\widetilde{\boldsymbol{\mathrm{R}}}}
\newcommand\matStilde{\widetilde{\boldsymbol{\mathrm{S}}}}
\newcommand\matTtilde{\widetilde{\boldsymbol{\mathrm{T}}}}
\newcommand\matUtilde{\widetilde{\boldsymbol{\mathrm{U}}}}
\newcommand\matVtilde{\widetilde{\boldsymbol{\mathrm{V}}}}
\newcommand\matWtilde{\widetilde{\boldsymbol{\mathrm{W}}}}
\newcommand\matXtilde{\widetilde{\boldsymbol{\mathrm{X}}}}
\newcommand\matYtilde{\widetilde{\boldsymbol{\mathrm{Y}}}}
\newcommand\matZtilde{\widetilde{\boldsymbol{\mathrm{Z}}}}

\newcommand\matGammatilde{\widetilde{\boldsymbol{\mathrm{\Gamma}}}}
\newcommand\matDeltatilde{\widetilde{\boldsymbol{\mathrm{\Delta}}}}
\newcommand\matEtatilde{\widetilde{\boldsymbol{\mathrm{\Eta}}}}
\newcommand\matThetatilde{\widetilde{\boldsymbol{\mathrm{\Theta}}}}
\newcommand\matLambdatilde{\widetilde{\boldsymbol{\mathrm{\Lambda}}}}
\newcommand\matXitilde{\widetilde{\boldsymbol{\mathrm{\Xi}}}}
\newcommand\matPitilde{\widetilde{\boldsymbol{\mathrm{\Pi}}}}
\newcommand\matSigmatilde{\widetilde{\boldsymbol{\mathrm{\Sigma}}}}
\newcommand\matPhitilde{\widetilde{\boldsymbol{\mathrm{\Phi}}}}
\newcommand\matPsitilde{\widetilde{\boldsymbol{\mathrm{\Psi}}}}
\newcommand\matOmegatilde{\widetilde{\boldsymbol{\mathrm{\Omega}}}}

\newcommand\matAbar{\bar{\boldsymbol{\mathrm{A}}}}
\newcommand\matBbar{\bar{\boldsymbol{\mathrm{B}}}}
\newcommand\matCbar{\bar{\boldsymbol{\mathrm{C}}}}
\newcommand\matDbar{\bar{\boldsymbol{\mathrm{D}}}}
\newcommand\matEbar{\bar{\boldsymbol{\mathrm{E}}}}
\newcommand\matFbar{\bar{\boldsymbol{\mathrm{F}}}}
\newcommand\matGbar{\bar{\boldsymbol{\mathrm{G}}}}
\newcommand\matHbar{\bar{\boldsymbol{\mathrm{H}}}}
\newcommand\matIbar{\bar{\boldsymbol{\mathrm{I}}}}
\newcommand\matJbar{\bar{\boldsymbol{\mathrm{J}}}}
\newcommand\matKbar{\bar{\boldsymbol{\mathrm{K}}}}
\newcommand\matLbar{\bar{\boldsymbol{\mathrm{L}}}}
\newcommand\matMbar{\bar{\boldsymbol{\mathrm{M}}}}
\newcommand\matNbar{\bar{\boldsymbol{\mathrm{N}}}}
\newcommand\matObar{\bar{\boldsymbol{\mathrm{O}}}}
\newcommand\matPbar{\bar{\boldsymbol{\mathrm{P}}}}
\newcommand\matQbar{\bar{\boldsymbol{\mathrm{Q}}}}
\newcommand\matRbar{\bar{\boldsymbol{\mathrm{R}}}}
\newcommand\matSbar{\bar{\boldsymbol{\mathrm{S}}}}
\newcommand\matTbar{\bar{\boldsymbol{\mathrm{T}}}}
\newcommand\matUbar{\bar{\boldsymbol{\mathrm{U}}}}
\newcommand\matVbar{\bar{\boldsymbol{\mathrm{V}}}}
\newcommand\matWbar{\bar{\boldsymbol{\mathrm{W}}}}
\newcommand\matXbar{\bar{\boldsymbol{\mathrm{X}}}}
\newcommand\matYbar{\bar{\boldsymbol{\mathrm{Y}}}}
\newcommand\matZbar{\bar{\boldsymbol{\mathrm{Z}}}}

\newcommand\matGammabar{\bar{\boldsymbol{\mathrm{\Gamma}}}}
\newcommand\matDeltabar{\bar{\boldsymbol{\mathrm{\Delta}}}}
\newcommand\matEtabar{\bar{\boldsymbol{\mathrm{\Eta}}}}
\newcommand\matThetabar{\bar{\boldsymbol{\mathrm{\Theta}}}}
\newcommand\matLambdabar{\bar{\boldsymbol{\mathrm{\Lambda}}}}
\newcommand\matXibar{\bar{\boldsymbol{\mathrm{\Xi}}}}
\newcommand\matPibar{\bar{\boldsymbol{\mathrm{\Pi}}}}
\newcommand\matSigmabar{\bar{\boldsymbol{\mathrm{\Sigma}}}}
\newcommand\matPhibar{\bar{\boldsymbol{\mathrm{\Phi}}}}
\newcommand\matPsibar{\bar{\boldsymbol{\mathrm{\Psi}}}}
\newcommand\matOmegabar{\bar{\boldsymbol{\mathrm{\Omega}}}}


\newcommand\matAhat{\widehat{\boldsymbol{\mathrm{A}}}}
\newcommand\matBhat{\widehat{\boldsymbol{\mathrm{B}}}}
\newcommand\matChat{\widehat{\boldsymbol{\mathrm{C}}}}
\newcommand\matDhat{\widehat{\boldsymbol{\mathrm{D}}}}
\newcommand\matEhat{\widehat{\boldsymbol{\mathrm{E}}}}
\newcommand\matFhat{\widehat{\boldsymbol{\mathrm{F}}}}
\newcommand\matGhat{\widehat{\boldsymbol{\mathrm{G}}}}
\newcommand\matHhat{\widehat{\boldsymbol{\mathrm{H}}}}
\newcommand\matIhat{\widehat{\boldsymbol{\mathrm{I}}}}
\newcommand\matJhat{\widehat{\boldsymbol{\mathrm{J}}}}
\newcommand\matKhat{\widehat{\boldsymbol{\mathrm{K}}}}
\newcommand\matLhat{\widehat{\boldsymbol{\mathrm{L}}}}
\newcommand\matMhat{\widehat{\boldsymbol{\mathrm{M}}}}
\newcommand\matNhat{\widehat{\boldsymbol{\mathrm{N}}}}
\newcommand\matOhat{\widehat{\boldsymbol{\mathrm{O}}}}
\newcommand\matPhat{\widehat{\boldsymbol{\mathrm{P}}}}
\newcommand\matQhat{\widehat{\boldsymbol{\mathrm{Q}}}}
\newcommand\matRhat{\widehat{\boldsymbol{\mathrm{R}}}}
\newcommand\matShat{\widehat{\boldsymbol{\mathrm{S}}}}
\newcommand\matThat{\widehat{\boldsymbol{\mathrm{T}}}}
\newcommand\matUhat{\widehat{\boldsymbol{\mathrm{U}}}}
\newcommand\matVhat{\widehat{\boldsymbol{\mathrm{V}}}}
\newcommand\matWhat{\widehat{\boldsymbol{\mathrm{W}}}}
\newcommand\matXhat{\widehat{\boldsymbol{\mathrm{X}}}}
\newcommand\matYhat{\widehat{\boldsymbol{\mathrm{Y}}}}
\newcommand\matZhat{\widehat{\boldsymbol{\mathrm{Z}}}}

\newcommand\matGammahat{\widehat{\boldsymbol{\mathrm{\Gamma}}}}
\newcommand\matDeltahat{\widehat{\boldsymbol{\mathrm{\Delta}}}}
\newcommand\matEtahat{\widehat{\boldsymbol{\mathrm{\Eta}}}}
\newcommand\matThetahat{\widehat{\boldsymbol{\mathrm{\Theta}}}}
\newcommand\matLambdahat{\widehat{\boldsymbol{\mathrm{\Lambda}}}}
\newcommand\matXihat{\widehat{\boldsymbol{\mathrm{\Xi}}}}
\newcommand\matPihat{\widehat{\boldsymbol{\mathrm{\Pi}}}}
\newcommand\matSigmahat{\widehat{\boldsymbol{\mathrm{\Sigma}}}}
\newcommand\matPhihat{\widehat{\boldsymbol{\mathrm{\Phi}}}}
\newcommand\matPsihat{\widehat{\boldsymbol{\mathrm{\Psi}}}}
\newcommand\matOmegahat{\widehat{\boldsymbol{\mathrm{\Omega}}}}

\newcommand{\fl}{\boldsymbol{\mathsf{fl}}}
\newcommand{\umach}{\textbf{\textup{u}}}
\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}




 \newcommand{\QRR}{\mathsf{QRR}}
\newcommand{\QR}{\mathsf{QR}}
\newcommand{\MM}{\mathsf{MM}}
\newcommand{\BTMM}{\mathsf{MM_{BT}}}
\newcommand{\TRID}{\mathsf{TRID}}
\newcommand{\DIAGONALIZE}{\mathsf{DIAGONALIZE}}
\newcommand{\GAP}{\mathsf{GAP}}
\newcommand{\HALVE}{\mathsf{HALVE}}
\newcommand{\FLINF}{\mathsf{INF}}
\newcommand{\cmm}{\beta}
\newcommand{\cfmm}{\xi}
\newcommand{\err}{\mathsf{err}}
\newcommand{\matmulexponent}{\omega}
\newcommand{\flopcost}{\mathcal{F}}
\newcommand{\projectormatrix}{\matPi}
\newcommand{\projectormatrixtilde}{\matPitilde}
\newcommand{\geneigmatrix}{\Lambda}
\newcommand{\fmmalgo}{FMM} \usepackage[utf8]{inputenc}
\usepackage{listings}
\usepackage{fullpage}
\usepackage{mathdots} 
\usepackage{enumerate}

\title{Deterministic complexity analysis of Hermitian eigenproblems}

\author{Aleksandros Sobczyk 
\\IBM Research and ETH Zurich
\\Zurich, Switzerland}
\date{}

\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{fullpage} \usepackage{hyperref} 

\RequirePackage[T1]{fontenc}  \RequirePackage[tt=false, type1=true]{libertine} \RequirePackage[varqu]{zi4} \RequirePackage[libertine]{newtxmath} 

\usepackage{setspace}
\usepackage{todonotes}
\hypersetup{
    colorlinks,
    linkcolor={blue},
    citecolor={blue},
    urlcolor={blue}
}


\begin{document}

\maketitle
\begin{abstract}
In this work we revisit the arithmetic and bit complexity of Hermitian eigenproblems. Recently, [BGVKS, FOCS 2020] proved that a (non-Hermitian) matrix $\matA$ can be diagonalized with a randomized algorithm in $O(n^{\omega}\log^2(\tfrac{n}{\epsilon}))$ arithmetic operations, where $\omega\lesssim 2.371$ is the square matrix multiplication exponent, and [Shah, SODA 2025] significantly improved the bit complexity for the Hermitian case. Our main goal is to obtain similar deterministic complexity bounds for various Hermitian eigenproblems. 
In the Real RAM model, we show that a Hermitian matrix can be diagonalized deterministically in $O(n^{\omega}\log(n)+n^2\mathrm{polylog}(\tfrac{n}{\epsilon}))$ arithmetic operations, improving the classic deterministic $\widetilde O(n^3)$ algorithms, and derandomizing the aforementioned state-of-the-art. 
The main technical step is a complete, detailed analysis of a well-known divide-and-conquer tridiagonal eigensolver of Gu and Eisenstat [GE95], when accelerated with the Fast Multipole Method, asserting that it can accurately diagonalize a symmetric tridiagonal matrix in nearly-$O(n^2)$ operations.
In finite precision, we show that an algorithm by Sch\"onhage [Sch72] to reduce a Hermitian matrix to tridiagonal form is stable in the floating point model, using $O(\log(\tfrac{n}{\epsilon}))$ bits of precision. This leads to a deterministic algorithm to compute all the eigenvalues of a Hermitian matrix in $O\left( n^{\omega}\flopcost\left(\log(\tfrac{n}{\epsilon})\right) + n^2\polylog(\tfrac{n}{\epsilon})\right)$ bit operations, where $\flopcost(b)\in\widetilde O(b)$ is the bit complexity of a single floating point operation on $b$ bits. This improves the best known $\widetilde{O}(n^3)$ deterministic and $O\left( n^{\omega}\log^2(\tfrac{n}{\epsilon})\mathcal{F}\left(\log(\tfrac{n}{\epsilon})\right)\right)$ randomized complexities. We conclude with some other useful subroutines such as computing spectral gaps, condition numbers, and spectral projectors, and with some open problems.
\end{abstract}
\newpage
\tableofcontents
\newpage
\section{Introduction}
    \label{section:introduction}
    Eigenproblems arise naturally in many applications. Given a matrix $\matA$, the goal is to compute (a subset of) the eigenvalues $\lambda$ and/or the eigenvectors $\vecv$, which satisfy
    \begin{align*}
        \matA\vecv = \lambda\vecv.
    \end{align*}
    The properties of the given matrix, as well as the quantities that need to be computed can vary depending on the application, giving rise to different variants of the eigenproblem.    These include $(i)$ \textit{eigenvalue problems}, such as the approximation of eigenvalues, singular values, spectral gaps, and condition numbers, $(ii)$ \textit{eigenspace problems}, which refer to the approximation of eigenvectors, spectral projectors, and invariant subspaces, and $(iii)$ \textit{diagonalization problems}, which involve the (approximate) computation of all the eigenvalues and eigenvectors of a matrix (or pencil), i.e., a full spectral factorization and/or the Singular Value Decomposition (SVD).
    
    
    In this work we focus on algorithms for \textit{Hermitian eigenproblems}, i.e., the special case where the input matrix is Hermitian. Our motivation to dedicate the analysis to this special class is twofold. 
    First, they arise in many fundamental applications in Machine Learning, Spectral Graph Theory, and Scientific Computing. 
    For example, the SVD, which is ubiquitous for many applications such as low rank approximations \cite{papadimitriou1998latent,frieze2004fast,drineas2008relative,clarkson2017low}, directly reduces to a Hermitian eigenproblem. 
    Second, the spectral theorem states that a Hermitian matrix $\matA$ can always be written in a factored form $\matA=\matQ\matLambda\matQ^*$, where $\matQ$ is unitary and $\matLambda$ is diagonal with real entries. 
    This alleviates several difficulties of the non-Hermitian case, which can lead to efficient dedicated algorithms. 

    Algorithms for eigenproblems have been studied for decades, some of the earliest being attributed to Jacobi \cite{jacobi1846leichtes,golub2000eigenvalue}. We refer to standard textbooks for an overview of the rich literature  \cite{demmel1997applied,golub2013matrix,parlett1998symmetric,bhatia2007perturbation,saad2011numerical}. 
    Some landmark works include the power method \cite{mises1929praktische}, the Lanczos algorithm \cite{lanczos1950iteration}, and the paramount QR algorithm \cite{francis1961qr,francis1962qr,kublanovskaya1962some}, which  has been recognized as one of the ``top ten algorithms of the twentieth century'' \cite{dongarra2000guest}, signifying the importance of the eigenproblem in science and engineering.   
    Given a Hermitian tridiagonal matrix $\matT$ with size $n\times n$, the algorithm can compute a set of approximate eigenvalues in $ O(n^2\log(\tfrac{n}{\epsilon}))$ arithmetic operations, based on the seminal analyses of Wilkinson \cite{wilkinson1968global} and Dekker and Traub \cite{dekker1971shifted}. A set of approximate eigenvectors can be also returned in $ O(n^3\log(\frac{n}{\epsilon}))$ operations. In conjunction with the classic unitary similarity transformation algorithm of Householder \cite{householder1958unitary}, the shifted-QR algorithm has heavylifted the computational burden of solving eigenvalue problems for decades, both in theory and in practice. 
    A detailed bit complexity analysis was provided recently in \cite{banks2021global1,banks2022global2,banks2022global3}.
    
    Despite the daunting literature, several details regarding the computational complexity of many algorithms remain unclear. 
    It is well-known, for example, that the cubic arithmetic complexity to compute the eigenvalues of a dense matrix is not optimal: a classic work by Pan and Chen \cite{pan1999complexity} showed that the eigenvalues can be approximated in $O(n^\omega)$ arithmetic operations, albeit without detailing how to also compute eigenvectors, or to fully diagonalize a matrix. 
    Here $\omega\geq 2$ is the \textit{matrix multiplication exponent}, i.e. the smallest number such that two $n\times n$ matrices can be multiplied in $O(n^{\omega+\eta})$ arithmetic operations, for any $\eta>0$. The current  best known upper bound is $\omega< 2.371339$  \cite{alman2024more}, and we will write $n^{\omega}$ instead of $n^{\omega+\eta}$ hereafter for simplicity. 
    Recently, Banks, Garza-Vargas, Kulkarni, and Srivastava \cite{banks2022pseudospectral} described a numerically stable randomized algorithm to compute a provably accurate diagonalization, in $\widetilde O(n^{\omega})$ operations, improving the previous  best-known bounds, specifically, $\widetilde O(n^3)$ (Hermitian) and $O(n^{10})$ (non-Hermitian) \cite{armentano2018stable}. \cite{shah2025hermitian} further improved the analysis for the Hermitian case, and several works have studied extensions and related applications \cite{demmel2024inverse,demmel2024generalized,schneider2024pseudospectral,sobczyk2024invariant}. 
    To date, we are not aware of any \emph{deterministic} algorithm  that achieves the same arithmetic (or bit) complexity with provable approximation guarantees, even for the Hermitian case.
    In this work, we proceed step-by-step and analyze several variants of the aforementioned eigenvalue, eigenspace, and diagonalization problems, in different \textit{models of computation}, and report complexity upper bounds with provable approximation guarantees.
    
    \subsection{Models of computation and complexity}
    From the Abel-Ruffini theorem it is known that the eigenvalues and/or the eigenvectors of matrices with size larger than $n=5$ can not be computed exactly. Even in exact arithmetic, they can only be approximated. Before analyzing algorithms, we first need to clarify what are the quantities of interest, to define how accuracy is measured, and what is the underlying model of computation. The main two models that we use to analyze algorithms are the Real RAM and the Floating Point model, described below. 

    \begin{description}
    \item[Exact real arithmetic (Real RAM):]
        For the Real RAM model we follow the  definition of \cite{erickson2022smoothing}. The machine has a memory (RAM) and registers that store real numbers in infinite precision. Moving real numbers between the memory and the registers takes constant time. A processor can execute arithmetic operations $\{+,-,\times,/,\sqrt\cdot,>\}$ on the real numbers stored in the registers exactly, without any errors, in constant time. Other functions such as $\log(x),\exp(x),$ and trigonometric functions, are not explicitly available, but they can be  efficiently approximated up to some additive error, e.g. with a truncated polynomial expansion, using a polylogarithmic number of basic arithmetic operations.
        Bit-wise operations on real numbers are forbidden, since this can give the machine unreasonable computing power \cite{hartmanis1974power,schonhage1979power,erickson2022smoothing}. 

    \item[Floating point:]
    In this model, a real number $\alpha\in\mathbb{R}$ is rounded to a floating point number $
        \fl(\alpha) = s\times 2^{e-t} \times m,$ where $s=\pm 1$, $e$ is the exponent, $t$ is the bias, and $m$ is the significand.
    Floating point arithmetic operations also introduce rounding errors, i.e., for two floating point numbers $\alpha$ and $\beta$, each  operation $\odot \in\{+,-,\times,/\}$ satisfies:
    \begin{align}
        \fl(\alpha\odot\beta) = (1+\theta)(\alpha\odot\beta),
        \qquad
        \text{and also }
        \qquad
        \fl(\sqrt{a})=(1+\theta)\sqrt{a},
        \label{eq:elementwise_flop_errors}
    \end{align} 
    where $|\theta|\leq\umach$, and $\umach$ is the machine precision. Assuming a total of $b$ bits for each number, every floating point operation costs $\flopcost(b)$ bit operations, where typically $\flopcost(b)\in O(b^2)$, or even $\widetilde O(b)$, with more advanced algorithms \cite{schonhage1971fast,furer2007faster,harvey2021integer}.
    More details can be found in Appendix \ref{appendix:floating_point_arithmetic}.
    \end{description}

    In Section \ref{section:hermitian_eigenvalues} we will also use as a subroutine an algorithm from \cite{bini1991parallel,bini1998computing}, which was originally analyzed in Boolean RAM model. We describe in detail how to use it in the corresponding section.
    

    \paragraph*{Arithmetic and boolean complexity} Given a model of computation, the \textit{arithmetic complexity} of an algorithm is quantified in terms of the arithmetic operations executed. The \textit{boolean} (or \textit{bit}) \textit{complexity}, accounts for the total number of boolean operations (i.e. boolean gates with maximum fan-in two).

\subsection{Notation}
Matrices and vectors are denoted with bold capital and small letters, respectively. 
$\lVert \matA\rVert$ is the spectral norm of $\matA$, $\lVert \matA\rVert_F$ its Frobenius norm, $\kappa(\matA)=\|\matA\|\|\matA^{\dagger}\|$ is the condition number, and $\Lambda(\matA)$ its spectrum. For the complexity analysis we denote the geometric series $S_{x}(m)=\sum_{l=1}^m (2^{x-2})^l$. As already mentioned, for simplicity, the complexity of multiplying two $n\times n$ matrices will be denoted as $O(n^\omega)$, instead of $O(n^{\omega+\eta})$, for arbitrarily small $\eta>0$. 
We also use the standard notation $\omega(a,b,c)$ for the complexity of multiplying two rectangular matrices with sizes $n^{a}\times n^b$ and $n^b\times n^c$ in time $O(n^{\omega(a,b,c)})$, and therefore $\omega:=\omega(1,1,1)$. For example, for $a=c=1$ and $b=2$, the best known bound for $\omega(1,2,1)\approx 3.250035$ \cite{alman2024more}, which is slightly better than naively performing $n$ square multiplications in  $O(n\cdot n^{\omega})\lesssim O(n^{3.371339})$. $\flopcost(b)$ denotes the bit complexity of a single floating point operation on $b$ bits. 

\subsection{Methods and Contributions}
    
    \subsubsection{Real RAM}
    \label{paragraph:intro_real_ram} We start with the analysis in exact arithmetic, which is the simplest model to analyze numerical algorithms. The goal is to obtain end-to-end upper bounds for the arithmetic complexity of approximately diagonalizing symmetric tridiagonal matrices and, ultimately, dense Hermitian matrices, as well as to approximate the SVD. To measure the accuracy, we follow the notion of \textit{backward-stability} (or \textit{backward-approximation}) for Hermitian diagonalization from 
    \cite{nakatsukasa2013stable}.
    Formally, the following problems are considered.
    \begin{problem} Backward-approximate diagonalization problems in exact arithmetic.
        \label{problem:problems_in_exact_arithmetic}
        \begin{enumerate}[(i)]
            \item \textbf{Symmetric arrowhead/tridiagonal diagonalization:} Given a symmetric arrowhead or tridiagonal matrix $\matA$ with size $n\times n$, $\|\matA\|\leq 1$, and accuracy $\epsilon\in(0,1)$, compute a diagonal matrix $\matLambdatilde$ and a matrix $\matUtilde$, such that $\matUtilde=\matU+\matE_{\matU}$, where $\matU$ is orthogonal and $\|\matE_{\matU}\|\leq \epsilon$, and $\lnorm \matA-\matU\matLambdatilde\matU^\top \rnorm \leq \epsilon$.
            \item \textbf{Hermitian diagonalization:} Given a Hermitian matrix $\matA$ with size $n\times n$, $\|\matA\|\leq 1$, and accuracy $\epsilon\in(0,1)$, compute a diagonal matrix $\matLambdatilde$ and a matrix $\matUtilde$, such that $\matUtilde=\matU+\matE_{\matU}$, where $\matU$ is unitary and $\|\matE_{\matU}\|\leq \epsilon$, and $\lnorm \matA-\matU\matLambdatilde\matU^* \rnorm \leq \epsilon$.
            \item \textbf{SVD:} Given a matrix $\matA\in\mathbb{C}^{m\times n}$, $m=n^k$, $k\geq 1$, $\|\matA\|\leq 1$, and accuracy $\epsilon\in(0,1)$ compute a diagonal matrix $\matSigmatilde$, an $m\times n$ matrix $\matUtilde=\matU+\matE_{\matU}$, and an $n\times n$ matrix $\matVtilde=\matV+\matE_{\matV},$ such that $\matU^*\matU=\matV^*\matV=\matI$, $\lnorm \matE_{\{\matU,\matV\}}\rnorm \leq \epsilon$, and $\lnorm \matA-\matU\matSigmatilde\matV^* \rnorm \leq \epsilon$.
        \end{enumerate}
    \end{problem}
 
    To obtain rigorous complexity guarantees for these problems, with respect to all the involved parameters, we start from Problem \ref{problem:problems_in_exact_arithmetic}-(i), namely, diagonalization of symmetric tridiagonal matrices. To that end, we revisit the so-called fast tridiagonal eigensolvers, which aim to reduce the complexity from cubic to $\widetilde O(n^2)$ operations. 
    Many such algorithms have been studied in the literature \cite{dongarra1987fully,dhillon1997new,bini1992practical,bini1991parallel,gill1990n,vogel2016superfast,ou2022superdc,stor2015accurate,barlow1993error}, most of which are based on the divide-and-conquer (DC) strategy of Cuppen \cite{cuppen1980divide}. 
    The algorithms in all of the aforementioned works have been rigorously analyzed, however, explicit complexity bounds in terms of strictly solving Problem \ref{problem:problems_in_exact_arithmetic}-(i) are not detailed. 
    We resolve this by providing an end-to-end complexity analysis of the algorithm of Gu and Eisenstat \cite{gu1995divide}. In their original work, the authors outlined how to accelerate several parts of the algorithm with the Fast Multipole Method (FMM) \cite{rokhlin1985rapid}, which could eventually lead to a final complexity of $\widetilde O(n^2)$. However, the actual analysis of this approach and the FMM details were not provided. 
    \cite{musco2018stability} further extended the analysis in floating point, but it also relies on a numerically stable FMM implementation, which is not detailed. In this work, we use the elegant FMM analysis of \cite{gu1993stable,livne2002n,cai2020stable}, which is particularly suited for the problems considered.
    It is detailed in the following Proposition \ref{proposition:fmm}. \begin{proposition}[FMM]
    \label{proposition:fmm}
    There exists an algorithm, which we refer to as $(\epsilon,n)$-approximate FMM (or $(\epsilon,n)$-\fmmalgo, for short), which takes as input:
    \begin{itemize}
        \item a kernel function $k(x)\in\lcurly \log(|x|), \frac{1}{x}, \frac{1}{x^2} \rcurly$,
        \item $2n+m$ real numbers: $\{x_1,\ldots,x_m\}\cup \{c_1,\ldots,c_n\}\cup\{y_1,\ldots,y_n\}$, and a constant $C$, such that $m\leq n$ and for all $i\in[m],j\in[n]$ it holds that
        \begin{align*}
            |x_i|,|c_j|,|y_j|<C
            \qquad
            \text{ and }
            \qquad
            |x_i-y_j|\geq \Omega(\poly(\tfrac{\epsilon}{n})).
        \end{align*}
    \end{itemize}
    It returns $m$ values $\widetilde f(x_1),\ldots,\widetilde f(x_m)$ such that
    $
        \labs
            \widetilde f(x_i)-f(x_i)
        \rabs
        \leq \epsilon,
    $
    for all $i\in[m]$, where $f(x) = \sum_{j=1}^n c_j k(x_i-y_j)$,
    in a total of $O\lpar 
        n\log^{\cfmm}(\tfrac{n}{\epsilon})
    \rpar$ arithmetic operations, where $\cfmm\geq 1$ is a small constant that is independent of $\epsilon,n$.
    \begin{proof}\textcolor{red}{TOPROVE 0}\end{proof}
\end{proposition}
By taking advantage of the FMM, the analysis from Section \ref{section:tridiagonal_diagonalization} and the supporting Appendices \ref{section:arrowhead_preliminaries} and \ref{appendix:fast_multipole_method} leads to the following Theorem \ref{theorem:alg_recursive_diagonalization}, whose proof can be found in Section \ref{section:proof_of_tridiagonal_diagonalization}.    
\begin{theorem}
    \label{theorem:alg_recursive_diagonalization}
    Given an unreduced symmetric tridiagonal matrix $\matT$ with size $n\times n$, $\|\matT\|\leq 1,$ an accuracy $\epsilon\in(0,1/2)$, and an $(\epsilon,n)$-\fmmalgo\   implementation as in Proposition \ref{proposition:fmm},  the recursive algorithm of \cite{gu1995divide} (Algorithm \ref{algorithm:recursive_diagonalization}), returns an approximately orthogonal matrix $\matUtilde$ and a diagonal matrix $\matLambdatilde$ such that
    \begin{align*}
        \lnorm \matT - \matUtilde\matLambdatilde\matUtilde^\top \rnorm \leq \epsilon,
        \quad
        \lnorm \matUtilde^\top \matUtilde - \matI \rnorm \leq \epsilon/n^2,
    \end{align*}
    or, stated alternatively,
    \begin{align*}
        \matUtilde=\matU+\matE_{\matU},
        \quad
        \matU^\top\matU=\matI,
        \quad
        \lnorm \matE_{\matU} \rnorm \leq \epsilon/n^2,
        \quad
        \lnorm \matT - \matU\matLambdatilde\matU^\top \rnorm \leq \epsilon,
        \quad
    \end{align*}
    using a total of $O\lpar n^2\log^{\cfmm+1}(\tfrac{n}{\epsilon})\rpar$ arithmetic operations and comparisons, where $\xi\geq 1$ is a small constant that depends on the specific FMM implementation and it is independent of $\epsilon,n$.
   
\end{theorem}
\begin{table}[htb]
    \centering
    \caption{Comparison of algorithms for the Problems \ref{problem:problems_in_exact_arithmetic} (i)-(iii) in the Real RAM model. Here $\xi\geq 1$ is a small constant which depends on the FMM implementation (see Prop. \ref{proposition:fmm}). The algorithms marked with (\textbf{R}) are randomized and succeed with high probability (at least $1-1/\poly(n)$).}
    \label{table:main_results_exact_arithmetic_diagonalization}
    {
    \begin{tabular}{lll}
    \hline
      & Arithmetic Complexity  & Comment  \\\hline\hline
    
        Prob. \ref{problem:problems_in_exact_arithmetic}-(i)
        &  
        &
    \\
         
        Arrowhead/Trid.
        &  
        &
    \\   
        diagonalization
        &  
        &
    \\   Refs. \cite{cuppen1980divide,o1990computing,gu1995divide}& 
        $O(n^3)+\widetilde O(n^2)$ 
        &
        Conjectured $ \widetilde O(n^2)$ 
    \\    
        Theorem \ref{theorem:alg_recursive_diagonalization}
        & 
        $O\lpar n^2\log^{\cfmm+1}(\tfrac{n}{\epsilon})\rpar$ 
        &
        Req. FMM (Prop. \ref{proposition:fmm})
    \\\hline
        Prob. \ref{problem:problems_in_exact_arithmetic}-(ii)
        &  
        &
    \\      
        Hermitian
        &  
        &
    \\      
        diagonalization
        &  
        &
    \\ 
        Refs. \cite{banks2022pseudospectral,shah2025hermitian} (\textbf{R}) 
        & 
        $O\lpar n^{\omega}\log^2(\tfrac{n}{\epsilon})\rpar$ 
        &
        -
    \\
        Ref. \cite{benor2018quasi} (\textbf{R}) & 
        $\widetilde O\lpar n^{\omega+1}\rpar$ 
        &
        Conjectured  $\widetilde O(n^{\omega})$
    \\
        Refs. \cite{dekker1971shifted,wilkinson1968global,hoffmann1978new} & 
        $O\lpar n^{3}\log(\tfrac{n}{\epsilon})\rpar$ 
        &
        Shifted-QR
    \\
        Ref. \cite{nakatsukasa2013stable} & 
        $\widetilde O\lpar n^{3}\rpar$ 
        &
        Req. separated spectrum
    \\
        Ref. \cite{pan1999complexity} 
        & 
        $O\lpar n^{\omega} + n\polylog(\tfrac{n}{\epsilon})\rpar$ 
        &
        Only eigenvalues
    \\    
        Corollary \ref{corollary:hermitian_diagonalization}
        & 
        $O\lpar n^\omega\log(n) + n^2\log^{\cfmm+1}(\tfrac{n}{\epsilon})\rpar$ 
        &
        Req. FMM (Prop. \ref{proposition:fmm})
    \\\hline
        Prob. \ref{problem:problems_in_exact_arithmetic}-(iii), SVD
        &  
        &
    \\  Shifted-QR on $\matA^*\matA$ & 
        $O\lpar n^{\omega(1,k,1)} + n^3\log(\tfrac{n}{\epsilon}) \rpar$ 
        &
        Partial error analysis 
    \\
        Refs. \cite{banks2022pseudospectral,shah2025hermitian,kacham2024faster} (\textbf{R}) 
        & 
        $O\lpar 
            n^{\omega(1,k,1)} 
            +
            n^{\omega}\log^2(\tfrac{n}{\epsilon})
        \rpar$ 
        &
        Partial error analysis
    \\    
        Theorem \ref{theorem:svd}
        & 
        $O\lpar n^{\omega(1,k,1)} + n^\omega\log(n) 
        +
        n^2\polylog(\tfrac{n\kappa(\matA)}{\epsilon})\rpar$ 
        &
        Req. FMM (Prop. \ref{proposition:fmm})
        \\\hline
    \end{tabular}
    }
\end{table}    

This result has a direct application to the dense Hermitian diagonalization problem. 
The best-known complexities of deterministic algorithms, with end-to-end analysis, are at least cubic. 
For example, the aforementioned shifted-QR algorithm requires $O(n^3\log(n/\epsilon))$ arithmetic operations, even for tridiagonal matrices. 
The cubic barrier originates from the accumulation of the elementary rotation matrices to form the final eigenvector matrix. 
The so-called \emph{spectral divide-and-conquer} methods (see e.g. \cite{demmel1997applied,nakatsukasa2013stable}) have also at least cubic complexities in the deterministic case. 
The two main difficulties in their analysis are the basis computation of spectral projectors, for which the best-known deterministic complexity bounds are $\Omega(n^3)$, e.g. via Strong Rank-Revealing QR (RRQR) factorization \cite{gu1996efficient}, and the choice of suitable splitting points, which relies on the existence of spectral gaps. 

Randomness can help to overcome certain difficulties in the analysis. \cite{demmel2007fastla} analyzed a numerically stable Randomized URV decomposition, which can be used to replace RRQR for basis computations in spectral DC algorithms. A highly parallelizable Hermitian diagonalization algorithm with end-to-end analysis was proposed in \cite{benor2018quasi}. While the reported sequential arithmetic complexity is $O(n^{\omega+1})$, the authors conjectured that it can be further reduced to $\widetilde O(n^{\omega})$.  The first end-to-end $\widetilde O(n^\omega)$ complexity upper bound was achieved in
\cite{banks2022pseudospectral}. One of the main techniques was to use random perturbations to ensure that the pseudospectrum is well-separated, which helps to find splitting points in spectral DC. \cite{shah2025hermitian} further improved the analysis for Hermitian matrices.  


Theorem \ref{theorem:alg_recursive_diagonalization} can be directly used to obtain a simple and deterministic solution for the  Hermitian diagonalization problem. 
Specifically, Corollary \ref{corollary:hermitian_diagonalization} states that the problem can be solved in $O(n^{\omega}\log(n) + n^2\polylog(n/\epsilon))$ arithmetic operations, which is both faster and fully deterministic. 
This is achieved by combining Theorem \ref{theorem:alg_recursive_diagonalization} with the (rather overlooked) algorithm of Schönhage \cite{schonhage1972unitare}, who proved that a Hermitian matrix can be reduced to tridiagonal form with unitary similarity transformations in $O(n^\omega\log(n)c(\omega))$ arithmetic operations, where $c(\omega)=O(1)$ if $\omega$ is treated as a fixed constant larger than $2$, while $c(\omega)=O(\log(n))$ if it turns out that $\omega=2$. 

As a consequence, Theorem \ref{theorem:svd} reports similar deterministic complexity results for the SVD. The SVD is a fundamental computational kernel in many applications, such as Principal Component Analysis \cite{jolliffe2002principal}, and it is also widely used as a subroutine for many other advanced algorithms, e.g. \cite{papadimitriou1998latent,frieze2004fast,drineas2008relative,boutsidis2014optimal,boutsidis2014randomized,cohen2015dimensionality,boutsidis2016optimal,clarkson2017low}, to name a few. A straightforward algorithm to compute it is to first form the Gramian matrix $\matA^*\matA$, and then diagonalize it. Other classic SVD algorithms, such as the widely adopted Golub-Kahan bidiagonalization and its variants \cite{golub1965calculating}, or polar decomposition-based methods \cite{nakatsukasa2013stable,nakatsukasa2010optimizing}, avoid the computation of $\matA^*\matA$ for numerical stability reasons, but they also rely on a diagonalization algorithm as a subroutine.  
\cite{kacham2024faster} elaborated on a matrix multiplication time SVD algorithm, by using \cite{banks2022pseudospectral} to diagonalize $\matA^\top \matA$, albeit without fully completing the backward stability analysis. 
The following Theorem \ref{theorem:svd}, which is proved in Appendix \ref{appendix:svd_analysis}, states our main result. 

\begin{theorem}[SVD]
    \label{theorem:svd}
    Let $\matA\in\mathbb{C}^{m\times n}$, $m=n^k$, $k\geq 1$. Assume that $1/n^c\leq \|\matA\|\leq 1$, for some constant $c$. Given accuracy $\epsilon\in(0,1/2)$ and an $(\epsilon,n)$-\fmmalgo\   (see Prop. \ref{proposition:fmm}), we can compute three matrices $\matUtilde\in\mathbb{C}^{m\times n},\matSigmatilde\in\mathbb{R}^{n\times n},\matVtilde\in\mathbb{C}^{n\times n}$ such that $\matSigmatilde$ is diagonal with positive diagonal elements and
    \begin{align*}
        \matA=\matUtilde\matSigmatilde\matVtilde^*,
        \quad
        \lnorm \matUtilde^* \matUtilde - \matI \rnorm \leq \epsilon,
        \quad
        \lnorm \matVtilde^* \matVtilde - \matI \rnorm \leq \epsilon/(\kappa(\matA)n^2)\ll \epsilon,
    \end{align*}
    or, stated alternatively,
    \begin{align*}
        \lnorm \matA-\matU\matSigmatilde\matV^*\rnorm \leq \epsilon,
        \quad
        \matUtilde&=\matU+\matE_{\matU},
        \quad
        \lnorm \matE_{\matU }\rnorm \leq \epsilon,
        \quad
        \matU^*\matU=\matI,
        \\
        \matVtilde&=\matV+\matE_{\matV},
        \quad
        \lnorm \matE_{\matV}\rnorm \leq \epsilon/n^2,
        \quad
        \matV^*\matV=\matI.
    \end{align*}
    The algorithm requires a total of at most $O\lpar n^{\omega(1,k,1)} + n^\omega\log(n)+ n^2\polylog(\tfrac{n\kappa(\matA)}{\epsilon})\rpar$ arithmetic operations, where $\kappa(\matA)=\|\matA\|\|\matA^\dagger\|$.
\end{theorem}


To summarize this section, in Table \ref{table:problems_in_finite_precision} we list all the deterministic arithmetic complexities achieved in the Real RAM model, for all the aforementioned problems, and compare with some important existing algorithms.


\subsubsection{Finite precision}
\label{paragraph:intro_finite_precision} Similar deterministic complexity upper bounds are obtained for several problems in finite precision. In particular, we study the following problems, for which we seek to bound the boolean complexity, i.e., the total number of bit operations.
\begin{problem}
    \label{problem:problems_in_finite_precision}
    Main problems in finite precision.
    \begin{enumerate}[(i)]
        \item \textbf{Tridiagonal reduction:} Given a Hermitian matrix $\matA$ with floating point elements, reduce $\matA$ to tridiagonal form using (approximate) unitary similarity transformations. In particular, return a tridiagonal matrix $\matTtilde$, and (optionally)  an approximately unitary matrix $\matQtilde$, such that \begin{align*}
            \lnorm \matQtilde\matQtilde^*-\matI\rnorm \leq \epsilon,
            \quad
            \text{and}
            \quad
            \lnorm \matA - \matQtilde\matTtilde\matQtilde^* \rnorm \leq \epsilon \lnorm \matA \rnorm,
            \end{align*} 
        \item \textbf{Hermitian eigenvalues:} Given a Hermitian matrix $\matA$, $\|\matA\|\leq 1$, and accuracy $\epsilon\in(0,1)$, compute a set of approximate eigenvalues $\widetilde\lambda_i$ such that $\labs \widetilde\lambda_i-\lambda_i(\matA)\rabs \leq \epsilon$.
    \end{enumerate}
\end{problem}
\begin{table}[htb]
    \caption{Boolean complexity for Problems \ref{problem:problems_in_finite_precision}, for matrix size $n\times n$ and accuracy $\epsilon\in(0,1)$. }
    \label{table:problems_in_finite_precision}
    \centering
    \noindent
        \begin{tabular}{lll}
        \hline
          & Boolean Complexity 
          & Comment
        \\\hline\hline
        Prob. \ref{problem:problems_in_finite_precision}-(i)        
        &
        &
    \\ 
    Tridiag. Reduction
        &
        &
    \\
        Refs. \cite{householder1958unitary,higham2002accuracy}
        & 
        $O\lpar 
            n^3  
            \flopcost\lpar 
                \log\left(\tfrac{n}{\epsilon}\right) 
            \rpar\rpar$ 
        &
        Standard Householder reduction
    \\    
        Theorem  \ref{theorem:stable_tridiagonal_reduction} 
        & 
        $O\lpar 
            n^{\omega}\log(n)
            \flopcost 
            \lpar 
                \log(\tfrac{n}{\epsilon}) 
            \rpar 
        \rpar$
        &
        \cite{schonhage1972unitare} with stable fast QR \cite{demmel2007fastla}
    \\\hline
            Prob. \ref{problem:problems_in_finite_precision}-(ii) 
            &
            &
        \\
            Herm. Eigenvalues
            &
            &
        \\
            Ref. \cite{shah2025hermitian}& 
            $O\lpar n^{\omega}\log^2(\tfrac{n}{\epsilon})
            \flopcost\Big( 
                \log(\tfrac{n}{\epsilon})
            \Big)
            \rpar$
            &
            Randomized, $\Pr[\text{success}]\geq 1-\frac{1}{n}$
        \\    
            Theorem  \ref{theorem:hermitian_eigenvalues}
            & 
            $O\lpar
                n^{\omega}\flopcost\Big( \log(\tfrac{n}{\epsilon}) \Big)
                +
                n^2\polylog(\tfrac{n}{\epsilon})
            \rpar$ 
            &
            Deterministic, Thm. \ref{theorem:stable_tridiagonal_reduction} + \cite{bini1998computing}
        \\
        \hline
        \end{tabular}
    \label{table:fp_results}
\end{table}
Regarding deterministic algorithms, with end-to-end-analysis, the standard approach is to first reduce the Hermitian matrix to tridiagonal form with Householder transformations \cite{householder1958unitary}, which can be done stably in $O(n^3)$ arithmetic operations using $O(\log(n/\epsilon))$ bits of precision; see e.g. \cite{higham2002accuracy}. 
Thereafter, there is a plethora of algorithms (e.g. the ones mentioned in the previous section) for the eigenvalues of the tridiagonal matrix, with varying complexities and stability properties. However, the total boolean complexity cannot be lower than $\Omega(n^3\flopcost(\log(\frac{n}{\epsilon})))$ due to the Householder reduction step.
Other well-known deterministic and numerically stable algorithms in the literature also require at least $n^3$ arithmetic operations to compute all the eigenvalues \cite{paige1980accuracy,demmel1992jacobi,nakatsukasa2013stable}, and at least $\polylog(n,1/\epsilon)$ bits of precision in a floating point machine. The arithmetic complexity of the algorithm of \cite{pan1999complexity} scales as $O(n^\omega)$ with respect to the matrix size $n$, but the boolean complexity can increase up to $O(n^{\omega+1})$ in rational arithmetic. \cite{louis2016accelerated} described a randomized algorithm to compute only the largest eigenvalue in nearly $O(n^\omega)$ bit complexity. The fastest algorithm to compute all the eigenvalues of a Hermitian matrix is \cite{shah2025hermitian}, which requires $O(n^\omega\polylog(n/\epsilon))$ boolean operations and succeeds with high probability. 


Randomized eigenvalue algorithms have also been studied in the sketching/streaming setting \cite{andoni2013eigenvalues,needell2022testing,swartworth2023optimal}.
The (optimal) algorithm of \cite{swartworth2023optimal} has not been analyzed in finite-precision, but, due to its simplicity, it should be straightforward to achieve. 
The  algorithm approximates all the eigenvalues of a Hermitian matrix $\matA$ up to additive error $\epsilon\|\matA\|_F$. However, to reduce the error to a spectral-norm bound $\epsilon\|\matA\|$, the algorithm internally needs to diagonalize a matrix with size $\Omega(n)$, and therefore it does not provide any improvement against any other Hermitian eigenvalue solver.
Nevertheless, our main results can be directly applied as the main eigenvalue subroutine of this algorithm, and to help analyze its bit complexity.

To improve the aforementioned Hermitian eigenvalue algorithms,
we first prove in Theorem \ref{theorem:stable_tridiagonal_reduction} it is proved that Schönhage's algorithm is numerically stable in the floating point model of computation. Thereafter, we carefully combine it with the algorithms of \cite{bini1991parallel,bini1992practical,bini1998computing} which provably and deterministically approximate all the eigenvalues of a symmetric tridiagonal matrix in $\widetilde O(n^2)$ boolean operations. The latter algorithm is analyzed in the Boolean RAM model, therefore, in order to use it we need to convert the floating point elements of the tridiagonal matrix that is returned by Theorem \ref{theorem:stable_tridiagonal_reduction} to integers, which can be done efficiently under reasonable assumptions on the initial number of bits used to represent the floating point numbers. 
This is described in detail in the proof of our main Theorem \ref{theorem:hermitian_eigenvalues}. Our result derandomizes and slightly improves the final bit complexity of the algorithm of \cite{shah2025hermitian}, which has the currently best known bit complexity for this problem. 
Table 
\ref{table:fp_results} summarizes this discussion.

As a direct consequence of Theorem \ref{theorem:hermitian_eigenvalues}, we also provide the analysis for several other useful subroutines related to eigenvalue/eigenvector computations, including:
\begin{enumerate}[(i)]
    \item \textbf{Singular values and condition number}: In Proposition \ref{proposition:alg_sigmak} we describe how to obtain relative error approximations of singular values. In Corollary \ref{corollary:alg_cond} we show how to compute the condition number.
    \item\textbf{Definite pencil eigenvalues}: In Corollary \ref{corollary:hermitian_definite_pencil_eigenvalues} we demonstrate how to extend Theorem \ref{theorem:hermitian_eigenvalues} to compute the eigenvalues of Hermitian-definite pencils. 
    \item\textbf{Spectral gaps}: In Corollary \ref{corollary:alg_deterministic_spectral_gap} we show how to compute the spectral gap and the midpoint between any two eigenvalues of a Hermitian-definite pencil. Our algorithm is deterministic and it requires significantly less bits of precision than the algorithm of \cite{sobczyk2024invariant}, who described a randomized algorithm for this problem that is slightly faster than applying \cite{banks2022pseudospectral} as a black-box, but it only computes a single spectral gap.
    \item\textbf{Spectral projector}: Corollary \ref{corollary:spectral_projector} details how to compute spectral projectors on invariant subspaces of Hermitian-definite pencils, which are important for many applications.
\end{enumerate}
\subsection{Outline}
The paper is organized as follows. In Section \ref{section:tridiagonal_diagonalization} we analyze the algorithm of \cite{gu1995divide} when implemented with the FMM and its applications (see also  Appendices \ref{section:arrowhead_preliminaries}, \ref{appendix:fast_multipole_method}, and \ref{appendix:tridiagonal_diagonalization}). In Section \ref{section:tridiagonal_reduction_stability} it is proved that Sch\"onhage's algorithm is numerically stable in floating point, and it is used as a preprocessing step to compute the eigenvalues of Hermitian matrices. For the proof we use the technical lemmas that are proved in the supporting Appendix \ref{appendix:tridiagonal_reduction}. In Section \ref{section:applications_tridiagonal_reduction} we mention some direct applications to compute singular values, pencil eigenvalues, spectral gaps, and spectral projectors. We finally conclude and state some open problems in Section \ref{section:conclusion}.

\subsection*{Acknowledgements}{
I am grateful to Efstratios Gallopoulos, Daniel Kressner, and David Woodruff for helpful discussions.}


\section{Diagonalization of symmetric tridiagonal and arrowhead matrices in Real RAM}
\label{section:tridiagonal_diagonalization}
Our main target is to compute the eigenvalues and the eigenvectors of tridiagonal symmetric matrices in nearly linear time. To derive the desired result, we provide an analysis the divide-and-conquer algorithm of Gu and Eisenstat \cite{gu1995divide}, when implemented with the FMM from Proposition \ref{proposition:fmm}.

The algorithm of \cite{gu1995divide} first ``divides'' the problem by partitioning the (unreduced) tridiagonal matrix $\matT$ as follows:
\begin{align*}
    \matT = \begin{pmatrix}
        \matT_1 & \beta_{k+1}\vece_k & \\
        \beta_{k+1}\vece_k^\top & \alpha_{k+1} & \beta_{k+2}\vece_1^\top \\
         & \beta_{k+2}\vece_1 & \matT_2
    \end{pmatrix}.
\end{align*}
If one has access to the spectral decomposition of $\matT_1$ and $\matT_2$, i.e. $\matT_1=\matQ_1\matD_1\matQ_1^\top$ and $\matT_2=\matQ_2\matD_2\matQ_2^\top$, then $\matT$ can be factorized as
\begin{align}
    \label{eq:tridiagonal_to_arrowhead}
    \begin{pmatrix}
        & \matQ_1 & \\
        1 & & \\
        & & \matQ_2
    \end{pmatrix}    
    \begin{pmatrix}
        \alpha_{k+1}& \beta_{k+1}\vecl_1^\top   & \beta_{k+2}\vecf_2^\top\\
        \beta_{k+1}\vecl_1 & \matD_1 & \\
         \beta_{k+2}\vecf_2& & \matD_2
    \end{pmatrix}
    \begin{pmatrix}
        & 1 & \\
        \matQ_1^\top & & \\
        & & \matQ_2^\top
    \end{pmatrix}
    =
    \matQ\matH\matQ^\top,
\end{align}
where $\vecl_1^\top$ is the last row of $\matQ_1$ and $\vecf_2^\top$ is the first row of $\matQ_2$, and $\matH$ has the so-called \textit{arrowhead} structure. Thus, given this form, to compute the spectral decomposition of $\matT$, it suffices to diagonalize $\matH$. One can then apply recursively the algorithm to compute the spectral decompositions of $\matT_1$ and $\matT_2$, and, finally, at the ``conquering stage,'' combine the solutions with the eigendecomposition of $\matH$.

For the individual steps to be computed efficiently, we will need to use the FMM. Specifically, we will need to use it to evaluate the functions that are listed in Equations \eqref{eq:fmm_1}-\eqref{eq:fmm_5}, where the evaluation points will also satisfy certain criteria that are detailed in Lemma \ref{lemma:arrowhead_preliminaries}. 
We ensure that these criteria are met by using a deflation pre-processing step (see also Appendix \ref{appendix:arrowhead_deflation}), which allows us to take advantage of the FMM, specifically, Proposition \ref{proposition:fmm}. 

\subsection{Symmetric arrowhead diagonalization}
The first step is to provide an end-to-end complexity analysis for the arrowhead diagonalization algorithm of \cite{gu1995divide}, when implemented with the FMM. We start with an $n\times n$ arrowhead matrix of the form
\begin{align}
    \label{eq:arrowhead}
    \matH=\begin{pmatrix}
        \alpha & \vecz^\top \\
        \vecz & \matD
    \end{pmatrix},
\end{align}
where $\matD$ is a diagonal matrix, $\vecz$ is a vector of size $n-1$ and $\alpha $ is a scalar. Without loss of generality we assume that $\|\matH\|\leq 1$. The main result is stated in Theorem \ref{theorem:arrowhead_diagonalization}. 

In order to prove Theorem \ref{theorem:arrowhead_diagonalization}, we expand in detail the following methodology which is outlined in \cite{gu1995divide}, by proving several technical lemmas  in Appendices \ref{section:arrowhead_preliminaries} and \ref{appendix:fast_multipole_method} that leverage the FMM.
\begin{enumerate}
    \item Deflation: The matrix $\matH$ is preprocessed to ensure that it satisfies the following:
    $
        d_{i+1}-d_i \geq \tau, \text{ and } |\vecz_i|\geq \tau,
    $
    where $\tau\in(0,1)$. This assumption implies several useful properties, described in Lemma \ref{lemma:arrowhead_preliminaries}, and it allows us to efficiently utilize the FMM in the subsequent steps. The deflation procedure is described in Proposition \ref{proposition:arrowhead_deflation}.
    \item Eigenvalues: The eigenvalues of the deflated matrix can be conveniently approximated as the roots of the corresponding secular equation (Eq. \eqref{eq:arrowhead_secular_equation}):
    $
        f(\lambda)=\lambda-\alpha+\sum_{j=2}^n \tfrac{\vecz_j^2}{d_j-\lambda}.
    $
    An FMM-based root finder is detailed in Lemma \ref{lemma:fmm_approximate_eigenvalues}.
    \item From Lemma \ref{lemma:arrowhead_reconstruction_from_shaft_and_eigenvalues}, the approximate eigenvalues returned by Lemma \ref{lemma:fmm_approximate_eigenvalues} are the exact eigenvalues of another arrowhead matrix $\matHhat=\begin{pmatrix}
        \widehat\alpha & \widehat\vecz^\top \\
        \widehat\vecz & \matD
    \end{pmatrix}$. There is an analytical expression for the elements of $\matHhat$ (see also  \cite{boley1977inverse,gu1995divide}). Lemma \ref{lemma:fmm_approximate_shaft} describes how to compute those elements with the FMM.
    \item Given the exact eigenvalues and the approximate elements of the matrix $\matHhat$, we can focus on its eigenvectors. In particular, in Lemma \ref{lemma:fmm_approximate_inner_products} it is shown how to use the FMM to approximate the inner products between the eigenvectors of $\matHhat$ and some arbitrary unit vector $\vecb.$ 
    Computing such inner products with all the columns of the identity, we obtain the final approximate eigenvector matrix of $\matH$ and, ultimately, an approximate diagonalization of $\matH$.
\end{enumerate}
\begin{remark}
    We note that, if we are only interested in a full diagonalization, Lemma \ref{lemma:fmm_approximate_shaft} is redundant, i.e., we can naively compute the elements exactly without the FMM with the same complexity. However, it is useful if we need to compute only a few matrix-vector products with the eigenvector matrix. Lemma \ref{lemma:fmm_approximate_inner_products}, details how to approximate such matrix-vector products efficiently with the FMM. 
\end{remark}



\begin{theorem}
    \label{theorem:arrowhead_diagonalization}
    Given a symmetric arrowhead matrix $\matH\in\mathbb{R}^{n\times n}$ as in Eq. \eqref{eq:arrowhead}, with $\|\matH\|\leq 1$, an accuracy parameter $\epsilon\in(0,1)$, a matrix $\matB$ with $r$ columns $\matB_i,i\in[r]$, where $\|\matB_i\|\leq 1$, and an $(\epsilon,n)$-\fmmalgo\   implementation (see Prop. \ref{proposition:fmm}), we can compute a diagonal matrix $\matLambdatilde$, and a matrix
    $\matQtilde_{\matB}$, such that 
    \begin{align*}
        \lnorm \matH-\matQ\matLambdatilde\matQ^\top \rnorm &\leq \epsilon,
        \quad
        \labs \lpar \matQ^\top\matB - \matQtilde_{\matB} \rpar_{i,j}\rabs \leq  \epsilon/n^2,
    \end{align*}
    where 
    $
        \matQ\in\mathbb{R}^{n\times n}
    $ is (exactly) orthogonal, in 
    $
        O\lpar nr\log^{\cfmm+1}(\tfrac{n}{\epsilon})\rpar
    $
    arithmetic operations and comparisons, where $\xi\geq 1$ is a small constant that depends on the specific FMM implementation and it is independent of $\epsilon,n$.

    Alternatively, if only want to compute a set of approximate values $\widetilde\lambda_1,\ldots,\widetilde\lambda_n$, such that $|\lambda_i(\matH)-\widetilde\lambda_i|\leq \epsilon$, the complexity reduces to $O\lpar n\log(\frac{1}{\epsilon})\log^{\cfmm}(\frac{n}{\epsilon})\rpar$ arithmetic operations.
    \begin{proof}\textcolor{red}{TOPROVE 1}\end{proof}
\end{theorem}


\subsection{Tridiagonal diagonalization}
\label{section:proof_of_tridiagonal_diagonalization}

Given the analysis for arrowhead diagonalization, we can now proceed to tridiagonal matrices.
The next lemma bounds the error of the reduction to arrowhead form when the spectral factorizations of the matrices $\matT_1$ and $\matT_2$ in Equation \eqref{eq:tridiagonal_to_arrowhead} are approximate rather than exact. This will be used as an inductive step for the final algorithm.
\begin{lemma}
\label{lemma:tridiagonal_assembly}
Let $\epsilon\in(0,1/2)$ be a given accuracy parameter and $\matT = \begin{pmatrix}
    \matT_1 & \beta_{k+1}\vece_k & \\
    \beta_{k+1}\vece_k^\top & \alpha_{k+1} & \beta_{k+2}\vece_1^\top \\
     & \beta_{k+2}\vece_1 & \matT_2
\end{pmatrix}$ be a tridiagonal matrix with size  $n\geq 3$ and $\|\matT\|\leq 1$, where $\matT_1=\matU_1\matD_1\matU_1^\top$ and $\matT_2=\matU_2\matD_2\matU_2^\top$ be the exact spectral factorizations of $\matT_1$ and $\matT_2$. Let $\matUtilde_1,\matDtilde_1,\matUtilde_2,\matDtilde_2$ be approximate spectral factorizations of $\matT_1,\matT_2$. If these factors satisfy
    \begin{align*}
        \lnorm
            \matT_{\{1,2\}} - \matUtilde_{\{1,2\}}\matDtilde_{\{1,2\}}\matUtilde_{\{1,2\}}^\top
        \rnorm 
        \leq \epsilon_1,
        \quad
        \lnorm \matUtilde_{\{1,2\}}\matUtilde_{\{1,2\}}^\top -\matI \rnorm &\leq \epsilon_1/n,
    \end{align*}
    for some $\epsilon_1\in(0,1/2)$, where $\matDtilde_{\{1,2\}}$ are both diagonal, then, assuming an $(\epsilon,n)$-\fmmalgo\   implementation as in Prop. \ref{proposition:fmm}, we can compute a diagonal matrix $\matDtilde$ and an approximately orthogonal matrix $\matUtilde$ such that
    \begin{align*}
        \lnorm\matUtilde^\top\matUtilde-\matI\rnorm\leq 3(\epsilon_1+\epsilon)/n,
        \quad \text{and} \quad
        \lnorm 
            \matT-\matUtilde\matDtilde\matUtilde^\top
        \rnorm \leq 2\epsilon_1+7\epsilon,
    \end{align*}
    in a total of $O\lpar n^2\log^{\cfmm+1}(\tfrac{n}{\epsilon})\rpar$ arithmetic operations and comparisons, where $\xi\geq 1$ is a small constant that depends on the specific FMM implementation and it is independent of $\epsilon,n$.
    \begin{proof}\textcolor{red}{TOPROVE 2}\end{proof}
\end{lemma}

Lemma \ref{lemma:tridiagonal_assembly} gives rise to the following recursive algorithm. We can finally proceed with the proof of Theorem \ref{theorem:alg_recursive_diagonalization}, which gives the complexity of Algorithm \ref{algorithm:recursive_diagonalization}.
\begin{algorithm}[htb]
    \caption{Recursive algorithm based on \cite{gu1995divide} to diagonalize a symmetric tridiagonal matrix.}
    \label{algorithm:recursive_diagonalization}
    \small
    \begin{algorithmic}[1]
        \Statex \textbf{Algorithm}: $[\matUtilde,\matLambdatilde]\leftarrow \DIAGONALIZE(\matT,\epsilon)$
        \If{$n\leq 2$}
            \State Compute $\matUtilde,\matLambdatilde$ to be the  exact diagonalization of $\matT$.
        \Else {\bf :}
            \State Partition $\matT = \begin{pmatrix}
                \matT_1 & \beta_{k+1}\vece_k & \\
                \beta_{k+1}\vece_k^\top & \alpha_{k+1} & \beta_{k+2}\vece_1^\top \\
                 & \beta_{k+2}\vece_1 & \matT_2
            \end{pmatrix}$.
            \State $[\matUtilde_1,\matDtilde_1]\leftarrow \DIAGONALIZE(\matT_1,\epsilon)$.
            \State $[\matUtilde_2,\matDtilde_2]\leftarrow \DIAGONALIZE(\matT_2,\epsilon)$.
            \State Assemble $\matUtilde,\matLambdatilde$ from $\matT,\matUtilde_1,\matDtilde_1,\matUtilde_2,\matDtilde_2$ using Lemma \ref{lemma:tridiagonal_assembly} with parameter $\epsilon$.
        \EndIf
        \State \Return $\matUtilde,\matLambdatilde$.
    \end{algorithmic}
\end{algorithm}


\begin{proof}\textcolor{red}{TOPROVE 3}\end{proof}

    \subsection{Hermitian diagonalization}
    Given an algorithm to diagonalize tridiagonal matrices, the following corollary is immediate.
    \begin{corollary}
    \label{corollary:hermitian_diagonalization}
    Let $\matA$ be a Hermitian matrix of size $n$ with $\|\matA\|\leq 1$. Given accuracy $\epsilon\in(0,1/2)$, and an $(\epsilon,n)$-\fmmalgo\   implementation of Prop. \ref{proposition:fmm}, we can compute a matrix $\matQtilde$ and a diagonal matrix $\matLambdatilde$ such that
    \begin{align*}
        \lnorm \matA - \matQtilde\matLambdatilde\matQtilde^* \rnorm \leq \epsilon,
        \quad
        \lnorm \matQtilde^* \matQtilde - \matI \rnorm \leq \epsilon/n^2.
    \end{align*}
    The algorithm requires a total of $O\lpar n^\omega\log(n) + n^2\log^{\cfmm+1}(\tfrac{n}{\epsilon})\rpar$ arithmetic operations and comparisons, where $\xi\geq 1$ is a small constant that depends on the specific FMM implementation and it is independent of $\epsilon,n$.
    \begin{proof}\textcolor{red}{TOPROVE 4}\end{proof}
    \end{corollary}

\section{Stability of tridiagonal reduction}
\label{section:tridiagonal_reduction_stability}
In this section we analyze the numerical stability and the boolean complexity of  Schönhage's algorithm the floating point model. For this we will use the following stable matrix multiplication and backward-stable QR factorization algorithms as subroutines from \cite{demmel2007fastla,demmel2007fastmm}. 
The corresponding definitions and imported results for these subroutines are deferred to Appendix \ref{appendix:tridiagonal_reduction_subroutines}.


\subsection{Matrix nomenclature}
Schönhage \cite{schonhage1972unitare} used a block variant of Rutishauser's algorithm \cite{rutishauser1963jacobi} to reduce a matrix to tridiagonal form, where elementary rotations are replaced with block factorizations; 
see also \cite{bischof2000framework,ballard2012communication,ballard2015avoiding} for similar methodologies. 
We start with a $n\times n$ block-pentadiagonal matrix $\matA^{(k,s,t)}$, where $k\in \big\{0,\ldots,\log(n)-2\big\}$ (we assume without loss of generality that $n$ is a power of two). 
The matrix $\matA^{(k,s,t)}$ is partitioned in $b_k\times b_k$ blocks of size $n_k\times n_k$ each, where $b_k=\tfrac{n}{n_k}$ and $n_k=2^{k}$. 
The integer $s\in 2,\ldots,b_k$ t denotes that all the blocks $\matA_{i,i-2}$ and $\matA_{i-2,i}$, for all $i=2,\ldots,s$ are equal to zero. $s=2$ is a special case to denote a full block pentadiagonal matrix. 
The integer $t\in \big\{s+2,\ldots, b_k\big\}$  denotes that the matrix has two additional nonzero blocks in the third off-diagonals, specifically at positions $\matA_{t,t-3}$ and $\matA_{t-3,t}$. 
These blocks are often called the ``bulge'' in the literature.
When $t=0$, there is no bulge. As a consequence, the matrix $\matA^{(k,2,0)}$ is full block-pentadiagonal, while the matrix $\matA^{(k,b_k,0)}$ is block-tridiagonal. An illustration of these definitions is shown in Equation \eqref{eq:block_pentadiagonal}. A box is placed around the bulge on the second matrix.
\begingroup
\small
\setlength\arraycolsep{0.8pt}
\begin{align}
    \label{eq:block_pentadiagonal}
    \overbrace{
        \begin{pmatrix}
            \matA_{1,1}  & \matA_{1,2}  &\matA_{1,3}   & 0             & 0           & 0           & 0           & 0           \\
            \matA_{2,1}  & \matA_{2,2}  &\matA_{2,3}   & \matA_{2,4}   & 0           & 0           & 0           & 0           \\
            \matA_{3,1}  & \matA_{3,2}  &\matA_{3,3}   & \matA_{3,4}   & \matA_{3,5} & 0           & 0           & 0           \\
            0            & \matA_{4,2}  &\matA_{4,3}   & \matA_{4,4}   & \matA_{4,5} & \matA_{4,6} & 0           & 0           \\
            0            & 0            &\matA_{5,3}   & \matA_{5,4}   & \matA_{5,5} & \matA_{5,6} & \matA_{5,7} & 0           \\
            0            & 0            & 0            & \matA_{6,4}   & \matA_{6,5} & \matA_{6,6} & \matA_{6,7} & \matA_{6,8} \\
            0            & 0            & 0            & 0             & \matA_{7,5} & \matA_{7,6} & \matA_{7,7} & \matA_{7,8} \\
            0            & 0            & 0            & 0             & 0           & \matA_{8,6} & \matA_{8,7} & \matA_{8,8} \\
        \end{pmatrix}
    }^{
        \matA^{(k,2,0)}
    },
    \ 
    \overbrace{
    \begin{pmatrix}
        \matA_{1,1}  & \matA_{1,2}  & 0            & 0             & 0           & 0           & 0           & 0           \\
        \matA_{2,1}  & \matA_{2,2}  &\matA_{2,3}   & 0             & 0           & 0           & 0           & 0           \\
        0            & \matA_{3,2}  &\matA_{3,3}   & \matA_{3,4}   & \matA_{3,5} & \boxed{\matA_{3,6}} & 0           & 0           \\
        0            & 0            &\matA_{4,3}   & \matA_{4,4}   & \matA_{4,5} & \matA_{4,6} & 0           & 0           \\
        0            & 0            &\matA_{5,3}   & \matA_{5,4}   & \matA_{5,5} & \matA_{5,6} & \matA_{5,7} & 0           \\
        0            & 0            &\boxed{\matA_{6,3}}   & \matA_{6,4}   & \matA_{6,5} & \matA_{6,6} & \matA_{6,7} & \matA_{6,8} \\
        0            & 0            & 0            & 0             & \matA_{7,5} & \matA_{7,6} & \matA_{7,7} & \matA_{7,8} \\
        0            & 0            & 0            & 0             & 0           & \matA_{8,6} & \matA_{8,7} & \matA_{8,8} \\
    \end{pmatrix}}
    ^{
        \matA^{(k,4,6)}
    }
    .
\end{align}
\normalsize
\endgroup

\subsection{Rotations}
The algorithm defines two types of block rotations $R_i$ and $R_i'$, which are unitary similarity transformations, with the following properties.
\begin{definition}[Rotations]
    The algorithm of \cite{schonhage1972unitare} uses the following two types of rotations:
    \begin{enumerate}
    \item $R_i(\matA^{(k,i,0)})$, for $i=2,\ldots,b_k-1$, operates on a block-pentadiagonal matrix without a bulge. It transforms the matrix $\matA^{(k,i,0)}$ to a matrix $\matA^{(k,i+1,i+3)}$. In paricular, the block $\matA_{i,i-1}$ becomes upper triangular, the block $\matA_{i+1,i-1}$ becomes zero, and a new bulge block arises at $\matA_{i,i+3}$. Due to symmetry, $\matA_{i-1,i}$ becomes lower triangular, $\matA_{i-1,i+1}$ is eliminated, and $\matA_{i+3,i}$ becomes non-zero. 
    \item $R_j'(\matA^{(k,s,j+1)})$, for some $j=s+1,\ldots,b_k-1$, operates on a block-pentadiagonal matrix with a bulge at positions $\matA_{j+1,j-2}$, $\matA_{j-2,j+1}$. It transforms the matrix $\matA^{(k,s,j+1)}$ to a matrix $\matA^{(k,s,j+3)}$, such that the bulge is moved two positions ``down-and-right'', i.e. the blocks $\matA_{j-2,j+1}$ and  $\matA_{j+1,j-2}$ become zero and the blocks $\matA_{j,i+3}$ and $\matA_{j,j+3}$ become the new bulge. In addition, the matrices $\matA_{j,j-2}$ and $\matA_{j+1,j-1}$ become upper triangular, and, by symmetry, the matrices $\matA_{j-2,j}$ and $\matA_{j-1,j+1}$ become lower triangular.
\end{enumerate}
\end{definition}

An example of the aforementioned rotations is illustrated in Equations \eqref{eq:rotation_r_i} and \eqref{eq:rotation_r_i_prime} in the Appendix, and in Lemmas \ref{lemma:rotation_r_i_floating_point} and \ref{lemma:rotation_r_i_prime_floating_point} it is proved that both types can be stably implemented in floating point using fast QR factorizations.

\subsection{Recursive bandwidth halving}
Using Lemmas \ref{lemma:rotation_r_i_floating_point} and \ref{lemma:rotation_r_i_prime_floating_point}, we can analyze the following Algorithm \ref{algorithm:halve}, which halves the bandwidth of a matrix. Its complexity and stability properties are stated in Lemma \ref{lemma:bandwidth_halving_floating_point_appendix} in the Appendix.
Applying this algorithm recursively gives the main Theorem \ref{theorem:stable_tridiagonal_reduction}.
\begin{algorithm}[htb]
    \caption{Halves the bandwidth of a Hermitian matrix with unitary rotations.}
    \label{algorithm:halve}
    \small
    \begin{algorithmic}[1]
        \Statex \textbf{Algorithm}: $[\matQhat^{(k)},\matA^{(k-1,2,0)}]\leftarrow \HALVE(\matA^{(k,2,0)},k,n)$
        \State Set $n_k=2^k$, $b_k=n/n_k$.
        \For{$i=2,\ldots, b_k$}
            \State Compute $\matQ_{i,i},\matA^{(k,i+1,i+3)} \leftarrow R_i(\matA^{(k,i,0)})$.
            \For{$j=i+2,\ldots,b_k$ with step $2$}
                \State Compute $\matQ_{i,j},\matA^{(k,i+1,j+3)} \leftarrow R'_j(\matA^{(k,i+1,j+1)})$.
            \EndFor
            \State Stack together all the matrices $\matQ_{i,j}$ to form $\matQ_i$.
        \EndFor
        \State Assemble the matrix $\matQhat^{(k)}$ by multiplying the matrices $\matQ_{i}$.
        \State \Return $\matQhat^{(k)},\matA^{(k-1,2,0)}$.
    \end{algorithmic}
\end{algorithm}

\begin{theorem}
    \label{theorem:stable_tridiagonal_reduction}
    There exists a floating point implementation of the tridiagonal reduction algorithm of \cite{schonhage1972unitare}, which takes as input a Hermitian matrix $\matA$, and returns a tridiagonal matrix $\matTtilde$, and (optionally) an approximately unitary matrix $\matQtilde$. If the machine precision $\umach$ satisfies
    $
        \umach \leq \epsilon\frac{1}{cn^{\beta+4}},
    $
    where $\epsilon\in(0,1)$, $c$ is a constant, and $\cmm$ is the same as in Corollary \ref{corollary:alg_qr}, which translates to $O(\log(n)+\log(1/\epsilon))$ bits of precision, then the following hold:
    \begin{align*}
        \lnorm \matQtilde\matQtilde^*-\matI\rnorm \leq \epsilon,
        \quad
        \text{and}
        \quad
        \lnorm \matA - \matQtilde\matTtilde\matQtilde^* \rnorm \leq \epsilon \lnorm \matA \rnorm.
    \end{align*}
    The algorithm executes at most $O\lpar
                    n^2 S_{\omega}(\log(n))
    \rpar$ floating point operations to return only $\matTtilde$, where $S_x(m)=\sum_{l=1}^m (2^{x-2})^l$.
    If $\matA$ is banded with $1\leq d\leq n$ bands, the floating point operations reduce to $O(n^2 S_{\omega}(\log(d))$.
    If $\matQtilde$ is also returned, the complexity increases to $O(n^2C_{\omega}(\log(n)))$, 
    where $C_{x}(n) := 
    \sum_{k=2}^{\log(n)-2}
    \lpar
        S_{x}(\log(n)-1) - S_{x}(k)
    \rpar$. If $\omega$ is treated as a constant $\omega\approx 2.371$ the corresponding complexities are $O(n^{\omega}), O(n^2d^{\omega-2}),$ and $O(n^{\omega}\log(n))$, respectively.
    \begin{proof}\textcolor{red}{TOPROVE 5}\end{proof}
\end{theorem}

\subsection{Eigenvalues of Hermitian matrices}
\label{section:hermitian_eigenvalues}
We now have all the prerequisites to compute  the eigenvalues Hermitian matrices in nearly matrix multiplication time in finite precision.
For this we can use the eigenvalue solver of \cite{bini1991parallel}, which has $\widetilde O(n^2)$ boolean complexity, albeit in the Boolean RAM model. Specifically, the algorithm accepts as input symmetric tridiagonal matrices with bounded integer entries. 

\begin{theorem}[Imported from \cite{bini1991parallel,bini1998computing}]
Let $\matT$ be a symmetric tridiagonal matrix with integer elements bounded in magnitude by $2^m$ for some $m$. Let $\epsilon=2^{-u}\in(0,1)$ be a desired accuracy. Algorithm 4.1 of \cite{bini1991parallel} computes a set of approximate eigenvalues $\widetilde\lambda_i\in\mathbb{R}$ (which are returned as rationals) such that
$
    \labs \widetilde\lambda_i-\lambda_i(\matT) \rabs < \epsilon.
$
The algorithm requires
$
    O\lpar
        n^2b\log^2(n)\log(nb)(\log^2(b)+\log(n))\log(\log(nb))
    \rpar
$
boolean operations, where $b=m+u$.
\label{theorem:bini_pan_tridiagonal_eigenvalues}
\end{theorem}

\begin{theorem}
    \label{theorem:hermitian_eigenvalues}
    Let $\matA$ be a (banded) Hermitian matrix, with $\|\matA\|\leq 1$, $1\leq d\leq n-1$ off-diagonals, and let $\epsilon\in(0,1)$ be an  accuracy parameter. Assume that the elements of $\matA$ are floating point numbers on a machine with precision $\umach$, $t=\log(1/\umach)$ bits for the significand, and $p=O(\log(\log(n)))$ bits for the exponent. There exists an algorithm that returns a set of $n$ approximate eigenvalues $\widetilde\lambda_1,\ldots,\widetilde\lambda_n$ such that
    \begin{align*}
        \labs \widetilde\lambda_i - \lambda_i(\matA) \rabs
        \leq
        \epsilon
    \end{align*}
    using at most 
    \begin{align*}
        O\lpar
            n^2S_{\matmulexponent}(\log(d))\cdot \flopcost(\log(\tfrac{n}{\epsilon}))
            +
            n^2\polylog(\tfrac{n}{\epsilon})
        \rpar
    \end{align*}
    boolean operations, where $\flopcost(b)$ is the bit complexity of a floating point operation on $b$ bits, and  $n^2S_{\matmulexponent}(\log(d))=O(n^2d^{\matmulexponent-2})$ if $\omega$ is treated as a constant greater than two.
    \begin{proof}\textcolor{red}{TOPROVE 6}\end{proof}
\end{theorem}



\section{Further applications of stable tridiagonal reduction and Hermitian eigenvalue solver}
\label{section:applications_tridiagonal_reduction}
In this section we state some applications of the tridiagonal reduction algorithm to some eigenproblems.


\subsection{Singular values and condition number}
\begin{proposition}
    \label{proposition:alg_sigmak}
        Given a matrix $\matA\in\mathbb{C}^{n\times n},$ with $\|\matA\|\leq 1$, an integer $k\in[n]$, and accuracy $\epsilon\in(0,1)$, we can compute a value $\widetilde\sigma_{k}\in (1\pm\epsilon)\sigma_k(\matA)$ in
        \begin{align*}
            O\lpar
                \lbrac
                    n^{\omega}\flopcost(\log(\tfrac{n}{\epsilon\sigma_k}))
                    +
                    n^2\polylog(\tfrac{n}{\epsilon\sigma_k})
                \rbrac
                \log(\log(\epsilon\sigma_k))
            \rpar
        \end{align*}
        boolean operations, deterministically.\\
        \textbf{Note:} If $\|\matA\|>1$, we can scale with $1/(n\|\matA\|_{\max})$ or $1/\|\matA\|_F$. The complexity is unaffected. 
    \begin{proof}\textcolor{red}{TOPROVE 7}\end{proof}
\end{proposition}
\begin{corollary}
    \label{corollary:alg_cond}
    Let $\matA\in\mathbb{C}^{n\times n}$, $\kappa=\kappa(\matA)$, and $\delta\in(0,1/2)$. We can compute $\widetilde\kappa$ which $\kappa\leq \widetilde\kappa\leq  3n\kappa$, in 
    \begin{align*}
        O\lpar
            \lbrac
                n^{\omega}\flopcost(\log(n\kappa))
                +
                n^2\polylog(n\kappa)
            \rbrac
            \log(\log(n\kappa))
        \rpar
    \end{align*}
    boolean operations deterministically. 
    \begin{proof}\textcolor{red}{TOPROVE 8}\end{proof}
\end{corollary}

\subsection{Definite pencil eigenvalues}
Using the proposed algorithms we can compute the eigenvalues of a Hermitian definite pencil.
\begin{corollary}
    \label{corollary:hermitian_definite_pencil_eigenvalues}
    Let $\matH$ be Hermitian, $\matS$ Hermitian positive-definite, both with size $n$ and floating point elements, on a machine with precision $\umach$, $t=\log(1/\umach)$ bits for the significand, and $p=O(\log(\log(n)))$ bits for the exponent. Assume that $\|\matH\|,\|\matS^{-1}\|\leq 1$, $\kappa(\matS)\in\poly(n)$, and that we have access to $\widetilde\kappa\in[\kappa(\matS),Z\kappa(\matS)]$, where $Z>1$ might be a constant or a function of $n$. There exists an algorithm that returns a set of $n$ approximate eigenvalues $\widetilde\lambda_1,\ldots,\widetilde\lambda_n$ such that
    \begin{align*}
        \labs \widetilde\lambda_i - \lambda_i(\matH,\matS) \rabs
        \leq
        \epsilon
    \end{align*}
    using at most 
    \begin{align*}
        O\Big(
            n^{\omega}
            \flopcost(\log(n)\log(Z\kappa(\matS)) + \log(\tfrac{1}{\epsilon}))
            +
            n^2\polylog(\tfrac{n}{\epsilon})
        \Big)
    \end{align*}
    boolean operations. If $\widetilde\kappa$ is not given, it can be computed up to a factor $Z=3n$ with Corollary \ref{corollary:alg_cond}.
    \begin{proof}\textcolor{red}{TOPROVE 9}\end{proof}
\end{corollary}
\begin{remark}
    In Theorem \ref{corollary:hermitian_definite_pencil_eigenvalues} we assumed that $\|\matH\|,\|\matS^{-1}\|\leq 1$. This is not a limitation since we can approximate $\|\matS^{-1}\|$ with Proposition \ref{proposition:alg_sigmak}, and then scale accordingly. Formally, let $\eta \gtrsim \|\matH\|$ and $\sigma\gtrsim \|\matS^{-1}\|$. Then we can rewrite the generalized eigenproblem
\begin{align*}
\matH\matC=\matS\matC\geneigmatrix
\quad
\Leftrightarrow
\quad
(\tfrac{1}{\eta}\matH)\matC = (\sigma\matS) \matC (\geneigmatrix\tfrac{1}{\eta\sigma})
\quad
\Leftrightarrow
\quad
\matH'\matC = \matS'\matC\geneigmatrix',     
\end{align*}
i.e. it is the same generalized eigenproblem only with scaled eigenvalues. Assuming that the matrices $\matH$ and $\matS$ are ``well-conditioned,'' i.e. their norms and condition numbers $\in \poly(n)$, the eigenvalues are scaled by at most a $1/\poly(n)$ factor, and thus it suffices to scale $\epsilon$ by $1/\poly(n)$ as well. We can thus safely make the unit-norms assumption. 
\end{remark}


\subsection{Spectral gaps}
In a similar way we can compute the spectral gap between a pair of eigenvalues of Hermitian matrices and Hermitian-definite pencils.

\begin{corollary}
    \label{corollary:spectral_gap}
    Let $\matA$ be a banded Hermitian matrix of size $n$ with $1\leq d\leq n-1$ off-diagonals, $\|\matA\|\leq 1$, and its eigenvalues $\lambda_1\leq \lambda_2\leq \ldots \leq \lambda_n$. For some integer $k\in[n-1]$, let $\mu_k=\frac{\lambda_k+\lambda_{k+1}}{2}$ and $\gap_k=\lambda_{k+1}-\lambda_k$. Given an accuracy $\epsilon\in(0,1)$, we can compute two values $\widetilde\mu_k$ and $\widetilde\gap_k$ such that
    \begin{align*}
            \widetilde\mu_k\in \mu_k\pm \epsilon\gap_k,
            \quad
            \text{and}
            \quad
            \widetilde\gap_k \in(1\pm\epsilon)\gap_k,
    \end{align*}
    in
     \begin{align*}
            O\lpar
                n^2
                \lbrac
                    d^{\omega-2}
                    \cdot
                    \flopcost(\log(\tfrac{n}{\epsilon\gap_k}))
                    +
                    \polylog(\tfrac{n}{\epsilon\gap_k})
                \rbrac
                \log(\log(\tfrac{1}{\epsilon\gap_k}))
            \rpar
    \end{align*}
    boolean operations.
    \begin{proof}\textcolor{red}{TOPROVE 10}\end{proof}
\end{corollary}

\begin{corollary}
    \label{corollary:alg_deterministic_spectral_gap}
    Let $\matH$ be Hermitian, $\matS$ Hermitian positive-definite, both with size $n$ and $\|\matH\|,\|\matS^{-1}\|\leq 1$, which define a Hermitian-definite pencil $(\matH,\matS)$. Given $k\in[n-1]$,  $\widetilde\kappa\in[\kappa(\matS),Z\kappa(\matS)]$, where $Z>1$ might be a constant or a function of $n$, and accuracy $\epsilon\in(0,1/2)$, we can compute $\widetilde\mu_k=\mu_k\pm\epsilon\gap_k$ and $\widetilde \gap_k=(1\pm\epsilon)\gap_k$, where $\mu_k=\tfrac{\lambda_k+\lambda_{k+1}}{2}$ and $\gap_k=\lambda_{k}-\lambda_{k+1}$. The algorithm requires 
    \begin{align*}
        O\lpar
            \lbrac
                n^{\omega}
                \flopcost\lpar
                    \log(n)\log(Z\kappa(\matS)) + \log(\tfrac{1}{\epsilon\gap_k})
                \rpar
                +
                n^2\polylog(\tfrac{n}{\epsilon\gap_k})
            \rbrac
            \log(\log(\tfrac{1}{\epsilon\gap_k}))
        \rpar
    \end{align*}
    boolean operations. If $\widetilde\kappa$ is not given, it can be computed up to a factor $Z=3n$ with Corollary \ref{corollary:alg_cond}.
    \begin{proof}\textcolor{red}{TOPROVE 11}\end{proof}
\end{corollary}


\subsection{Spectral projectors and invariant subspaces}
For a Hermitian matrix $\matA$, the algorithm called $\GAP$ in \cite{sobczyk2024invariant} computes the spectral gap and the midpoint in the spirit of Corollary \ref{corollary:spectral_gap} using
$O\lpar n^{\omega}\log(\tfrac{1}{\epsilon\gap_k})\log(\tfrac{1}{\delta\epsilon\gap_k})
\cdot 
\flopcost\Big( 
    \log^3(\tfrac{n}{\delta\epsilon\gap_k})\log(\tfrac{1}{\delta\epsilon\gap_k})\log(n)
\Big)
\rpar$ boolean operations and succeeds with probability $1-\delta$.
On the other hand, if $\omega$ is treated as a constant larger than two, the algorithm of Corollary \ref{corollary:spectral_gap} requires
\begin{align*}
    O\lpar
        n^{\omega}\flopcost(\log(n))
            +
        n^2\polylog(n/\epsilon)
    \rpar
\end{align*}
boolean operations, which is significantly faster than $\GAP$. 
Moreover, it is deterministic, and it has a lower complexity for banded matrices.
A key difference between the two algorithms is that the $\GAP$ algorithm of \cite{sobczyk2024invariant} is fully analyzed in floating point, while the algorithm of \ref{corollary:spectral_gap} requires \cite{bini1998computing} which is analyzed in Boolean RAM.

Note that originally the $\GAP$ algorithm was used as a preliminary step to locate the gap and the midpoint in order to compute spectral projectors on invariant subspaces. Corollary \ref{corollary:spectral_gap} can serve as a direct replacement, providing an end-to-end deterministic algorithm for computing spectral projectors.

\begin{corollary}
    \label{corollary:spectral_projector}
    Let $(\matH,\matS)$ be a Hermitian 
     definite pencil of size $n$, with $\|\matH\|,\|\matS^{-1}\|\leq 1$, and  $\lambda_1\leq\lambda_2\leq\ldots\leq \lambda_n$ its eigenvalues. Given as input $\matH$, $\matS$, an integer $1\leq k\leq n-1$, an error parameter $\epsilon\in(0,1)$, we can compute a matrix $\projectormatrixtilde_k$ such that
    \begin{align*}
            \lnorm \projectormatrixtilde_k - \projectormatrix_k \rnorm \leq \epsilon,
    \end{align*}
    where $\projectormatrix_k$ is the true spectral projector on the invariant subspace that is associated with the $k$ smallest eigenvalues, in 
    \begin{align*}
        O\lpar
            n^{\omega}
            \flopcost
            \lpar
                \log(n)
                \log^3(\tfrac{1}{\gap_k})\log(\tfrac{n\kappa}{\epsilon\gap_k})
            \rpar
            \lpar
            \log(\tfrac{1}{\gap_k})+\log(\log(\tfrac{n\kappa}{\epsilon\gap_k}))
            \rpar
            +
            n^{2}\polylog(\tfrac{n\kappa}{\gap_k})
        \rpar
    \end{align*}
    boolean operations, where $\kappa:=\kappa(\matS)=\|\matS\|\|\matS^{-1}\|$ and $\gap_k=\lambda_{k+1}-\lambda_k$.
\begin{proof}\textcolor{red}{TOPROVE 12}\end{proof}
\end{corollary}


\subsection{Stable inversion and factorizations}
Theorem \ref{theorem:stable_tridiagonal_reduction} can be used to invert a matrix stably in floating point, improving the bit requirement of the logarithmically-stable algorithm of \cite{demmel2007fastla}, however, at the cost of increasing the arithmetic complexity by a factor of $\log(n),$ which ultimately leads to a slower algorithm. To achieve this, we first form the matrix $\begin{pmatrix}
    0 & \matA^* \\
    \matA & 0
\end{pmatrix},$ and then reduce it to tridiagonal form. Afterwards we can solve $O(n)$ linear systems stably with QR factorization of the tridiagonal matrix, where the right hand sides are the columns of $\matQtilde$, in time $O(n)$ each. We perform one final multiplication to obtain the inverse. We do not expand the analysis further since the boolean complexity of the algorithm is slower than the one of \cite{demmel2007fastla} (even if we use a slow algorithm for basic arithmetic operations).
The stable inversion algorithm can be used to obtain stable factorization algorithms, such as LU \cite{demmel2007fastla} or Cholesky \cite{sobczyk2024invariant}.






\section{Conclusion}
\label{section:conclusion}
In this work we provided a deterministic complexity analysis for Hermitian eigenproblems. In the Real RAM model, we reported nearly-linear complexity upper bounds for the full diagonalization of arrowhead and tridiagonal matrices, and nearly matrix multiplication-type complexities for diagonalizing Hermitian matrices and for the SVD. This was achieved by analyzing the divide-and-conquer algorithm of \cite{gu1995divide}, when implemented with the Fast Multipole Method. We also showed that the tridiagonal reduction algorithm of \cite{schonhage1972unitare} is numerically stable in the floating point model. This paved the way to obtain improved deterministic boolean complexities for computing the eigenvalues, singular values,  spectral gaps, and spectral projectors, of Hermitian matrices and Hermitian-definite pencils.
Some interesting questions for future research are the following.
\begin{enumerate}
    \item \textbf{Stability of arrowhead diagonalization:} The FMM-accelerated arrowhead diagonalization algorithm was only analyzed in the Real RAM model. Several works \cite{gu1995divide,vogel2016superfast,ou2022superdc,cai2020stable} have provided stabilization techniques of related algorithms in floating point, albeit, without an end-to-end complexity analysis. Such an analysis will be insightful to better understand the boolean complexity of (Hermitian) diagonalization. 
    \item \textbf{Condition number in the SVD complexity:} The complexity of the SVD in Theorem \ref{theorem:svd} has a polylogarithmic dependence on the condition number. 
    Frustratingly, we were not able to remove it at the time of this writing.
    \item \textbf{Non-Hermitian diagonalization:} Sch\"onhage also proved that a non-Hermitian matrix can be reduced to upper Hessenberg form in matrix multiplication time \cite{schonhage1972unitare}. In this work we only provided the stability analysis for the Hermitian case. It would be interesting to investigate whether the Hessenberg reduction algorithm can be used to diagonalize non-Hermitian matrices in matrix multiplication time deterministically (e.g. in conjunction with \cite{banks2021global1,banks2022global2,banks2022global3}).
    \item \textbf{Deterministic pseudospectral shattering:} One of the main techniques of the seminal work of \cite{banks2022pseudospectral} is called ``pseudospectral shattering.'' The main idea is to add a tiny random perturbation to the original matrix to ensure that the minimum eigenvalue gap between any pair of eigenvalues will be at least polynomial in $1/n$. We highlight that the deflation preprocessing step in Proposition \ref{proposition:arrowhead_deflation} has this precise effect: the pseudospectrum is shattered with respect to a deterministic grid. Can this be generalized to obtain \textit{deterministic} pseudospectral shattering techniques, for Hermitian or even non-Hermitian matrices?
\end{enumerate}


\bibliographystyle{plain}
\begin{thebibliography}{10}

\bibitem{alman2024more}
Josh Alman, Ran Duan, Virginia~Vassilevska Williams, Yinzhan Xu, Zixuan Xu, and Renfei Zhou.
\newblock More asymmetry yields faster matrix multiplication.
\newblock In {\em Proc. 2025 ACM-SIAM Symposium on Discrete Algorithms}, pages 2005--2039. SIAM, 2025.

\bibitem{andoni2013eigenvalues}
Alexandr Andoni and Huy~L Nguyen.
\newblock Eigenvalues of a matrix in the streaming model.
\newblock In {\em Proc. 24th ACM-SIAM Symposium on Discrete Algorithms}, pages 1729--1737. SIAM, 2013.

\bibitem{armentano2018stable}
Diego Armentano, Carlos Beltr{\'a}n, Peter B{\"u}rgisser, Felipe Cucker, and Michael Shub.
\newblock A stable, polynomial-time algorithm for the eigenpair problem.
\newblock {\em Journal of the European Mathematical Society}, 20(6):1375--1437, 2018.

\bibitem{ballard2012communication}
Grey Ballard, James Demmel, and Nicholas Knight.
\newblock Communication avoiding successive band reduction.
\newblock {\em ACM SIGPLAN Notices}, 47(8):35--44, 2012.

\bibitem{ballard2015avoiding}
Grey Ballard, James Demmel, and Nicholas Knight.
\newblock Avoiding communication in successive band reduction.
\newblock {\em ACM Transactions on Parallel Computing}, 1(2):1--37, 2015.

\bibitem{banks2022pseudospectral}
Jess Banks, Jorge Garza-Vargas, Archit Kulkarni, and Nikhil Srivastava.
\newblock Pseudospectral {S}hattering, the {S}ign {F}unction, and {D}iagonalization in {N}early {M}atrix {M}ultiplication {T}ime.
\newblock {\em Foundations of Computational Mathematics}, pages 1--89, 2022.

\bibitem{banks2022global2}
Jess Banks, Jorge Garza-Vargas, and Nikhil Srivastava.
\newblock {Global Convergence of Hessenberg Shifted QR II: Numerical Stability}.
\newblock {\em arXiv}, 2022.

\bibitem{banks2021global1}
Jess Banks, Jorge Garza-Vargas, and Nikhil Srivastava.
\newblock Global convergence of {H}essenberg {S}hifted {QR I: E}xact {A}rithmetic.
\newblock {\em Foundations of Computational Mathematics}, pages 1--34, 2024.

\bibitem{banks2022global3}
Jess Banks, Jorge Garza-Vargas, and Nikhil Srivastava.
\newblock Global {Convergence of Hessenberg Shifted QR III: Approximate Ritz Values via Shifted Inverse I}teration.
\newblock {\em SIAM Journal on Matrix Analysis and Applications}, 46(2):1212--1246, 2025.

\bibitem{barlow1993error}
Jesse~L Barlow.
\newblock Error analysis of update methods for the symmetric eigenvalue problem.
\newblock {\em SIAM Journal on Matrix Analysis and Applications}, 14(2):598--618, 1993.

\bibitem{benor2018quasi}
Michael Ben-Or and Lior Eldar.
\newblock {A Quasi-Random Approach to Matrix Spectral Analysis}.
\newblock In {\em Proc. 9th Innovations in Theoretical Computer Science Conference}, pages 6:1--6:22. Schloss Dagstuhl--Leibniz-Zentrum fuer Informatik, 2018.

\bibitem{bhatia2007perturbation}
Rajendra Bhatia.
\newblock {\em Perturbation bounds for matrix eigenvalues}.
\newblock SIAM, 2007.

\bibitem{bini1991parallel}
Dario Bini and Victor Pan.
\newblock Parallel complexity of tridiagonal symmetric eigenvalue problem.
\newblock In {\em Proc. 2nd ACM-SIAM Symposium on Discrete Algorithms}, pages 384--393, 1991.

\bibitem{bini1992practical}
Dario Bini and Victor Pan.
\newblock Practical improvement of the divide-and-conquer eigenvalue algorithms.
\newblock {\em Computing}, 48(1):109--123, 1992.

\bibitem{bini1998computing}
Dario Bini and Victor~Y Pan.
\newblock Computing matrix eigenvalues and polynomial zeros where the output is real.
\newblock {\em SIAM Journal on Computing}, 27(4):1099--1115, 1998.

\bibitem{bischof2000framework}
Christian~H Bischof, Bruno Lang, and Xiaobai Sun.
\newblock A framework for symmetric band reduction.
\newblock {\em ACM Transactions on Mathematical Software}, 26(4):581--601, 2000.

\bibitem{boley1977inverse}
D~Boley and Gene~Howard Golub.
\newblock Inverse eigenvalue problems for band matrices.
\newblock In {\em Numerical Analysis: Proceedings of the Biennial Conference Held at Dundee, June 28--July 1, 1977}, pages 23--31. Springer, 1977.

\bibitem{boutsidis2014optimal}
Christos Boutsidis and David~P Woodruff.
\newblock Optimal {CUR} matrix decompositions.
\newblock In {\em Proc. 46th ACM Symposium on Theory of Computing}, pages 353--362, 2014.

\bibitem{boutsidis2016optimal}
Christos Boutsidis, David~P Woodruff, and Peilin Zhong.
\newblock Optimal principal component analysis in distributed and streaming models.
\newblock In {\em Proc. 48th ACM Symposium on Theory of Computing}, pages 236--249, 2016.

\bibitem{boutsidis2014randomized}
Christos Boutsidis, Anastasios Zouzias, Michael~W Mahoney, and Petros Drineas.
\newblock Randomized dimensionality reduction for $ k $-means clustering.
\newblock {\em IEEE Transactions on Information Theory}, 61(2):1045--1062, 2014.

\bibitem{cai2020stable}
Difeng Cai and Jianlin Xia.
\newblock A stable matrix version of the fast multipole method: stabilization strategies and examples.
\newblock {\em ETNA-Electronic Transactions on Numerical Analysis}, 54, 2020.

\bibitem{clarkson2017low}
Kenneth~L Clarkson and David~P Woodruff.
\newblock Low-rank approximation and regression in input sparsity time.
\newblock {\em Journal of the ACM}, 63(6):1--45, 2017.

\bibitem{cohen2015dimensionality}
Michael~B Cohen, Sam Elder, Cameron Musco, Christopher Musco, and Madalina Persu.
\newblock Dimensionality reduction for k-means clustering and low rank approximation.
\newblock In {\em Proc. 47th ACM Symposium on Theory of Computing}, pages 163--172, 2015.

\bibitem{cuppen1980divide}
Jan~JM Cuppen.
\newblock A divide and conquer method for the symmetric tridiagonal eigenproblem.
\newblock {\em Numerische Mathematik}, 36:177--195, 1980.

\bibitem{darve2000fast2}
Eric Darve.
\newblock The fast multipole method i: Error analysis and asymptotic complexity.
\newblock {\em SIAM Journal on Numerical Analysis}, 38(1):98--128, 2000.

\bibitem{darve2000fast}
Eric Darve.
\newblock The fast multipole method: numerical implementation.
\newblock {\em Journal of Computational Physics}, 160(1):195--240, 2000.

\bibitem{dekker1971shifted}
TJ~Dekker and JF~Traub.
\newblock The shifted {QR} algorithm for {Hermitian} matrices.
\newblock {\em Linear Algebra and its Applications}, 4(3):137--154, 1971.

\bibitem{demmel2007fastla}
James Demmel, Ioana Dumitriu, and Olga Holtz.
\newblock Fast linear algebra is stable.
\newblock {\em Numerische Mathematik}, 108(1):59--91, 2007.

\bibitem{demmel2007fastmm}
James Demmel, Ioana Dumitriu, Olga Holtz, and Robert Kleinberg.
\newblock Fast matrix multiplication is stable.
\newblock {\em Numerische Mathematik}, 106(2):199--224, 2007.

\bibitem{demmel2024inverse}
James Demmel, Ioana Dumitriu, and Ryan Schneider.
\newblock Fast and inverse-free algorithms for deflating subspaces.
\newblock {\em arXiv}, 2024.

\bibitem{demmel2024generalized}
James Demmel, Ioana Dumitriu, and Ryan Schneider.
\newblock {Generalized Pseudospectral Shattering and Inverse-Free Matrix Pencil Diagonalization}.
\newblock {\em Foundations of Computational Mathematics}, pages 1--77, 2024.

\bibitem{demmel1992jacobi}
James Demmel and Kre{\v{s}}imir Veseli{\'c}.
\newblock Jacobi’s method is more accurate than {QR}.
\newblock {\em SIAM Journal on Matrix Analysis and Applications}, 13(4):1204--1245, 1992.

\bibitem{demmel1997applied}
James~W Demmel.
\newblock {\em Applied numerical linear algebra}.
\newblock SIAM, 1997.

\bibitem{dhillon1997new}
Inderjit~Singh Dhillon.
\newblock {\em A new $O(N^2)$ algorithm for the symmetric tridiagonal eigenvalue/eigenvector problem}.
\newblock University of California, Berkeley, 1997.

\bibitem{dongarra2000guest}
Jack Dongarra and Francis Sullivan.
\newblock Guest {Editors Introduction} to the top 10 algorithms.
\newblock {\em Computing in Science \& Engineering}, 2(01):22--23, 2000.

\bibitem{dongarra1987fully}
Jack~J Dongarra and Danny~C Sorensen.
\newblock A fully parallel algorithm for the symmetric eigenvalue problem.
\newblock {\em SIAM Journal on Scientific and Statistical Computing}, 8(2):s139--s154, 1987.

\bibitem{drineas2008relative}
Petros Drineas, Michael~W Mahoney, and Shan Muthukrishnan.
\newblock Relative-error {CUR} matrix decompositions.
\newblock {\em SIAM Journal on Matrix Analysis and Applications}, 30(2):844--881, 2008.

\bibitem{erickson2022smoothing}
Jeff Erickson, Ivor Van Der~Hoog, and Tillmann Miltzow.
\newblock Smoothing the gap between {NP and ER}.
\newblock {\em SIAM Journal on Computing}, 0(0):FOCS20--102--FOCS20--138, 2022.

\bibitem{francis1961qr}
John~GF Francis.
\newblock The {QR} transformation a unitary analogue to the {LR} transformation—{P}art 1.
\newblock {\em The Computer Journal}, 4(3):265--271, 1961.

\bibitem{francis1962qr}
John~GF Francis.
\newblock The {QR} transformation—{P}art 2.
\newblock {\em The Computer Journal}, 4(4):332--345, 1962.

\bibitem{frieze2004fast}
Alan Frieze, Ravi Kannan, and Santosh Vempala.
\newblock Fast{ Monte-Carlo} algorithms for finding low-rank approximations.
\newblock {\em Journal of the ACM}, 51(6):1025--1041, 2004.

\bibitem{furer2007faster}
Martin F{\"u}rer.
\newblock Faster integer multiplication.
\newblock In {\em Proc. 39th ACM Symposium on Theory of Computing}, pages 57--66, 2007.

\bibitem{gill1990n}
Doron Gill and Eitan Tadmor.
\newblock An {$O(N^2)$ Method for Computing the Eigensystem of $N\times N$ Symmetric Tridiagonal Matrices by the Divide and Conquer Approach}.
\newblock {\em SIAM Journal on Scientific and Statistical Computing}, 11(1):161--173, 1990.

\bibitem{golub1965calculating}
Gene Golub and William Kahan.
\newblock Calculating the singular values and pseudo-inverse of a matrix.
\newblock {\em Journal of the Society for Industrial and Applied Mathematics, Series B: Numerical Analysis}, 2(2):205--224, 1965.

\bibitem{golub2000eigenvalue}
Gene~H Golub and Henk~A Van~der Vorst.
\newblock Eigenvalue computation in the 20th century.
\newblock {\em Journal of Computational and Applied Mathematics}, 123(1-2):35--65, 2000.

\bibitem{golub2013matrix}
Gene~H Golub and Charles~F Van~Loan.
\newblock {\em Matrix Computations}.
\newblock Johns Hopkins University Press, 2013.

\bibitem{gu1993stable}
Ming Gu and Stanley~C Eisenstat.
\newblock A stable and fast algorithm for updating the singular value decomposition, 1993.

\bibitem{gu1995divide}
Ming Gu and Stanley~C Eisenstat.
\newblock A divide-and-conquer algorithm for the symmetric tridiagonal eigenproblem.
\newblock {\em SIAM Journal on Matrix Analysis and Applications}, 16(1):172--191, 1995.

\bibitem{gu1996efficient}
Ming Gu and Stanley~C Eisenstat.
\newblock Efficient algorithms for computing a strong rank-revealing qr factorization.
\newblock {\em SIAM Journal on Scientific Computing}, 17(4):848--869, 1996.

\bibitem{hartmanis1974power}
Juris Hartmanis and Janos Simon.
\newblock On the power of multiplication in random access machines.
\newblock In {\em 15th Annual Symposium on Switching and Automata Theory}, pages 13--23. IEEE, 1974.

\bibitem{harvey2021integer}
David Harvey and Joris Van Der~Hoeven.
\newblock Integer multiplication in time {$O(n\log n)$}.
\newblock {\em Annals of Mathematics}, 193(2):563--617, 2021.

\bibitem{higham2002accuracy}
Nicholas~J Higham.
\newblock {\em Accuracy and stability of numerical algorithms}.
\newblock SIAM, 2002.

\bibitem{hoffmann1978new}
Walter Hoffmann and Beresford~N Parlett.
\newblock A new proof of global convergence for the tridiagonal {QL} algorithm.
\newblock {\em SIAM Journal on Numerical Analysis}, 15(5):929--937, 1978.

\bibitem{householder1958unitary}
Alston~S Householder.
\newblock Unitary triangularization of a nonsymmetric matrix.
\newblock {\em Journal of the ACM}, 5(4):339--342, 1958.

\bibitem{jacobi1846leichtes}
C.G.J. Jacobi.
\newblock {\"U}ber ein leichtes {V}erfahren die in der {T}heorie der {S}\"acularstörungen vorkommenden {G}leichungen numerisch aufzul\"osen.
\newblock {\em Journal für die reine und angewandte Mathematik}, 30:51--94, 1846.

\bibitem{jolliffe2002principal}
Ian~T Jolliffe.
\newblock {\em Principal component analysis for special types of data}.
\newblock Springer, 2002.

\bibitem{kacham2024faster}
Praneeth Kacham and David~P Woodruff.
\newblock Faster {Algorithms for Schatten-p Low Rank A}pproximation.
\newblock In {\em Proc. Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques}. Schloss Dagstuhl--Leibniz-Zentrum f{\"u}r Informatik, 2024.

\bibitem{kublanovskaya1962some}
Vera~N Kublanovskaya.
\newblock On some algorithms for the solution of the complete eigenvalue problem.
\newblock {\em USSR Computational Mathematics and Mathematical Physics}, 1(3):637--657, 1962.

\bibitem{lanczos1950iteration}
Cornelius Lanczos.
\newblock An iteration method for the solution of the eigenvalue problem of linear differential and integral operators.
\newblock {\em Journal of Research of the National Bureau of Standards}, 45(4), 1950.

\bibitem{livne2002n}
Oren~E Livne and Achi Brandt.
\newblock {$N$} roots of the secular equation in {$O(N)$} operations.
\newblock {\em SIAM Journal on Matrix Analysis and Applications}, 24(2):439--453, 2002.

\bibitem{louis2016accelerated}
Anand Louis and Santosh~S Vempala.
\newblock Accelerated newton iteration for roots of black box polynomials.
\newblock In {\em Proc. 57th IEEE Symposium on Foundations of Computer Science}, pages 732--740. IEEE, 2016.

\bibitem{martinsson2007accelerated}
Per-Gunnar Martinsson and Vladimir Rokhlin.
\newblock An accelerated kernel-independent fast multipole method in one dimension.
\newblock {\em SIAM Journal on Scientific Computing}, 29(3):1160--1178, 2007.

\bibitem{melman1995numerical}
A~Melman.
\newblock Numerical solution of a secular equation.
\newblock {\em Numerische Mathematik}, 69:483--493, 1995.

\bibitem{mises1929praktische}
RV~Mises and Hilda Pollaczek-Geiringer.
\newblock Praktische {Verfahren der Gleichungsaufl{\"o}s}ung.
\newblock {\em ZAMM-Journal of Applied Mathematics and Mechanics/Zeitschrift f{\"u}r Angewandte Mathematik und Mechanik}, 9(1):58--77, 1929.

\bibitem{musco2018stability}
Cameron Musco, Christopher Musco, and Aaron Sidford.
\newblock Stability of the {L}anczos method for matrix function approximation.
\newblock In {\em Proc. 29th ACM-SIAM Symposium on Discrete Algorithms}, pages 1605--1624. SIAM, 2018.

\bibitem{nakatsukasa2010optimizing}
Yuji Nakatsukasa, Zhaojun Bai, and Fran{\c{c}}ois Gygi.
\newblock Optimizing {H}alley's iteration for computing the matrix polar decomposition.
\newblock {\em SIAM Journal on Matrix Analysis and Applications}, 31(5):2700--2720, 2010.

\bibitem{nakatsukasa2013stable}
Yuji Nakatsukasa and Nicholas~J Higham.
\newblock Stable and efficient spectral divide and conquer algorithms for the symmetric eigenvalue decomposition and the {SVD}.
\newblock {\em SIAM Journal on Scientific Computing}, 35(3):A1325--A1349, 2013.

\bibitem{needell2022testing}
Deanna Needell, William Swartworth, and David~P Woodruff.
\newblock Testing positive semidefiniteness using linear measurements.
\newblock In {\em Proc. 2022 IEEE Symposium on Foundations of Computer Science}, pages 87--97. IEEE, 2022.

\bibitem{o1990computing}
Dianne~P O'Leary and Gilbert~W Stewart.
\newblock Computing the eigenvalues and eigenvectors of symmetric arrowhead matrices.
\newblock {\em Journal of Computational Physics}, 90(2):497--505, 1990.

\bibitem{ou2022superdc}
Xiaofeng Ou and Jianlin Xia.
\newblock Superdc: superfast divide-and-conquer eigenvalue decomposition with improved stability for rank-structured matrices.
\newblock {\em SIAM Journal on Scientific Computing}, 44(5):A3041--A3066, 2022.

\bibitem{paige1980accuracy}
Chris~C Paige.
\newblock Accuracy and effectiveness of the {L}anczos algorithm for the symmetric eigenproblem.
\newblock {\em Linear algebra and its Applications}, 34:235--258, 1980.

\bibitem{pan1999complexity}
Victor~Y Pan and Zhao~Q Chen.
\newblock The complexity of the matrix eigenproblem.
\newblock In {\em Proc. 31st ACM Symposium on Theory of Computing}, pages 507--516, 1999.

\bibitem{papadimitriou1998latent}
Christos~H Papadimitriou, Hisao Tamaki, Prabhakar Raghavan, and Santosh Vempala.
\newblock Latent semantic indexing: A probabilistic analysis.
\newblock In {\em Proc. 17th ACM Symposium on Principles of Database Systems}, pages 159--168, 1998.

\bibitem{parlett1998symmetric}
Beresford~N Parlett.
\newblock {\em The {S}ymmetric {E}igenvalue {P}roblem}.
\newblock SIAM, 1998.

\bibitem{rokhlin1985rapid}
Vladimir Rokhlin.
\newblock Rapid solution of integral equations of classical potential theory.
\newblock {\em Journal of computational physics}, 60(2):187--207, 1985.

\bibitem{rutishauser1963jacobi}
H~Rutishauser.
\newblock On {J}acobi rotation patterns.
\newblock In {\em Proceedings of Symposia in Applied Mathematics}, volume~15, pages 219--239, 1963.

\bibitem{saad2011numerical}
Yousef Saad.
\newblock {\em Numerical methods for large eigenvalue problems: revised edition}.
\newblock SIAM, 2011.

\bibitem{schneider2024pseudospectral}
Ryan Schneider.
\newblock {\em Pseudospectral divide-and-conquer for the generalized eigenvalue problem}.
\newblock University of California, San Diego, 2024.

\bibitem{schonhage1972unitare}
Arnold Sch{\"o}nhage.
\newblock Unit{\"a}re transformationen gro{\ss}er matrizen.
\newblock {\em Numerische Mathematik}, 20:409--417, 1972.

\bibitem{schonhage1979power}
Arnold Sch{\"o}nhage.
\newblock On the power of random access machines.
\newblock In {\em Proc. International Colloquium on Automata, Languages, and Programming}, pages 520--529. Springer, 1979.

\bibitem{schonhage1971fast}
Arnold Sch{\"o}nhage and Volker Strassen.
\newblock Fast multiplication of large numbers.
\newblock {\em Computing}, 7:281--292, 1971.

\bibitem{shah2025hermitian}
Rikhav Shah.
\newblock Hermitian {Diagonalization in Linear P}recision.
\newblock In {\em Proc. 2025 ACM-SIAM Symposium on Discrete Algorithms}, pages 5599--5615. SIAM, 2025.

\bibitem{sobczyk2024invariant}
Aleksandros Sobczyk, Marko Mladenovic, and Mathieu Luisier.
\newblock Invariant subspaces and {PCA} in nearly matrix multiplication time.
\newblock {\em Advances in Neural Information Processing Systems}, 37:19013--19086, 2024.

\bibitem{stor2015accurate}
Nevena~Jakov{\v{c}}evi{\'c} Stor, Ivan Slapni{\v{c}}ar, and Jesse~L Barlow.
\newblock Accurate eigenvalue decomposition of real symmetric arrowhead matrices and applications.
\newblock {\em Linear Algebra and its Applications}, 464:62--89, 2015.

\bibitem{sun2001matrix}
Xiaobai Sun and Nikos~P Pitsianis.
\newblock A matrix version of the fast multipole method.
\newblock {\em SIAM Review}, 43(2):289--300, 2001.

\bibitem{swartworth2023optimal}
William Swartworth and David~P Woodruff.
\newblock Optimal eigenvalue approximation via sketching.
\newblock In {\em Proc. 55th ACM Symposium on Theory of Computing}, pages 145--155, 2023.

\bibitem{vogel2016superfast}
James Vogel, Jianlin Xia, Stephen Cauley, and Venkataramanan Balakrishnan.
\newblock Superfast divide-and-conquer method and perturbation analysis for structured eigenvalue solutions.
\newblock {\em SIAM Journal on Scientific Computing}, 38(3):A1358--A1382, 2016.

\bibitem{wilkinson1968global}
James~Hardy Wilkinson.
\newblock Global convergene of tridiagonal {QR} algorithm with origin shifts.
\newblock {\em Linear Algebra and its Applications}, 1(3):409--420, 1968.

\end{thebibliography}

\newpage

\appendix


\section{Preliminaries for symmetric arrowhead diagonalization}
\label{section:arrowhead_preliminaries}

This section contains the required preliminaries to describe the algorithm for symmetric arrowhead diagonalization. Our analysis relies on the methodology of \cite{gu1995divide}, but we refer also to \cite{o1990computing,stor2015accurate} for related techniques. We first state the following lemma which describes the desired properties of the input matrix.
\begin{lemma}
\label{lemma:arrowhead_preliminaries}
Let $\matH=\begin{pmatrix}
        \alpha & \vecz^\top \\
        \vecz & \matD
\end{pmatrix}$, $\|\matH\|\leq 1$, be an arrowhead matrix in the form of Eq. \eqref{eq:arrowhead}.
Suppose that the elements of $\matH$ satisfy the following properties, for all $i=2,\ldots,n$:
\begin{align}
    \label{eq:arrowhead_desiderata}
    d_{i+1}-d_i \geq \tau, \quad \text{and} \quad |\vecz_i|\geq \tau,
\end{align}
for some $\tau\in(0,1)$, where $d_i$ is the $i$-th element of $\matD$. The eigenvalues $\lambda_i$ are the roots of the secular equation
\begin{align}
    f(\lambda)=\lambda-\alpha+\sum_{j=2}^n \tfrac{\vecz_j^2}{d_j-\lambda},
    \label{eq:arrowhead_secular_equation}
\end{align}
and they satisfy the following interlacing property:
\begin{align}
    \lambda_1 < d_2 < \lambda_2 < \ldots < d_n < \lambda_n.
    \label{eq:arrowhead_interlacing}
\end{align}
Moreover, it also holds that
\begin{align}
    \label{eq:arrowhead_eigenvalue_boundaries_distance}
    &\min\lcurly
        \labs d_i-\lambda_i \rabs,
        \labs d_{i+1}-\lambda_i \rabs
    \rcurly
    \geq
    \frac{\tau^3}{n+1},\quad \text{for all }i=2,\ldots, n-1,\nonumber
    \\
    &\min\lcurly
        \labs d_2-\lambda_1 \rabs,
        \labs d_n-\lambda_n \rabs
    \rcurly
    \geq
    \frac{\tau^3}{n},
\end{align}
i.e., there is a well defined gap between the eigenvalues and their boundaries.
\begin{proof}\textcolor{red}{TOPROVE 13}\end{proof}
\end{lemma}


\subsection{Deflation}
\label{appendix:arrowhead_deflation}
In order to ensure that the given matrix satisfies the requirements of Equation \eqref{eq:arrowhead_desiderata} we will use deflation, specifically the methodology of section 4 in \cite{gu1995divide}.

\begin{proposition}[Arrowhead deflation]
    \label{proposition:arrowhead_deflation}
    Let $\matH=\begin{pmatrix}
        \alpha & \vecz^\top \\
        \vecz & \matD
    \end{pmatrix}$ be an arrowhead matrix in the form of Eq. \eqref{eq:arrowhead}. There exists an orthogonal matrix $\matG$ and a matrix $\matHtilde=\begin{pmatrix}
        \matHtilde' & 0 \\
        0 & \matDtilde
    \end{pmatrix}$ such that $\matDtilde$ is diagonal and $\matHtilde'=
    \begin{pmatrix}
        \widetilde\alpha' & \vecztilde'^\top \\
        \vecztilde' & \matDtilde'
    \end{pmatrix}
    $
    is an arrowhead matrix that satisfies the requirements of Equation \eqref{eq:arrowhead_desiderata}, specifically:
    \begin{align*}
    \widetilde d'_{j+1}-\widetilde d'_j \geq \tau, \quad
    |\vecztilde'_i|\geq \tau,
    \quad
    \text{and}
    \quad
    \|\matH-\matG\matHtilde\matG^\top\|\leq n\tau.    
    \end{align*}
      $\matHtilde$ can be computed in $O(n\log(n))$ arithmetic operations and comparisons, and the product $\matG^\top\matB$ for some matrix $\matB$ with $r$ columns can be computed in additional $O(nr)$ arithmetic operations on-the-fly.
    \begin{proof}\textcolor{red}{TOPROVE 14}\end{proof}
\end{proposition}


\subsection{Reconstruction from approximate eigenvalues}


If we have access to a set of approximate eigenvalues $\widehat\lambda_i$ of $\matH$ that also satisfy the same interlacing property, then we can construct a matrix $\matHhat$  that is close to $\matH$, and $\widehat\lambda_i$ are the eigenvalues of $\matHhat$. This is achieved with the following lemma from \cite{boley1977inverse}. We use its restatement from \cite{gu1995divide}.
\begin{lemma}[\cite{boley1977inverse,gu1995divide}]
    \label{lemma:arrowhead_reconstruction_from_shaft_and_eigenvalues}
    Given a set of $n$ numbers  $\widehat\lambda_1,\ldots,\widehat\lambda_n$ and a diagonal matrix $\matD=\diag(d_2,\ldots,d_n)$ such that
    \begin{align*}
        \widehat\lambda_1 < d_2 < \widehat\lambda_2 < \ldots < d_n <\widehat\lambda_n,
    \end{align*}
    there exists a symmetric arrowhead matrix $\matHhat=\begin{pmatrix}
        \widehat\alpha & \widehat\vecz^\top \\
        \widehat\vecz & \matD
    \end{pmatrix},$ whose eigenvalues are $\widehat\lambda_i$. In this case
    \begin{align}
        \label{eq:alpha_hat_and_zeta_hat}
        |\widehat\vecz_i| &= \sqrt{
            (d_i-\widehat\lambda_1)(\widehat\lambda_n-d_i)
            \prod_{j=2}^{i-1}
                \frac{\widehat\lambda_j-d_i}
                {d_j-d_i}
            \prod_{j=i}^{n-1}
                \frac{\widehat\lambda_j-d_i}
                {d_{j+1}-d_i}
        },
        \nonumber
        \\
        \widehat\alpha &= \widehat\lambda_1+\sum_{j=2}^n(\widehat\lambda_j-d_j),
    \end{align}
    where the sign of\ \  $\widehat\vecz_i$ can be chosen arbitrarily.
\end{lemma}

We shall use the spectral decomposition of $\matHhat$ as a backward approximate spectral decomposition of $\matH$. We write 
\begin{align*}
    \matH=\begin{pmatrix}
        \alpha & \vecz^\top \\
        \vecz & \matD
    \end{pmatrix},
    \quad \text{ and }\quad 
    \matHhat=\begin{pmatrix}
        \widehat\alpha & \widehat\vecz^\top \\
        \widehat\vecz & \matD
    \end{pmatrix},
\end{align*}
in which case $\|\matH-\matHhat\|\leq |\alpha-\widehat\alpha| + \|\vecz-\widehat\vecz\|$. It suffices to show that $\widehat\alpha \approx \alpha $ and $\widehat\vecz \approx \vecz$.

\begin{lemma}
    \label{lemma:h_backward_approximation}
    Let
    $\matH=\begin{pmatrix}
        \alpha & \vecz^\top \\
        \vecz & \matD
    \end{pmatrix}$
    be a symmetric arrowhead matrix with $\|\matH\|\leq 1$, satisfying the properties of Lemma \ref{lemma:arrowhead_preliminaries} with parameter $\tau$, and let $\widehat\lambda_1,\widehat\lambda_2,\ldots,\widehat\lambda_n$ be a set of approximate eigenvalues that satisfy
    \begin{align*}
        \widehat\lambda_1< d_2 < \widehat\lambda_2 < \ldots <d_n < \widehat\lambda_n, 
        \quad \text{and} \quad
        |\widehat\lambda_i -\lambda_i| \leq \epsilon\tfrac{\tau^3}{2(n+1)},
    \end{align*}
    for some $\epsilon\in(0,1/n)$. Then the quantities $\widehat\alpha$, $\veczhat$, and $\matHhat$ from Lemma \ref{lemma:arrowhead_reconstruction_from_shaft_and_eigenvalues} satisfy:
    \begin{align*}
        |\alpha-\widehat\alpha| &\leq \frac{\epsilon\tau^3}{2},\quad
        \|\vecz-\widehat\vecz\| \leq \frac{n\epsilon}{1-n\epsilon}, \quad
        \|\matH-\matHhat\| \leq \frac{\epsilon\tau^3}{2} + \frac{n\epsilon}{1-n\epsilon}.
    \end{align*}
    \begin{proof}\textcolor{red}{TOPROVE 15}\end{proof}
\end{lemma}






\section{Fast Arrowhead diagonalization}
\label{appendix:fast_arrowhead_diagonalization}
In this section we provide the full analysis of the algorithm of \cite{gu1995divide} when accelerated with the FMM.
The FMM was introduced in \cite{rokhlin1985rapid}, to accelerate the evaluation of integrals between interacting bodies, and it has been comprehensively analyzed in the literature \cite{darve2000fast,darve2000fast2,cai2020stable,sun2001matrix,martinsson2007accelerated,livne2002n,gu1993stable}. Consider a function of the form
\begin{align}
    f(x) = \sum_{j=1}^n c_j k(x-x_j),
    \label{equation:fmm_base}
\end{align}
where $x_j,c_j$ are constants and $k(x)$ is a suitably chosen kernel function, typically one of $\{\log(x), \tfrac{1}{x},\tfrac{1}{x^2}\}$. The FMM can be used to approximately evaluate $f(x)$ over $m$ different points $x_i$ in only $\widetilde O(m+n)$ arithmetic operations (suppressing logarithmic terms in the accuracy), instead of the naive $O(mn)$ evaluation. 

\subsection{Fast Multipole Method}
\label{appendix:fast_multipole_method}
For our analysis, we will need to use the FMM to evaluate the following functions \eqref{eq:fmm_1}-\eqref{eq:fmm_5}, each on $O(n)$ points. 
For each function, there is a guarantee that the evaluation points will satisfy certain criteria. More precisely, the magnitude of the denominators and the logarithm arguments  will have a well-defined lower bound. Here $\tau\in(0,1)$ is a parameter that is determined later.
\begingroup
\allowdisplaybreaks
\begin{align}
    &\text{Function:}
    &
    \text{Guaran}&\text{tees for FMM:}
    & 
    \text{  Used in:}\qquad
    \nonumber
    \\
    f(\lambda)&=\sum_{j=1}^n\frac{\vecz_j^2}{d_j-\lambda},
    &
    |d_j-\lambda|&\geq \Omega(\poly(\tfrac{\tau}{n})),
    & \text{Lemma \ref{lemma:fmm_approximate_eigenvalues}},
    \label{eq:fmm_1}
    \\
    f(d_i)&=\sum_{j=2}^{n}\log(|\widehat\lambda_j-d_i|),
    &
    |\widehat\lambda_j-d_i|&\geq \Omega(\poly(\tfrac{\tau}{n})),
    & \text{Lemma \ref{lemma:fmm_approximate_shaft}},
    \label{eq:fmm_2}
    \\
    f(d_i)&=\sum_{j=2}^{n}\log(|d_{j}-d_i|),
    &
    |d_j-d_i|&\geq \tau,
    & \text{Lemma \ref{lemma:fmm_approximate_shaft}},
    \label{eq:fmm_3}
    \\
    f(\lambda)&=\sum_{k=2}^n\frac{(1+\epsilon_k)\veczhat_k\vecq_k}{d_k-\lambda},
    &
    |d_k-\lambda|&\geq \Omega(\poly(\tfrac{\tau}{n})),
    & \text{Lemma \ref{lemma:fmm_approximate_inner_products}},
    \label{eq:fmm_4}
    \\
    f(\lambda)&=\sum_{k=2}^n\frac{(1+\epsilon_k)^2\veczhat^2_k}{(d_k-\lambda)^2},
    &
    |d_k-\lambda|&\geq \Omega(\poly(\tfrac{\tau}{n})),
    & \text{Lemma \ref{lemma:fmm_approximate_inner_products}}.
    \label{eq:fmm_5}
\end{align}
\endgroup

The fact that the magnitudes of the denonimators and the logarithm arguments are bounded from below allows us to use the seminal FMM analysis of \cite{gu1993stable,livne2002n,cai2020stable}. 
To the best of our knowledge, \cite{gu1993stable} is one of the first works to rigorously analyze the application of the FMM on such kernel functions with end-to-end approximation and complexity bounds. In Section 3.4 they achieved an error of $O\lpar
\epsilon \sum_i\tfrac{|x_i|}{|\omega^2-d_i^2|}
\rpar$
for the function $\Phi(\omega)=\sum_i\frac{x_i}{d_i^2-\omega^2}$, assuming that $d_i$ and $\omega$ satisfy similar interlacing properties to ours,
in $O(n\log^2(1/\epsilon))$ arithmetic operations. This complexity translates to $O(n\log^2(\tfrac{n}{\tau\epsilon}))$ arithmetic operations, if we rescale $\epsilon$ appropriately to obtain an absolute error $O(\epsilon)$, i.e., it satisfies the guarantees of Proposition \ref{proposition:fmm} with $\xi=2$. \cite{livne2002n} used a kernel-softening approach and report similar bounds in $O(n\log(1/\epsilon))$ operations. More recently, \cite{cai2020stable} developed a general framework based on the matrix-version of the FMM \cite{sun2001matrix}, and provided a thorough analysis and explicit bounds similar to \cite{gu1993stable}, which can be used to obtain guarantees for all the functions \eqref{eq:fmm_1}-\eqref{eq:fmm_5}. 

For completeness, we summarize the main ideas and results of the aforementioned works and provide a short proof for Proposition \ref{proposition:fmm} below.
We remind once more that we do not account for floating point errors, but rather focus only on the error FMM approximation errors. Obtaining full, end-to-end complexity analysis under floating point errors has its own merit, and it is left as future work.
\begin{proposition}[FMM]
    \label{proposition:fmm_appendix}
    There exists an algorithm, which we refer to as $(\epsilon,n)$-approximate FMM (or $(\epsilon,n)$-\fmmalgo, for short), which takes as input 
    \begin{itemize}
        \item a kernel function $k(x)=\lcurly \log(|x|), \frac{1}{x}, \frac{1}{x^2} \rcurly$,
        \item $2n+m$ real numbers: $\{x_1,\ldots,x_m\}\cup \{c_1,\ldots,c_n\}\cup\{y_1,\ldots,y_n\}$, and a constant $C$, such that $m\leq n$ and for all $i\in[m],j\in[n]$ it holds that
        \begin{align*}
            |x_i|,|c_j|,|y_j|<C
            \qquad
            \text{ and }
            \qquad
            |x_i-y_j|\geq \Omega(\poly(\tfrac{\epsilon}{n})).
        \end{align*}
    \end{itemize}
    It returns $m$ values $\widetilde f(x_1),\ldots,\widetilde f(x_m)$ such that
    $
        \labs
            \widetilde f(x_i)-f(x_i)
        \rabs
        \leq \epsilon,
    $
    for all $i\in[m]$, where $f(x) = \sum_{j=1}^n c_j k(x_i-y_j)$,
    in a total of $O\lpar 
        n\log^{\cfmm}(\tfrac{n}{\epsilon})
    \rpar$ arithmetic operations, where $\cfmm\geq 1$ is a small constant that is independent of $\epsilon,n$.
\begin{proof}\textcolor{red}{TOPROVE 16}\end{proof}
\end{proposition}













\subsection{Computing the eigenvalues with bisection}

Using the FMM as a black-box evaluation of the targeted kernel functions, we can compute all the eigenvalues of an arrowhead matrix which are given by the roots of the secular equation.
We highlight that \cite{livne2002n} provided a rigorous analysis of the Newton iteration, based on the results of \cite{melman1995numerical}, to compute all the roots of the secular equation. The reported complexity is $O(n\log(1/\epsilon))$, however, this does not include the number of Newton steps. Due to the well-known quadratic convergence properties, the number of Newton steps should be bounded by $O(\log(\log(1/\epsilon)))$. For the sake of simplicity and completeness, instead of repeating the analysis of \cite{livne2002n}, we will use the following standard bisection method to compute all the eigenvalues in a total of $O(n\log^2(1/\epsilon))$ operations. It might be slightly slower than the Newton iteration (up to a log factor), but it is significantly simpler and it does not require the evaluation of derivatives. It should therefore be both easier to implement and to analyze in finite precision. 

\begin{lemma}
    \label{lemma:fmm_approximate_eigenvalues}
    Let $\matH=\begin{pmatrix}
        \alpha & \vecz^\top\\
        \vecz & \matD
    \end{pmatrix}
    $
    be a symmetric arrowhead matrix 
    that satisfies the properties of Lemma \ref{lemma:arrowhead_preliminaries} for some parameter $\tau\in(0,1)$,
    with $\|\matH\|\leq 1$, 
    and assume that the diagonal elements of $\matD$ are sorted:
    $
        d_2 < d_3 <\ldots < d_n.
    $ 
    Given $\epsilon\in(0,1)$, and assuming an $(\epsilon,n)$-\fmmalgo\   implementation as in Proposition \ref{proposition:fmm}, we can compute approximate eigenvalues $\widehat\lambda_i$ such that
    \begin{align*}
        \widehat\lambda_1< d_2 < \widehat\lambda_2 < \ldots <d_n < \widehat\lambda_n, 
        \quad \text{and} \quad
        |\widehat\lambda_i -\lambda_i| \leq \epsilon,
    \end{align*}
    in $O\lpar
        n\log(\tfrac{1}{\epsilon})\log^{\cfmm}(\frac{n}{\tau\epsilon})
    \rpar$ arithmetic operations.
    \begin{proof}\textcolor{red}{TOPROVE 17}\end{proof}
\end{lemma}


\subsection{Approximating the elements of the shaft}
As a next step, we use the trick of \cite{gu1995divide} to approximate the elements of $\matHhat$ from Lemma \ref{lemma:h_backward_approximation}. 
\begin{lemma}
    \label{lemma:fmm_approximate_shaft}
    Let $\matH=\begin{pmatrix}
        \alpha & \vecz^\top\\
        \vecz & \matD
    \end{pmatrix}
    $
    be a symmetric arrowhead matrix with $\|\matH\|\leq 1$, that satisfies the requirements of Lemma \ref{lemma:arrowhead_preliminaries} with parameter $\tau$, and let $d_2 < \ldots < d_n$ be the diagonal elements of  $\matD$. From Lemma \ref{lemma:h_backward_approximation}, if we are given a set of approximate eigenvalues $\widehat\lambda_i$ that satisfy 
    \begin{align*}
        \widehat\lambda_1< d_2 < \widehat\lambda_2 < \ldots <d_n < \widehat\lambda_n, 
        \quad \text{and} \quad
        |\widehat\lambda_i -\lambda_i| \leq \epsilon\tfrac{\tau^3}{2(n+1)},
    \end{align*}
    for some $\epsilon\in(0,1/n)$, then there exists a matrix $\matHhat=\begin{pmatrix}
        \widehat\alpha & \widehat\vecz^\top \\
        \widehat\vecz & \matD
    \end{pmatrix},$ such that $\widehat\lambda_i$ are the exact eigenvalues of $\matHhat$ and 
    $\|\matH-\matHhat\| 
    \leq 
    \frac{\epsilon\tau^3}{2} + \frac{n\epsilon}{1-n\epsilon}
    $.
    Assuming an $(\epsilon,n)$-\fmmalgo, for any $\epsilon_{\vecz}\in(0,1/2)$, we can compute an approximate
    vector $\widehat\vecz'$ such that $|\widehat\vecz_i-\widehat\vecz_i'|\leq \epsilon_{\vecz}|\widehat\vecz_i|$ and  $\|\widehat\vecz'-\widehat\vecz\|\leq \epsilon_{\vecz} (1 + \frac{n\epsilon}{1-n\epsilon})$ in $
        O\lpar
            n\log(\tfrac{1}{\epsilon_{\vecz}})
            \log^{\cfmm}(\tfrac{n}{\tau\epsilon_{\vecz}}))
        \rpar
    $ arithmetic operations. The matrix $\matHhat'=\begin{pmatrix}
        \widehat\alpha & \widehat\vecz'^\top \\
        \widehat\vecz' & \matD
    \end{pmatrix}$ satisfies
    \begin{align*}
        \|\matH-\matHhat'\| \leq 
            \frac{\epsilon\tau^3}{2} 
            +
            \epsilon_{\vecz} + (1+\epsilon_{\vecz})\frac{n\epsilon}{1-n\epsilon}.
    \end{align*}
    \begin{proof}\textcolor{red}{TOPROVE 18}\end{proof}
\end{lemma}

\subsection{Approximating inner products with the eigenvectors}
In the last part of the analysis we provide bounds on the errors for inner products of the form $\vecuhat^\top_i\vecq$, between the $i$-th eigenvector and some arbitrary vector $\vecq$. Following the procedure of Section 5 in \cite{gu1995divide}, we write
\begin{align*}
    \vecuhat^\top_i\vecq = \frac{-\vecq_1+\Phi(\widehat\lambda_i)}{\sqrt{1+\Psi(\widehat\lambda_i)}},
\end{align*}
where $\Phi(\lambda)=\sum_{k=2}^n\frac{\veczhat_k\vecq_k}{d_k-\lambda}$ and 
$\Psi(\lambda)=
    \sum_{k=2}^n
        \frac{\veczhat_k^2}{(d_k-\lambda)^2}.
$
To compute all the products for all $i$ with $\vecq$, we can use FMM to approximate $\Phi(\lambda)$ and $\Psi(\lambda)$ on $n$ points. 


\begin{lemma}
    \label{lemma:fmm_approximate_inner_products}
        Let $\matH=\begin{pmatrix}
            \alpha & \vecz^\top\\
            \vecz & \matD
        \end{pmatrix}
        $
        be a symmetric arrowhead matrix with $\|\matH\|\leq 1$, that satisfies the requirements of Lemma \ref{lemma:arrowhead_preliminaries} with parameter $\tau$, and let $d_2 < d_3 < \ldots < d_n$ be the diagonal elements of  $\matD$. Let $\epsilon_{\vecz}$, $\widehat\lambda_i$, $\veczhat$, $\veczhat'$, $\matHhat$, and $\matHhat'$ be the same as in Lemma \ref{lemma:fmm_approximate_shaft}. 
        Let $\vecq$ be a fixed vector with $\|\vecq\|\leq 1$, and $\vecuhat_i, i\in[n]$, be the eigenvectors of $\matHhat$.
        Assuming an $(\epsilon,n)$-\fmmalgo, we can approximate all the inner products 
        $\vecuhat_i^\top\vecq$,  by some values $x_i$ such that 
        \begin{align*}
            |\vecuhat_i^\top\vecq - x_i| \leq 147\epsilon_{\vecz}
                \tfrac{(n+1)^2}{\tau^6},
        \end{align*}
        for all $i\in[n]$ simultaneously, in $O\lpar
            n\log(\tfrac{1}{\epsilon_{\vecz}})
            \log^{\cfmm}(\tfrac{n}{\tau\epsilon_{\vecz}}))
        \rpar$ arithmetic operations.
    \begin{proof}\textcolor{red}{TOPROVE 19}\end{proof}
\end{lemma}


\subsection{Proof of Theorem \ref{theorem:arrowhead_diagonalization}}
We can finally combine all the results to prove Theorem \ref{theorem:arrowhead_diagonalization}, which we restate below for readability.
\begin{theorem}
    \label{theorem:arrowhead_diagonalization_appendix}
    Given a symmetric arrowhead matrix $\matH\in\mathbb{R}^{n\times n}$ as in Eq. \eqref{eq:arrowhead}, with $\|\matH\|\leq 1$, an accuracy parameter $\epsilon\in(0,1)$, a matrix $\matB$ with $r$ columns $\matB_i,i\in[r]$, where $\|\matB_i\|\leq 1$, and an $(\epsilon,n)$-\fmmalgo\   implementation (see Prop. \ref{proposition:fmm}), we can compute a diagonal matrix $\matLambdatilde$, and a matrix
    $\matQtilde_{\matB}$, such that 
    \begin{align*}
        \lnorm \matH-\matQ\matLambdatilde\matQ^\top \rnorm &\leq \epsilon,
        \quad
        \labs \lpar \matQ^\top\matB - \matQtilde_{\matB} \rpar_{i,j}\rabs \leq  \epsilon/n^2,
    \end{align*}
    where 
    $
        \matQ\in\mathbb{R}^{n\times n}
    $ is (exactly) orthogonal, in 
    $
        O\lpar nr\log^{\cfmm+1}(\tfrac{n}{\epsilon})\rpar
    $
    arithmetic operations and comparisons.

    Alternatively, if only want to compute a set of approximate values $\widetilde\lambda_1,\ldots,\widetilde\lambda_n$, such that $|\lambda_i(\matH)-\widetilde\lambda_i|\leq \epsilon$, the complexity reduces to $O\lpar n\log(\frac{1}{\epsilon})\log^{\cfmm}(\frac{n}{\epsilon})\rpar$ arithmetic operations.
    \begin{proof}\textcolor{red}{TOPROVE 20}\end{proof}
\end{theorem}


\section{Tridiagonal diagonalization}
\label{appendix:tridiagonal_diagonalization}
\subsection{Omitted proofs}
The next lemma bounds the error of the reduction to arrowhead form when the spectral factorizations of the matrices $\matT_1$ and $\matT_2$ in Equation \eqref{eq:tridiagonal_to_arrowhead} are approximate rather than exact.
\begin{lemma}[Restatement of Lemma \ref{lemma:tridiagonal_assembly}]
\label{lemma:tridiagonal_assembly_appendix}
Let $\epsilon\in(0,1/2)$ be a given accuracy parameter and $\matT = \begin{pmatrix}
    \matT_1 & \beta_{k+1}\vece_k & \\
    \beta_{k+1}\vece_k^\top & \alpha_{k+1} & \beta_{k+2}\vece_1^\top \\
     & \beta_{k+2}\vece_1 & \matT_2
\end{pmatrix}$ be a tridiagonal matrix with size  $n\geq 3$ and $\|\matT\|\leq 1$, where $\matT_1=\matU_1\matD_1\matU_1^\top$ and $\matT_2=\matU_2\matD_2\matU_2^\top$ be the exact spectral factorizations of $\matT_1$ and $\matT_2$. Let $\matUtilde_1,\matDtilde_1,\matUtilde_2,\matDtilde_2$ be approximate spectral factorizations of $\matT_1,\matT_2$. If these factors satisfy
    \begin{align*}
        \lnorm
            \matT_{\{1,2\}} - \matUtilde_{\{1,2\}}\matDtilde_{\{1,2\}}\matUtilde_{\{1,2\}}^\top
        \rnorm 
        \leq \epsilon_1,
        \quad
        \lnorm \matUtilde_{\{1,2\}}\matUtilde_{\{1,2\}}^\top -\matI \rnorm &\leq \epsilon_1/n,
    \end{align*}
    for some $\epsilon_1\in(0,1/2)$, where $\matDtilde_{\{1,2\}}$ are both diagonal, then, assuming an $(\epsilon,n)$-\fmmalgo\   implementation as in Prop. \ref{proposition:fmm}, we can compute a diagonal matrix $\matDtilde$ and an approximately orthogonal matrix $\matUtilde$ such that
    \begin{align*}
        \lnorm\matUtilde^\top\matUtilde-\matI\rnorm\leq 3(\epsilon_1+\epsilon)/n,
        \quad \text{and} \quad
        \lnorm 
            \matT-\matUtilde\matDtilde\matUtilde^\top
        \rnorm \leq 2\epsilon_1+7\epsilon,
    \end{align*}
    in a total of $O\lpar n^2\log^{\cfmm+1}(\tfrac{n}{\epsilon})\rpar$ arithmetic operations and comparisons.
    \begin{proof}\textcolor{red}{TOPROVE 21}\end{proof}
\end{lemma}


\begin{lemma}
    \label{lemma:tridiagonal_assembly_eigenvalues_only}
Let $\epsilon\in(0,1/2)$ be a given accuracy parameter and $\matT = \begin{pmatrix}
    \matT_1 & \beta_{k+1}\vece_k & \\
    \beta_{k+1}\vece_k^\top & \alpha_{k+1} & \beta_{k+2}\vece_1^\top \\
     & \beta_{k+2}\vece_1 & \matT_2
\end{pmatrix}$ be a tridiagonal matrix with size $n\geq 2$, where $\matT_1=\matU_1\matD_1\matU_1^\top$ and $\matT_2=\matU_2\matD_2\matU_2^\top$ be the exact spectral factorizations of $\matT_1$ and $\matT_2$. Let $\matUtilde_1,\matDtilde_1,\matUtilde_2,\matDtilde_2$ be approximate spectral factorizations of $\matT_1,\matT_2$. Assume that these factors satisfy
    \begin{align*}
        \lnorm
            \matT_{\{1,2\}} - \matUtilde_{\{1,2\}}\matDtilde_{\{1,2\}}\matUtilde_{\{1,2\}}^\top
        \rnorm 
        \leq \epsilon_1,
        \quad
        \lnorm \matUtilde_{\{1,2\}}\matUtilde_{\{1,2\}}^\top -\matI \rnorm &\leq \epsilon_1/n,
    \end{align*}
    for some $\epsilon_1\in(0,1/2)$, where $\matDtilde_{\{1,2\}}$ are both diagonal. Assume also that $\matDtilde_1,\matDtilde_2$, as well as the last row $\vecltilde_1^\top$ of $\matUtilde_1$, and the first row $\vecftilde_2^\top$ of $\matUtilde_2$, are explicitly available. 
    
    Then we can compute a diagonal matrix $\matDtilde$, as well as the first row $\vecltilde$ and/or the last row $\vecftilde$ of an approximately orthogonal matrix $\matUtilde$, which satisfy
    \begin{align*}
        \lnorm\matUtilde^\top\matUtilde-\matI\rnorm\leq 3(\epsilon_1+\epsilon)/n,
        \quad \text{and} \quad
        \lnorm 
            \matT-\matUtilde\matDtilde\matUtilde^\top
        \rnorm \leq 2\epsilon_1+7\epsilon,
    \end{align*}
    in a total of $O\lpar n\log^{\cfmm+1}(\tfrac{n}{\epsilon})\rpar$ arithmetic operations.
    \begin{proof}\textcolor{red}{TOPROVE 22}\end{proof}
\end{lemma}

\subsection{Approximating only the eigenvalues of a tridiagonal matrix}
The following result gives the complexity of approximating only the eigenvalues of $\matT$, instead of a full diagonalization. 
The main observation is that Lemma \ref{lemma:tridiagonal_assembly} can be simplified if we only need the eigenvalues. This simplification is listed in \ref{lemma:tridiagonal_assembly_eigenvalues_only} in the Appendix.
Using this as an inductive step, we obtain the following Corollary.
\begin{corollary}
    \label{corollary:tridiagonal_eigenvalues}
    Let $\matT$ be a symmetric unreduced tridiagonal matrix with $\|\matT\|\leq 1$ and $\epsilon\in(0,1/2)$ be a given accuracy parameter. Assuming access to an $(\epsilon,n)$-\fmmalgo, for $\tau\in\Theta(\poly(\tfrac{\epsilon}{n}))$, we can compute approximate eigenvalues $\widetilde\lambda_1,\ldots,\widetilde\lambda_n$ such that
    $|\widetilde\lambda_i-\lambda_i(\matT)|\leq \epsilon$ in $O\lpar n\log^{\cfmm+2}(\tfrac{n}{\epsilon})\rpar$ arithmetic operations.
    \begin{proof}\textcolor{red}{TOPROVE 23}\end{proof}
\end{corollary}

\subsection{Application: Hermitian diagonalization}
\label{appendix:hermitian_diagonalization_analysis}
Theorem \ref{theorem:alg_recursive_diagonalization} has a direct application to diagonalize Hermitian matrices. The following result is immediate.
\begin{corollary}
    \label{corollary:hermitian_diagonalization_appendix}
    Let $\matA$ be a Hermitian matrix of size $n$ with $\|\matA\|\leq 1$. Given accuracy $\epsilon\in(0,1/2)$, and an $(\epsilon,n)$-\fmmalgo\   implementation of Prop. \ref{proposition:fmm}, we can compute a matrix $\matQtilde$ and a diagonal matrix $\matLambdatilde$ such that
    \begin{align*}
        \lnorm \matA - \matQtilde\matLambdatilde\matQtilde^* \rnorm \leq \epsilon,
        \quad
        \lnorm \matQtilde^* \matQtilde - \matI \rnorm \leq \epsilon/n^2,
    \end{align*}
    or, stated alternatively,
    \begin{align*}
        \matQtilde=\matQ+\matE_{\matQ},
        \quad
        \matQ^\top\matQ=\matI,
        \quad
        \lnorm \matE_{\matQ} \rnorm \leq \epsilon/n^2,
        \quad
        \lnorm \matA - \matQ\matLambdatilde\matQ^\top \rnorm \leq \epsilon.
    \end{align*}
    The algorithm requires a total of $O\lpar n^\omega\log(n) + n^2\log^{\cfmm+1}(\tfrac{n}{\epsilon})\rpar$ arithmetic operations and comparisons.
    \begin{proof}\textcolor{red}{TOPROVE 24}\end{proof}
\end{corollary}

\subsection{Singular Value Decomposition}
\label{appendix:svd_analysis}

In this section we provide the proof of Theorem \ref{theorem:svd}. 
\begin{proof}\textcolor{red}{TOPROVE 25}\end{proof}







\section{Floating point arithmetic}
\label{appendix:floating_point_arithmetic}

The sign $s$ is $+$ if the corresponding bit is one, and $-$ if the bit is zero. The exponent $e$ is stored as a binary number in the so-called \textit{biased form}, and its range is $e\in[-M,M]$, where $M=2^{p-1}$. The significand $m$ is an integer that satisfies $2^{t-1}\leq m \leq 2^t-1$, where the lower bound is enforced to ensure that the system is \textit{normalized}, i.e. the first bit of $m$ is always $1$.
We can therefore write $\fl(\alpha)$ in a more intuitive representation
\begin{align*}
    \fl(\alpha) = \pm 2^{e}\times \lpar \tfrac{m_1}{2} + \tfrac{m_2}{2^2} + \ldots + \tfrac{m_t}{2^t} \rpar,
\end{align*}
where the first bit $m_1$ of $m$ is always equal to one for normalized numbers. The range of normalized numbers is therefore $[2^{-M},2^{M}(2-2^{-t})]$. Numbers that are smaller than $2^{-M}$ are called \textit{subnormal} and they will be ignored for simplicity, since we can either add more bits in the exponent. Similarly, numbers that are larger than $2^{M}(2-2^{-t})$ are assumed to be numerically equal to infinity, denoted by $\FLINF$. 

From \cite[Theorem 2.2]{higham2002accuracy},
for all real numbers $\alpha$ in the normalized range it holds that \begin{align*}
    \fl(\alpha) = (1+\theta)\alpha,
\end{align*}
where $\theta\in\mathbb{R}$ satisfies $|\theta|\leq 2^{-t}:=\umach$, where $\umach$ is the \textit{machine precision}. Clearly, $t=O(\log(1/\umach))$, in which case we can always obtain a bound for the number of required bits of a numerical algorithm if we have an upper bound for the precision $\umach$. We will write the same for complex numbers which are represented as a pair of normalized floating point numbers.

The floating point implementation of each arithmetic operation $\odot \in\{+,-,\times,/\}$ also satisfies
\begin{align}
    \fl(\alpha\odot\beta) = (1+\theta)(\alpha\odot\beta),\quad |\theta|\leq\umach.
\end{align}
Divisions and multiplications with $1$ and $2$ do not introduce errors (for the latter we simply increase/decrease the exponent).
We assume that we also have an implementation of $\sqrt{\cdot}$ such that $\fl(\sqrt{\alpha})=(1+\theta)\sqrt{\alpha}$ where $|\theta|\leq \umach$.
From \cite[Lemma 3.1]{higham2002accuracy}, we can bound products of errors as
\begin{align*}
    \prod_{i=1}^n (1+\theta_i)^{\rho_i} = 1+\eta_n,
\end{align*}
where $\rho_i=\pm 1$ and $|\eta_n|\leq \tfrac{n\umach}{1-n\umach}$.

The above can be extended also for complex arithmetic (see \cite[Lemma 3.5]{higham2002accuracy}), where the bound becomes $|\theta|\leq O(\umach)$, but we will ignore the constant prefactor for simplicity. 

Operations on matrices can be analyzed in a similar manner. Let $\otimes$ denote the element-wise multiplication between two matrices and $\oslash$ the element-wise division. The floating point representation of a matrix $\matA$ satisfies
\begin{align*}
    \fl(\matA) = \matA+\matDelta \otimes \matA,\quad |\matDelta_{i,j}|\leq \umach.
\end{align*}
It can be shown that $\|\matDelta\|\leq \umach\sqrt{n}\|\matA\|$. 
For any operation $\odot\in\{+,-,\otimes,\oslash\}$ and matrices $\matA$ and $\matB$ it holds that
\begin{align}
    \label{eq:matrix_flop_errors}
    \fl(\matA\odot \matB) = \matA \odot \matB +\matDelta\otimes(\matA \odot \matB), 
    \quad 
    |\matDelta_{i,j}|\leq \umach, 
    \quad 
    \|\matDelta\otimes(\matA \odot \matB)\|\leq \umach\sqrt{n}\|\matA\odot\matB\|.
\end{align}




\section{Reduction to tridiagonal form - omitted proofs and definitions}
\label{appendix:tridiagonal_reduction}

\subsection{Imported subroutines}
\label{appendix:tridiagonal_reduction_subroutines}
In this appendix we mention some preliminary results that we use in the analysis.
\begin{theorem}[$ \MM$, stable fast matrix multiplication \cite{demmel2007fastmm,demmel2007fastla}]
\label{theorem:fast_mm}
For every $\eta>0$, there exists a fast matrix multiplication algorithm $ \MM$ which takes as input two matrices $\matA,\matB\in\mathbb{C}^{n\times n}$ and returns $\matC\leftarrow  \MM(\matA,\matB)$ such that
\begin{align*}
    \|\matC-\matA\matB\| \leq n^{c_{\eta}}\cdot\umach\|\matA\|\|\matB\|,
\end{align*}
on floating point machine with precision $\umach$, for some constant $c_{\eta}$ independent of $n$. It requires $O(n^{\omega+\eta})$ floating point operations.
\end{theorem}
We can also assume that if the result $\matA\matB$ is Hermitian, then $\MM(\matA,\matB)$ will be Hermitian as well (see e.g. \cite{sobczyk2024invariant}).

\begin{theorem}[Cf. \cite{demmel2007fastla}]
\label{theorem:alg_qr}
Given a matrix $\matA\in\mathbb{C}^{m\times n}$, $m\geq n$, there exists an algorithm $[\matQ,\matR]\leftarrow\QR(\matA)$ which returns an upper triangular matrix $\matR\in\mathbb{C}^{m\times n}$ and a matrix $\matQ\in\mathbb{C}^{m\times m}$, such that:
\begin{align*}
    (\matQ+\matE_{\matQ})^*(\matA+\matE_{\matA}) = \matR, 
    \quad
    \|\matE_{\matQ}\|\leq n^{c_{\QR}}\umach,
    \quad
    \|\matE_{\matA}\|\leq n^{c_{\QR}}\umach\|\matA\|,
\end{align*}
for some constant $c_{\QR}$, where the matrix $\matQ+\matE_{\matQ}$ is unitary. The algorithm executes $O(mn^{\omega-1})$ floating point operations on a machine with precision $\umach$.
\end{theorem}
In our analysis it will be useful the following re-statement of the aforementioned results.
\begin{corollary}
\label{corollary:alg_qr}
There exists a global constant $\cmm\geq \max\{c_{\QR},c_{\eta},1\}$, such that the matrices from Theorems \ref{theorem:fast_mm} and \ref{theorem:alg_qr} satisfy the following properties
    \begin{align*}
        \lnorm  \MM(\matA,\matB)-\matA\matB \rnorm &\leq n^{\cmm} \umach \|\matA\|\|\matB\|,
        \\
        \lnorm \matA - \matQ\matR \rnorm &\leq  n^{\cmm}\umach\|\matA\|, 
        \\
        \|\matR\|&\leq (1+n^{\cmm}\umach)\|\matA\|,
        \\
        \|\matQ\|&\leq \sqrt{1+n^{\cmm}\umach},
        \\
        \|\matQ^{-1}\|&\leq \frac{1}{\sqrt{1-n^{\cmm}\umach}},
        \\
        \max\{
            \|\matI-\matQ\matQ^*\|,\|\matI-\matQ^*\matQ\|
        \}
        &\leq
        n^{\cmm}\umach.
    \end{align*}
\begin{proof}\textcolor{red}{TOPROVE 26}\end{proof}
\end{corollary}

Using square fast matrix multiplication we can obtain an algorithm to compute two banded matrices with the same bandwidth
\begin{corollary}
    \label{corollary:block_tridiagonal_mm}
    Let $\matA$,$\matB$ in $\mathbb{C}^{n\times n}$, where, without loss of generality, $n$ is a power of two. Assume that $\matA$ and $\matB$ are block-tridiagonal, with block size $n_k=2^k$ for some $k\in \{0,1,\ldots,\log(n)-2\}$. We can compute a matrix $\matC'$ such that $\|\matC'-\matA\matB\|\leq \umach n_k^{\beta}\|\matA\|\|\matB\|$ in $O(nn_k^{\omega-1})$ floating point operations.
    \begin{proof}\textcolor{red}{TOPROVE 27}\end{proof}
\end{corollary}

\subsection{Rotations}
An example of the rotations is illustrated in Equations \eqref{eq:rotation_r_i} and \eqref{eq:rotation_r_i_prime}. In Equation \eqref{eq:rotation_r_i}, the matrices $\matA'_{1,2}$ and $\matA'_{2,1}$ are lower and upper triangular, respectively. In Equation \eqref{eq:rotation_r_i_prime}, the matrices $\matA'_{4,2}$ and $\matA'_{5,3}$ are upper triangular, while $\matA'_{2,4}$ and $\matA'_{3,5}$ are lower triangular. 
\begingroup
\setlength\arraycolsep{1.5pt}
\begin{align}
    \label{eq:rotation_r_i}
    \overbrace{
    \begin{pmatrix}
        \matA_{1,1}  & \matA_{1,2}  & \boxed{\matA_{1,3}}   & 0             & 0           & 0           & 0           & 0           \\
        \matA_{2,1}  & \matA_{2,2}  &\matA_{2,3}   & \matA_{2,4}   & 0           & 0           & 0           & 0           \\
        \boxed{\matA_{3,1}}  & \matA_{3,2}  &\matA_{3,3}   & \matA_{3,4}   & \matA_{3,5} & 0           & 0           & 0           \\
        0            & \matA_{4,2}  &\matA_{4,3}   & \matA_{4,4}   & \matA_{4,5} & \matA_{4,6} & 0           & 0           \\
        0            & 0            &\matA_{5,3}   & \matA_{5,4}   & \matA_{5,5} & \matA_{5,6} & \matA_{5,7} & 0           \\
        0            & 0            & 0            & \matA_{6,4}   & \matA_{6,5} & \matA_{6,6} & \matA_{6,7} & \matA_{6,8} \\
        0            & 0            & 0            & 0             & \matA_{7,5} & \matA_{7,6} & \matA_{7,7} & \matA_{7,8} \\
        0            & 0            & 0            & 0             & 0           & \matA_{8,6} & \matA_{8,7} & \matA_{8,8} \\
    \end{pmatrix}
    }^{
        \matA^{(k,2,0)},\ i=2
    }
    \stackrel{R_2}{\longrightarrow}
    \overbrace{
    \begin{pmatrix}
        \matA_{1,1}  & \matA'_{1,2}  & 0   & 0             & 0           & 0           & 0           & 0           \\
        \matA'_{2,1}  & \matA_{2,2}  &\matA_{2,3}   & \matA_{2,4}   & \boxed{\matA_{2,5}}           & 0           & 0           & 0           \\
        0  & \matA_{3,2}  &\matA_{3,3}   & \matA_{3,4}   & \matA_{3,5} & 0           & 0           & 0           \\
        0            & \matA_{4,2}  &\matA_{4,3}   & \matA_{4,4}   & \matA_{4,5} & \matA_{4,6} & 0           & 0           \\
        0            & \boxed{\matA_{5,2}} &\matA_{5,3}   & \matA_{5,4}   & \matA_{5,5} & \matA_{5,6} & \matA_{5,7} & 0           \\
        0            & 0            & 0            & \matA_{6,4}   & \matA_{6,5} & \matA_{6,6} & \matA_{6,7} & \matA_{6,8} \\
        0            & 0            & 0            & 0             & \matA_{7,5} & \matA_{7,6} & \matA_{7,7} & \matA_{7,8} \\
        0            & 0            & 0            & 0             & 0           & \matA_{8,6} & \matA_{8,7} & \matA_{8,8} \\
    \end{pmatrix}
    }^{
        \matA^{(k,3,5)}
    }
\end{align}
\endgroup

\begingroup
\setlength\arraycolsep{1.5pt}
\begin{align}
    \label{eq:rotation_r_i_prime}
    \overbrace{
    \begin{pmatrix}
        \matA_{1,1}  & \matA'_{1,2}  & 0   & 0             & 0           & 0           & 0           & 0           \\
        \matA'_{2,1}  & \matA_{2,2}  &\matA_{2,3}   & \matA_{2,4}   & \boxed{\matA_{2,5}}           & 0           & 0           & 0           \\
        0  & \matA_{3,2}  &\matA_{3,3}   & \matA_{3,4}   & \matA_{3,5} & 0           & 0           & 0           \\
        0            & \matA_{4,2}  &\matA_{4,3}   & \matA_{4,4}   & \matA_{4,5} & \matA_{4,6} & 0           & 0           \\
        0            & \boxed{\matA_{5,2}} &\matA_{5,3}   & \matA_{5,4}   & \matA_{5,5} & \matA_{5,6} & \matA_{5,7} & 0           \\
        0            & 0            & 0            & \matA_{6,4}   & \matA_{6,5} & \matA_{6,6} & \matA_{6,7} & \matA_{6,8} \\
        0            & 0            & 0            & 0             & \matA_{7,5} & \matA_{7,6} & \matA_{7,7} & \matA_{7,8} \\
        0            & 0            & 0            & 0             & 0           & \matA_{8,6} & \matA_{8,7} & \matA_{8,8} \\
    \end{pmatrix}
    }^{
        \matA^{(k,3,5)}
    }
    \stackrel{R'_4}{\longrightarrow}
    \overbrace{
        \begin{pmatrix}
            \matA_{1,1}  & \matA'_{1,2} & 0            & 0                   & 0           & 0           & 0           & 0           \\
            \matA'_{2,1} & \matA_{2,2}  &\matA_{2,3}   & \matA'_{2,4}        & 0           & 0           & 0           & 0           \\
            0            & \matA_{3,2}  &\matA_{3,3}   & \matA_{3,4}         & \matA'_{3,5} & 0           & 0           & 0           \\
            0            & \matA'_{4,2} &\matA_{4,3}   & \matA_{4,4}         & \matA_{4,5} & \matA_{4,6} & \boxed{\matA_{4,7}} & 0           \\
            0            & 0            &\matA'_{5,3}  & \matA_{5,4}         & \matA_{5,5} & \matA_{5,6} & \matA_{5,7} & 0           \\
            0            & 0            & 0            & \matA_{6,4}         & \matA_{6,5} & \matA_{6,6} & \matA_{6,7} & \matA_{6,8} \\
            0            & 0            & 0            & \boxed{\matA_{4,7}} & \matA_{7,5} & \matA_{7,6} & \matA_{7,7} & \matA_{7,8} \\
            0            & 0            & 0            & 0                   & 0           & \matA_{8,6} & \matA_{8,7} & \matA_{8,8} \\
        \end{pmatrix}
    }^{
        \matA^{(k,3,7)}
    }
\end{align}
\endgroup

The rotations can be conveniently implemented in floating point using fast QR factorizations.

\begin{lemma}
    \label{lemma:rotation_r_i_floating_point}
    Let $\matA^{(k,i,0)}$ be a block-pentadiagonal matrix with no bulge of the form of Equation \eqref{eq:block_pentadiagonal}, consisting of $b_k\times b_k$ blocks of size $n_k\times n_k=2^{k}\times 2^{k}$ each. The rotation $[\matA^{(k,i+1,i+3)},\matQ] \leftarrow R_i(\matA^{(k,i,0)})$ can be implemented in $O(n_k^{\omega})$ floating point operations. The resulting matrix $\matA^{(k,i+1,i+3)}$ has a bulge at positions $(i,i+3)$ and $(i+3,i)$, and $\matQ$ is approximately unitary. Moreover, if $\umach\leq 1/n^{\cmm}$, then
    \begin{align*}
        \lnorm 
            \matA^{(k,i,0)} - \matQ\matA^{(k,i+1,i+3)} \matQ^*
        \rnorm
        \leq
        C\umach n^{\cmm}\|\matA^{(k,i,0)}\|,
    \end{align*}
    where $C$ is a constant independent of $n$.
    \begin{proof}\textcolor{red}{TOPROVE 28}\end{proof}
\end{lemma}


\begin{lemma}
    \label{lemma:rotation_r_i_prime_floating_point}
    Let $\matA^{(k,s,j+1)}$, $j\geq s+1$, be a block-pentadiagonal matrix of the form of Equation \eqref{eq:block_pentadiagonal}, with a bulge at position $(j+1,j-2)$, consisting of $b_k\times b_k$ blocks of size $n_k\times n_k=2^{k}\times 2^{k}$ each. The rotation $[\matA^{(k,s,j+3)},\matQ] \leftarrow R_j'(\matA^{(k,s,j+1)})$ can be implemented in $O(n_k^{\omega})$ floating point operations. The resulting matrix $\matA^{(k,s,j+3)}$ has a bulge at positions $(j+3,j)$ and $(j,j+3)$, and $\matQ$ is approximately unitary. Moreover, if $\umach\leq 1/n^{\cmm}$, then
    \begin{align*}
        \lnorm 
            \matA^{(k,s,j+1)} - \matQ\matA^{(k,s,j+3)} \matQ^*
        \rnorm
        \leq
        C'\umach (2n_k)^{\cmm}\|\matA^{(k,s,j+1)}\|,
    \end{align*}
    where $C'$ is a constant independent of $n$.
    \begin{proof}\textcolor{red}{TOPROVE 29}\end{proof}
\end{lemma}


\subsection{Bandwidth halving}





\begin{lemma}
    \label{lemma:bandwidth_halving_floating_point_appendix}
    Let $\matA^{(k,2,0)}$ be a full block-pentadiagonal matrix of the form of Equation \eqref{eq:block_pentadiagonal}, with no bulge, consisting of $b_k\times b_k$ blocks of size $n_k\times n_k=2^{k}\times 2^{k}$ each. For any $k\in [\log(n)-2]$, if $\umach\leq \tfrac{1}{C_1n^{\cmm+3}}$, where $C_1$ is a constant, then Algorithm \ref{algorithm:halve} returns a matrix $\matQhat^{(k)}$ that is approximately orthogonal, and a matrix $\matA^{(k-1,2,0)}$ such that
    \begin{align*}
        \lnorm 
            \matA^{(k,2,0)} - \matQhat^{(k)}\matA^{(k-1,2,0)} \matQhat^{(k)*}
        \rnorm
        &\leq
        C_2\umach n^{\cmm+3}\|\matA^{(k,2,0)}\|,
        \\
        \lnorm 
            \matQhat^{(k)}\matQhat^{(k)*} - \matI
        \rnorm
        &\leq
        C_3\umach n^{\cmm+3}
    \end{align*}
    where $C_2,C_3$ are also constants independent of $n$. It requires 
    $O\lpar
        n^2
        \lpar 
            S_{\omega}(\log(n)-1)
            -
            S_{\omega}(k)
        \rpar
    \rpar$
    floating point operations, where $S_{x}(m):=\sum_{l=1}^{m} \lpar 2^{x-2} \rpar^{l}$. If $\matQhat^{(k)}$ is not required, the complexity reduces to 
    $O(n^2n_k^{\omega-2})=O(n^2(2^{\omega-2})^k)$.
    \begin{proof}\textcolor{red}{TOPROVE 30}\end{proof}
\end{lemma}


\begin{theorem}[Restatement of Theorem \ref{theorem:stable_tridiagonal_reduction}]
    \label{theorem:stable_tridiagonal_reduction_appendix}
    There exists a floating point implementation of the tridiagonal reduction algorithm of \cite{schonhage1972unitare}, which takes as input a Hermitian matrix $\matA$, and returns a tridiagonal matrix $\matTtilde$, and (optionally) an approximately unitary matrix $\matQtilde$. If the machine precision $\umach$ satisfies
    $
        \umach \leq \epsilon\frac{1}{cn^{\beta+4}},
    $
    where $\epsilon\in(0,1)$, $c$ is a constant, and $\cmm$ is the same as in Corollary \ref{corollary:alg_qr}, which translates to $O(\log(n)+\log(1/\epsilon))$ bits of precision, then the following hold:
    \begin{align*}
        \lnorm \matQtilde\matQtilde^*-\matI\rnorm \leq \epsilon,
        \quad
        \text{and}
        \quad
        \lnorm \matA - \matQtilde\matTtilde\matQtilde^* \rnorm \leq \epsilon \lnorm \matA \rnorm.
    \end{align*}
    The algorithm executes at most $O\lpar
                    n^2 S_{\omega}(\log(n))
    \rpar$ floating point operations to return only $\matTtilde$, where $S_x(m)=\sum_{l=1}^m (2^{x-2})^l$.
    If $\matA$ is banded with $1\leq d\leq n$ bands, the floating point operations reduce to $O(n^2 S_{\omega}(\log(d))$.
    If $\matQtilde$ is also returned, the complexity increases to $O(n^2C_{\omega}(\log(n)))$, 
    where $C_{x}(n) := 
    \sum_{k=2}^{\log(n)-2}
    \lpar
        S_{x}(\log(n)-1) - S_{x}(k)
    \rpar$. If $\omega$ is treated as a constant $\omega\approx 2.371$ the corresponding complexities are $O(n^{\omega}), O(n^2d^{\omega-2}),$ and $O(n^{\omega}\log(n))$, respectively.
    \begin{proof}\textcolor{red}{TOPROVE 31}\end{proof}
\end{theorem}
We have the following direct Corollary.

\begin{corollary}
    \label{corollary:tridiagonal_reduction_eigenvalues}
    The eigenvalues of the matrix $\matTtilde$ returned by the algorithm of Theorem \ref{theorem:stable_tridiagonal_reduction} satisfy
    \begin{align*}
        \labs \lambda_i(\matTtilde)-\lambda_i(\matA) \rabs \leq \epsilon \|\matA\|.
    \end{align*}
    \begin{proof}\textcolor{red}{TOPROVE 32}\end{proof}
\end{corollary}




\end{document}
