\section{Proof of \Cref{quasi_apx}}
\label{sec: quasi_apx}

In this section, we provide the proof of \Cref{quasi_apx}, showing that every quasi-bipartite graph with $k$ terminals admits a quality-$(1+\eps)$ contraction-based cut sparsifier of size $k^{O(1/\eps^2)}\cdot (1/\eps)^{O(1/\eps^4)}$. 

%The construction of the sparsifiers works as follows: for each vertex, we will randomly construct an imaginery vertex, which connects to only $\tilde{O}(1/\eps^2)$ terminals. Then we will treat this vertex as the imaginery vertex. We prove that for any terminal cut, the contribution of the vertex will at most go up at most $(1+\eps)$ fraction in expectation if it behaves the same as the imaginery vertex. By concentration bounds, we guarantee the size of each terminal cut only go up $(1+\eps)$ fraction with good probability. Regard to the size of the sparsifier, we contract the vertices whose imaginery vertex has the same profile since they behave the same in every terminal cut. Since each imaginery vertices only have $\tilde{O}(1/\eps^2)$ edges, the total number of possible profile for imagine vertices can be upper bounded by \Cref{quasi_1}.




\subsection{Sparsifying stars by sampling}
\label{subsec: sparsified star}

Throughout, we use the parameter $c = \ceil{100/\eps^2}$. %Let $\uset$ be the collection of all subsets of $T$ with size at most $c$.

For every non-terminal $v$ in $G$, denote by $G_v$ the star induced by all incident edges of $v$. We will construct another star $H_v$ that contains at most $c$ edges to mimick the behaviors of $G_v$, as follows.

Denote by $w$ the total weight of all edges in $G_v$.
We say an edge $e$ is \emph{heavy} iff $w(e)\ge w/c$, otherwise we say it is \emph{light}.
Denote by $h$ the number of heavy edges in $G_v$ and by $w^h$ the total weight of them, so $h\le c$.
We define a distribution $\dset$ on light edges as follows: the probability for a light edge $e$ is 
$\text{Pr}_{\dset}(e)=\frac{w(e)}{w-w^h}.$
If $|V(G_v)|\le c$, then we let $H_v=G_v$.
Otherwise, we sample $c-h$ light edges from $\dset$ with replacement. For each sampled edge, we assign to it a new weight of $\frac{w-w^h}{c-h}$.
The graph $H_v$ simply contains all heavy edges in $G_v$ and all sampled light edges (with their new weight).

For a subset $S\subseteq T$, let $w_S(v)$ be the total weight of edges in $G_v$ connecting $v$ to a vertex in $S$, so $w=w_T(v)$, and similarly let $w'_S(v)$ be the total weight of edges in $H_v$ connecting $v$ to a vertex in $S$. 
In this subsection we will omit $v$ and only write $w_S,w'_S$ for notational simplicity.
The following observation is immediate.

\begin{observation}
\label{obs: property}
Graph $H_v$ contains at most $c$ edges with a total weight of $w$. Moreover, for every subset $S\subseteq T$, $\ex{w'_S}=w_S$.
\end{observation}

We now measure the similarity of graphs $H_v$ and $G_v$ in terms of terminal cuts.
For a subset $S\subseteq T$, if $w'_S>w/2$, then in graph $H_v$, $v$ lies on the $S$ side of the $(S, T\setminus S)$ min-cut, otherwise $v$ lies on the $T\setminus S$ side. 
%
We prove the following claim.

\begin{claim}
\label{clm: difference in contribution}
For a subset $S\subseteq T$ with $w_S<w/2$, $\big(w-2\cdot w_S\big) \cdot \pr{w'_S>w/2} \le \eps \cdot w_S$.
\end{claim}
\begin{proof}
Assume by scaling that $w=1$.
%
We use the following Chernoff bound on negative associated random variables.
	
    \begin{lemma} [\cite{dubhashi1996balls}] \label{chernoff}
		Suppose $X_1,\dots,X_n$ are negatively associated random variables taking values in $\{0,1\}$. Let $X=\sum_{1\le  i\le n}X_i$ and $\mu=\ex{X}$. Then for any $\delta>0$,
		\begin{itemize}
			\item $\pr{X > (1+\delta)\mu} \le e^{-\frac{\delta^2 \mu}{(2+\delta)}}$; and
			\item $\pr{X > (1+\delta)\mu} \le \frac{e^{-\mu}}{(1+\delta)^{1+\delta}}$.
		\end{itemize}
	\end{lemma}
Let $X_e$ be the random variable for the new weight of edge $e$ in $H_v$ (that is, if $e$ is heavy, then $X_e=w(e)$, if $e$ is light and sampled, $X_e=\frac{w-w^h}{c-h}$, and if $e$ is light and not sampled, $X_e=0$). According to our sampling process, variables $\set{X_e}_{e\in E(G_v)}$ are negatively associated.
From \Cref{chernoff}, letting $\alpha = 1/2 - w_S$ and $\delta = \alpha/w_S$, we have
%
	\begin{itemize}
		\item $\pr{w'_S > 1/2} \le e^{-\frac{c \delta^2 w_S}{(2+\delta)}} = e^{-\frac{c \delta \alpha }{(2+\delta)}}$; and
		\item $\pr{w'_S > 1/2} \le \frac{e^{-c w_S}}{(1+\delta)^{1+\delta}} \le \frac{e^{-c w_S}}{\big(\frac{1}{2w_S}\big)^{\frac{1}{2w_S}}}$;
	\end{itemize}
(we have assumed that all edges are light, as the heavy edges have weights unchanged and can only help the concentration property of $w'_S$).
We consider the following three cases. 
%	
	\paragraph{Case 1: $1/2 - \eps/5 < w_S \le 1/2$.} In this case, $(1-2\cdot w_S)<2\cdot\eps/5<\eps\cdot w_S$.
%	
	\paragraph{Case 2: $1/4 < w_S < 1/2 - \eps/5$.} In this case, since $\delta < 1$,
%
	\begin{align*}
	\pr{w'_S > 1/2} \le e^{-\frac{c \delta \alpha}{3}} < e^{- \frac{2 c \alpha^2}{3}} < \frac{3}{2 c \alpha^2} < \frac{\eps}{12\alpha},
	\end{align*}
%	
	where the second inequality is because $w_S < 1/2$; the third one is becuase for any $x>0$, $e^{-x}<1/x$; and the last one is because $c=\ceil{100/\eps^2}$ and $\alpha>\eps/5$. Therefore, $2\alpha \cdot \pr{w'_S > 1/2} < \eps/6 < \eps\cdot w_S$. 
%	
	\paragraph{Case 3: $w_S \le 1/4$.} In this case, $2\cdot w_S \le 1/2$, so $\pr{w'_S > 1/2} \le \frac{e^{-c w_S}}{(1/2w_S)^2} \le \frac{4w_S}{c}$. Therefore, $2\alpha \cdot \pr{w'_S > 1/2} < \frac{12 w_S}{c} < \eps\cdot w_S$, since $\alpha < 1/2$.
\end{proof}

The following statement is a byproduct of the Case 3 above and will be useful later.
%
\begin{claim} \label{prop:v2}
	If $w_S \le 1/(2e)$, $\pr{w'_S > 1/2} \le e^{-\frac{w}{2w_S}}$.
\end{claim}

\subsection{Construction of the cut sparsifier}

\subsubsection*{Step 1. Computing special cuts and keeping important vertices}

For every pair $t,t'$ of terminals in $T$, we compute a min-cut in $G$ separating $t$ and $t'$. This min-cut induces a terminal partition $(S_{t,t'},T\setminus S_{t,t'})$ where $t\in S_{t,t'}$ and $t'\in T\setminus S_{t,t'}$. We call such partitions \emph{special cuts}.

\begin{claim} \label{lem:contribution}
For every vertex $v$ and every subset $S\subseteq T$, there is a special terminal cut $(\tilde S,T\setminus \tilde S)$, such that 
$$\frac{\min \{w_{\tilde S}(v),w(v)-w_{\tilde S}(v)\}}{\mc_G(\tilde S)} \ge \frac{1}{k}\cdot \frac{\min \{w_S(v),w(v)-w_S(v)\}}{\mc_G(S)}.$$
\end{claim}

\begin{proof}
Denote $\alpha=\frac{\min \{w_{S}(v),w(v)-w_{S}(v)\}}{\mc_G(S)}$, so $\frac{w_S(v)}{\mc_G(S)},\frac{w(v)-w_S(v)}{\mc_G(S)}\ge \alpha$. Therefore, there exist terminals $t\in S$ and $t' \notin S$ such that both $(v,t),(v,t')$ have weight at least $\frac{\alpha \cdot \mc_G(S)}{k}$. Consider the special cut $(S_{t,t'},T\setminus S_{t,t'})$ which is the min-cut in $G$ separating terminals $t,t'$, so $\mc_G(S_{t,t'})\le \mc_G(S)$.
Denote $\tilde S=S_{t,t'}$.
On the other hand, since the cut $(\tilde S,T\setminus \tilde S)$ separates $t,t'$, $(v,t),(v,t')$ cannot both lie in $E(v,\tilde S)$ or $E(v,T\setminus \tilde S)$. Therefore, $w_{\tilde S}(v),w(v)-w_{\tilde S}(v)\ge \frac{\alpha \cdot \mc_G(S)}{k}$.
Altogether,
\[\frac{\min\set{w_{\tilde S}(v),w(v)-w_{\tilde S}(v)}}{\mc_G(\tilde S)}\ge \frac{\alpha \cdot \mc_G(S)}{k\cdot \mc_G(\tilde S)}=\frac{\alpha}{k}=\frac{1}{k}\cdot \frac{\min \{w_S(v),w(v)-w_S(v)\}}{\mc_G(S)}.\]
\end{proof}

Let $\eta = \frac{\eps^4}{1000k}$. We call a vertex $v$ \emph{important} if there is a special terminal cut $(S,T\setminus S)$ such that the contribution of $v$ on it (that is, the value of $\mc_{G_v}(S,T\setminus S)$) is at least $(\eta/k)\cdot \mc_G(S)$. There are at most $k \cdot k/\eta  = O(k^3/\eps^4)$ important vertices. We keep all important vertices and their incident edges in the cut sparsifier. That is, important vertices are not contracted with any other vertices.

According to \Cref{lem:contribution}, every non-important vertex $v$ contributes at most $\eta \cdot \mc_G(S)$ to any cut $\mc(S,T\setminus S)$. We handle them next.



\subsubsection*{Step 2. Contracting non-important vertices based on their sparsified stars}

Recall that for each non-important non-terminal $v$, we have constructed in \Cref{subsec: sparsified star} a sparsified star $H_v$ mimicking the original star $G_v$ in $G$. Let $\pi_v$ be the profile of vertex $v$ in graph $H_v$. For each profile $\pi$, we contract all non-important vertices $v$ with $\pi_v=\pi$ into a single node. This completes the construction of the contraction-based sparsifier. Denote the resulting graph by $G'$.

Clearly, $G'$ is obtained from $G$ by contractions only, so it is indeed a contraction-based sparsifier. The next observation shows that it contains at most 
$k^{O(1/\eps^2)}\cdot (1/\eps)^{O(1/\eps^4)}$ vertices.

\begin{observation}
$|V(G')|\le k^{O(1/\eps^2)}\cdot (1/\eps)^{O(1/\eps^4)}$.
\end{observation} 
\begin{proof}
We have shown that the number of important vertices is $O(k^3/\eps^4)$. From \Cref{quasi_1}, the number of different profiles of non-important vertices is at most $\binom{k}{c}\cdot c^{O(c^2)}\le k^{O(1/\eps^2)}\cdot (1/\eps)^{O(1/\eps^4)}$.
\end{proof}

In the rest of this section, we prove that $G'$ is indeed a quality-$(1+O(\eps))$ cut sparsifier.

Fix a subset $S\subseteq T$ and we analyze the difference between $\mc_G(S)$ and $\mc_{G'}(S)$.
For a non-terminal $v$, let $x_v$ and $x'_v$ be the contribution of the vertex $v$ for the cut $S$ in $G$ and $G'$ respectively, so $\mc_G(S) = \sum_v x_v$. 

\begin{observation}
If $v$ is an important vertex, then $x'_v=x_v$. If $v$ is a non-important vertex, then $x'_v$ is either $x_v$ or $w(v)-x_v$. Moreover, $\ex{x'_v} \le (1+\eps)x_v$ and $x_v \le \eta \cdot \mc_G(S)$.
\end{observation}
\begin{proof}
Since we have kept all important vertices in Step 1, their contribution to every terminal min-cut does not change.
For a non-important vertex $v$, since we constructed a sparsified star $H_v$ for $G_v$, and decide the side of $v$ for each terminal cut using the profile of $v$ in $H_v$ instead of $G_v$. 
Therefore, when $w_S(v)\le w(v)/2$ and $w'_S(v)\le w(v)/2$, then $x'_v=x_v$, otherwise $x'_v=w(v)-x_v$, and the difference is $x'_v-x_v=w(v)-2x_v$.
We have proved in \Cref{clm: difference in contribution} that for a subset $S\subseteq T$ with $w_S(v)<w(v)/2$, $\big(w(v)-2\cdot w_S(v)\big) \cdot \pr{w'_S(v)>w(v)/2} \le \eps \cdot w_S(v)$, and so $\ex{x'_v} \le (1+\eps)x_v$. Finally, from our discussion in Step 1, $x_v \le \eta \cdot \mc_G(S)$. 
\end{proof}

We classify all non-important vertices into two types. 
Let $V_1$ be the set that contains all vertices $v$ such that $x_v/w(v) \ge \eps^2/k^2$, and let $V_2$ contain the rest of non-important vertices. 
The proof is completed by the following claims.

\begin{claim}
$\pr{\sum_{v \in V_1} (x'_v - \ex{x'_v}) \le \eps\cdot \mc_G(S)} \le 2^{-2k}$.
\end{claim}
\begin{proof}
We use the following version of Hoeffding's inequality.
\begin{lemma} \label{hoeffding}
	Let $X_1,\dots,X_n$ be independent random variables such that $a_i \le X_i \le b_i$. Let $S_n = \sum_{1\le  i\le n}X_i$. Then for any $t>0$,
	$\pr{S_n - \ex{S_n} \ge t} \le e^{-\frac{2t^2}{\sum_{i=1}^n(b_i-a_i)^2}}.
	$
\end{lemma}

Let $\gamma = \frac{\eps}{10}$.
For any vertex $v$ in $V_1$, $x'_v$ is bounded by $x_v/\gamma$, so we can upper and lower bound $x'_v$ by $b_v = x_v/\gamma$ and $a_v=0$. Also note that 
$\sum_{v\in V_1} (b_v-a_v)^2 \le \frac{1}{\gamma^2} \sum_{v\in V_1} x^2_v.$
Since for any $v \in V_1$, $x_v \le \eta \cdot \mc_G(S)$, $\sum_{v\in V_1} x^2_v \le \eta \cdot(\mc_G(S))^2$, which means 
$$
\sum_{v\in V_1} (b_v-a_v)^2 \le \frac{\eta}{\gamma^2} \cdot (w(v))^2 \le \frac{\eps^2}{10k}\cdot (\mc_G(S))^2.
$$
From \Cref{hoeffding}, 
$$
\pr{\sum_{v \in V_1} (x'_v - \ex{x'_v}) \le \eps\cdot \mc_G(S)} \le e^{-\frac{2\eps^2 (\mc_G(S))^2}{\frac{\eps^2}{10k}\cdot(\mc_G(S))^2}} \le 2^{-2k}.
$$
\end{proof}


\begin{claim}
$\pr{\sum_{v\in V_2} x'_v > \sum_{v\in V_2} x_v + 2\eps\cdot \mc_G(S)} <e^{-2k}$.
\end{claim}
\begin{proof}
We use the approach for proving the Chernoff bound, i.e. applying Markov's inequality on the moment generate function of $\sum_{v \in V_2} x'_v$. Since for every $v \in V_2$, $x_v \le \gamma \cdot w(v)$, from \Cref{prop:v2}, $\pr{x'_v = w(v)-x_v} < e^{-\frac{w(v)}{2x_v}}$. For any $t>0$, 
\begin{align*}
\ex{e^{tx'v}} < e^{tx_v} + e^{-\frac{w(v)}{2x_v}}(e^{tw(v)}-e^{tx_v}) < e^{tx_v}(1+e^{-\frac{w(v)}{2x_v}}(e^{tw(v)}-1)) < e^{tx_v + e^{-\frac{w(v)}{2x_v}}(e^{tw(v)}-1)}.
\end{align*}

Let $t = \frac{2k}{\eps \cdot\mc_G(S)}$. Since $x_v \le \eta \cdot\mc_G(S)$, $t \le \frac{2 \eta k}{\eps x_v}$, which means $tw(v) \le \frac{2\eta k w(v)}{\eps x_v} \le \frac{w(v)}{4x_v}$. This means $e^{-\frac{w(v)}{2x_v}}(e^{tw(v)}-1)< e^{-\frac{w(v)}{4x_v}}$. Since $\frac{w(v)}{x_v} \ge (1/\gamma)$ and $\gamma=\frac{\eps}{10}$, we have $\frac{w(v)}{x_v}>4( \log (w(v))-\log(x_v) + \log (1/\eps))$. So $e^{-\frac{w(v)}{4x_v}}<\frac{\eps x_v}{w(v)}$. Therefore, if $tw(v) > 1$,
$$
\ex{e^{tx'_v}}<e^{tx_v + e^{-\frac{w(v)}{2x_v}}(e^{tw(v)}-1)}<e^{tx_v + \eps x_v/w(v)}<e^{(1+\eps)tx_v}.
$$
On the other hand, if $tw(v) \le 1$, $e^{tw(v)-1} < 2tw(v)$. Since $\frac{w(v)}{x_v} \ge (1/\gamma)$ and $\gamma=\frac{\eps}{10}$, we have $\frac{w(v)}{2x_v} > \log w(v) - \log x_v + \log (1/\eps)$, which means $e^{-\frac{w(v)}{2x_v}}<\eps\cdot \frac{x_v}{w(v)}$. Therefore,
$$
\ex{e^{tx'_v}}<e^{tx_v + e^{-\frac{w(v)}{2x_v}}(e^{tw(v)}-1)}<e^{tx_v + \eps\frac{x_v}{w(v)} \cdot tw(v)} < e^{(1+\eps)tx_v}.
$$

Therefore in either case, we have $\ex{e^{tx'_v}}<e^{(1+\eps)tx_v}$, and so
$$\ex{e^{t\sum_{v\in V_2} x'_v}} < e^{(1+\eps)t\sum_{v\in V_2} x_v}.$$ 
Therefore,
\[
\begin{split}
\pr{\sum_{v\in V_2} x'_v > \sum_{v\in V_2} x_v + 2\eps \cdot\mc_G(S)} & < e^{(1+\eps)t\sum_{v\in V_2} x_v-t(\sum_{v \in V_2} x_v + 2\eps \cdot\mc_G(S))}\\
& < e^{-\eps t \cdot\mc_G(S)}<e^{-2k}.
\end{split}
\]
\end{proof}

Therefore, $\pr{\sum_v x'_v > (1+3\eps)\cdot\mc_G(S)} < 2^{-2k+1}$. Taking the union bound over all subsets $S\subseteq T$, with probability at least $1-2^{k-1}$, $G'$ is indeed a quality-$(1+3\eps)$ cut sparsifier.






























\iffalse
Given a vertex $v$, let $w(v)$ as the total weight of the edges incident on $v$. For any set of terminals $S$, let $w_S(v)$ be the total weight of edges between $v$ and $S$. We construct the imaginery vertex as follows: let $c = \frac{100}{\eps^2}$. We say an edge $e$ is large is the weight $w(e)$ is at least $w(v)/c$. Suppose we have $n(v)$ large edges, and the total weight of the large edge is $\ell(v)$, then we randomly sample $c-n(v)$ small edges where the probability of each small edges $e$ being sampled is $w(e)(c-n(v))/(w(v)-\ell(v))$. We reweight each sampled edge to $(w(v)-\ell(v))/(c-n(v)$, and delete unsampled edges. We call the resulting vertex $v'$. Note that $v'$ has exactly $c$ edges and $w(v')=w(v)$. Moreover, for any $S$, $\ex{w_S(v')}=w_S(v)$.

We will use the behave of $v'$ instead of $v$ when calculating the cut size of each terminal cut. In other word, for any $S$, if $w_S(v')>w(v')/2$, then we will include $v$ in the cut, otherwise we exclude $v$ in the cut. The contribution of $v$ changes only when $w_S(v)>w(v)/2$ but $w_S(v')<w(v')/2$ or $w_S(v)<w(v)/2)$ but $w_S(v')>w(v')/2$. We only consider the case when $w_S(v)<w(v)/2$, the other case is covered by terminal cut $T \setminus S$. We show that in expectation, the contribution of $v$ in $S$ is at most $(1+\eps)w_S(v)$. Without lose of generality, we assume $w(v) = 1$ since all the probabilities does not change under normalization.

Note that if $w_S(v')>w(v')/2$, the contribution of $v$ in the cut $S$ will be $1-w_S(v)$ instead of $w_S(v)$, the contribution increased by $1-2w_S(v)$. Therefore, it is sufficient to prove that $(1-2w_S(v)) \cdot \pr{w_S(v')>w(v')/2} \le \eps w_S(v)$, which we do next. 

\begin{claim}
$(1-2w_S(v)) \cdot \pr{w_S(v')>w(v')/2} \le \eps w_S(v)$
\end{claim}
\begin{proof}
We will use the following Chernoff bound on negative correlated random variables.

    \begin{lemma} [\cite{dubhashi1996balls}] \label{chernoff}
	Suppose $X_1,\dots,X_n$ are negatively correlated random variables taking values in $\{0,1\}$. Let $X=\sum_{1\le  i\le n}X_i$ and $\mu=\ex{X}$. Then for any $\delta>0$,
	\begin{itemize}
		\item $\pr{X > (1+\delta)\mu} \le e^{-\delta^2 \mu / (2+\delta)}$; and
		\item $\pr{X > (1+\delta)\mu} \le \frac{e^{-\mu}}{(1+\delta)^{1+\delta}}$.
	\end{itemize}
\end{lemma}
Let $\alpha = 1/2 - w_S(v)$ and $\delta = \alpha/w_S(v)$, by \Cref{chernoff}

\begin{itemize}
    \item $\pr{w_S(v') > 1/2} \le e^{-c \delta^2 w_S(v) / (2+\delta)} \le e^{-c \delta \alpha / (2+\delta)}$;
    \item $\pr{w_S(v') > 1/2} \le \frac{e^{-c w_S(v)}}{(1+\delta)^{1+\delta}} \le \frac{e^{-c w_S(v)}}{(1/2w_S(v))^{1/2w_S(v)}}$;
\end{itemize}

since large edges only help concentration.

Base on $w_S(v)$, we consider three cases. 

\paragraph{Case 1: $1/2 - \eps/5 < w_S(v) \le 1/2$.} In this case, $(1-2w_S(v))<2\eps/5<\eps w_S(v)$.

\paragraph{Case 2: $1/4 < w_S(v) < 1/2 - \eps/5$.} In this case, we set $\delta < 1$, we have 

\begin{align*}
    \pr{w_S(v') > 1/2} \le e^{-c \delta \alpha / 3} < e^{- 2 c \alpha^2 / 3} < \frac{3}{2 c \alpha^2} < \frac{\eps}{12\alpha}
\end{align*}

where the second inequality is because $w_S(v) < 1/2$, the third one is becuase for any $x>0$, $e^{-x}<1/x$, the last one is because $c=100/\eps^2$ and $\alpha>\eps/5$. Therefore, $2\alpha \cdot \pr{w_S(v') > 1/2} < \eps/6 < \eps w_S(v)$. 

\paragraph{Case 3: $w_S(v) \le 1/4$} In this case, $2w_S(v) \le 1/2$, so we have

\begin{align*}
    \pr{w_S(v') > 1/2} \le \frac{e^{-c w_S(v)}}{(1/2w_S(v))^2} \le \frac{4w_S(v)}{c}
\end{align*}

Therefore, $2\alpha \cdot \pr{w_S(v') > 1/2} < 12 w_S(v)/c < \eps w_S(v)$ since $\alpha < 1/2$.
\end{proof}

The following statement is a byproduct of the analysis and it will be useful in the later analysis.

\begin{claim} \label{prop:v2}
    If $w_S(v) \le 1/2e$, $\pr{w_S(v') > 1/2} \le e^{-\frac{w(v)}{2w_S(v)}}$
\end{claim}

\subsection{Concentration}
In this section, we give the way of contructing the cut sparsifier. By the last section, if we treat each vertex as its imaginary vertex, then for each terminal cut, the size will go up by a factor of at most $(1+\eps)$ in expectation, thus we need to prove the concentration bound for each cut. There are two difficulties, the first is that it is possible that for some cut, all the contribution is from a small number of vertices. To solve this, we will not use the imaginary vertices for these vertices, and prove that the total number of this kind of vertices are bounded. The second is that there might be some vertex $v$ and terminal cut $S$ such that $w_S(v)$ is too small compare to $w(v)$, this means that the variance of $w_S(v)$ is too high so that some common concentration bound (like Chernoff bound) is not useful. For these vertices, we will follow the proof of Chernoff bound and directly analyze the moment generation function for these vertices. 

We first show that there are not a lot of vertices that are important to some terminal cut. Define special terminal cuts $S_1,\dots, S_k$ as follows: let $S_1$ be the smallest terminal cut (break tie arbitrarily). We say $S_1$ is the type $1$ cut. For any $2 \le i \le S_k$, let $S_i$ be the smallest terminal cut that is not type $i-1$ cut (break tie arbitrarily), and we say a cut $S$ is type $i$ if for any two terminals $t_1, t_2$ on different sides of $S$, there is a cut $S_j$ in $\{S_1,\dots, S_i\}$ such that also split them. If during the definition, there is an $i$ such that all terminal cuts are type $i$, then we stop the process. However, it is clear that the process will stop before the $k^{th}$ step. We prove the following lemma:

\begin{lemma} \label{lem:contribution}
    For any vertex $v$ and any $S$, if $\alpha = \frac{\min \{w_S(v),w(v)-w_S(v)\}}{w(S)}$, then there is a special terminal cut $S_j$ such that $\frac{\min \{w_{S_j}(v),w(v)-w_{S_j}(v)\}}{w(S_j)} \ge \alpha/k$.
\end{lemma}

\begin{proof}
    By definition, we have $w_S(v)/w(S)$ and $(w(v)-w_S(v))/w(S)$ both at least $\alpha$, which means that there is a terminal $t_1 \in S$ and a terminal $t_2 \notin S$ that the weight of $(v,t_1)$ nad $(v,t_2)$ are both at least $\alpha w(S)/k$. Suppose $S$ is a type $i$ cut but not a type $i-1$ cut, by definition, there is a special cut $S_j$ where $j \le i$ such that $S_j$ seperates $t_1$ and $t_2$, which means $w_{S_j}(v)/w(S_j)$ and $(w(v)-w_{S_j}(v))/w(S_j)$ are both at least $\alpha w(S)/k$. On the other hand, since $S$ is not type $i-1$ cut, we have $w(S_j) \le w(S_i) \le w(S)$. Therefore, both $w_{S_j}(v)/w(S_j)$ and $(w(v)-w_{S_j}(v))/w(S_j)$ are at least $\alpha w(S_j)/k$.
\end{proof}

Let $\eta = \frac{\eps^4}{1000k}$ We call a vertex $v$ important if there is a special terminal cut $S_i$ such that the contribution of $v$ on the cut $S_i$ is at least $\eta/k w(S_i)$. There are at most $k \cdot k/eta  = poly(k,\eps))$ important vertices. By \Cref{lem:contribution}, for any $v$ that is not important, the contribution of $v$ for any cut $S$ is at most $\eta w(S)$. 

We construct the cut sparsifier as follows: For any vertex that is not important, we randomly construct an imaginary vertex. Then we contract all vertices that has the same profile for their imaginary vertices. Note that each imaginary vertex has at most $1/\eps^2$ edges, by \Cref{quasi_1}, imaginary vertices have at most $\binom{k}{1/\eps^2} (1/\eps^2)^{1/\eps^4} = k^{O(1/\eps^2)}\eps^{-O(1/\eps^4)}$ profiles. Thus the size of the sparsifier is at most $poly(k,\eps) + k^{O(1/\eps^2)}\eps^{-O(1/\eps^4)} = k^{O(1/\eps^2)}\eps^{-O(1/\eps^4)}$. In the rest of this section, we prove that it is indeed a $(1+O(\eps))$-cut sparsifier.

We fix a terminal cut $S$ and analyze the probabilty that the cut size in the sparsifier is increased by a factor of at most $(1+O(\eps))$. Denote $G'$ as the the sparsifier. For any vertex $v$, let $x_v$ and $x'_v$ (we omit $S$ in the definition because we are focus on a fixed cut $S$) be the contribution of the vertex $v$ for the cut $S$ in $G$ and $G'$ respectively. By definition, the size of $S$ in $G$ is $w(S) = \sum_v x_v$. If $v$ is an important vertex, then $x_v = x'_v$. otherwise $x'_v$ is either $x_v$ or $w(v)-x_v$, and we guarantee that $\ex{x'_v} \le (1+\eps)x_v$ and $x_v \le \eta w(S)$.

We partition $v$ into two types. Let $\gamma = \frac{\eps}{10}$, $V_1$ be the set of unimportant vertices such that $x_v/w(v) \ge \eps^2/k^2$, and $V_2$ be the set of the rest of unimportant vertices. We first bound $\sum_{v \in V_1} x'_v$, we use the following version of Hoeffding's inequality:

\begin{claim} \label{hoeffding}
    Let $X_1,\dots,X_n$ be independent random variables such that $a_i \le X_i \le b_i$. Let $S_n = \sum_{1\le  i\le n}X_i$. Then for any $t>0$,
    $\pr{S_n - \ex{S_n} \ge t} \le e^{-\frac{2t^2}{\sum_{i=1}^n(b_i-a_i)^2}}.
    $
\end{claim}

For any vertex $v$ in $V_1$, $x'_v$ is bounded by $x_v/\gamma$, so we can upper and lower bound $x'_v$ by $b_v = x_v/\gamma$ and $a_v=0$, and 
$$
    \sum_{v\in V_1} (b_v-a_v)^2 \le \frac{1}{\gamma^2} \sum_{v\in V_1} x^2_v
$$

Note that for any $v \in V_1$, $x_v \le \eta w(S)$, we have $\sum_{v\in V_1} x^2_v \le \eta (w(S))^2$, which means 
$$
    \sum_{v\in V_1} (b_v-a_v)^2 \le \frac{\eta}{\gamma^2} w^2(v) \le (\eps^2/10k) (w(S))^2
$$

By \Cref{hoeffding}, we have 
$$
    \pr{\sum_{v \in V_1} (x'_v - \ex{x'_v}) \le \eps w(S)} \le e^{-\frac{2\eps^2 (w(S))^2}{(\eps^2/10k)(w(S)^2}} \le 2^{-2k}
$$

Next, we bound $\sum_{v \in V_2} x'_v$. We follow the framwork of the proof of Chernoff bound, i.e. use Markov's inequality on the moment generate function of $
\sum_{v \in V_2} x'_v$. Since for any $v \in V_2$, we have $x_v \le \gamma w(v)$, by \Cref{prop:v2}, $\pr{x'_v = w(v)-x_v} < e^{-\frac{w(v)}{2x_v}}$. For any $t>0$, we have 
\begin{align*}
    \ex{e^{tx'v}} < e^{tx_v} + e^{-\frac{w(v)}{2x_v}}(e^{tw(v)}-e^{tx_v}) < e^{tx_v}(1+e^{-\frac{w(v)}{2x_v}}(e^{tw(v)}-1)) < e^{tx_v + e^{-\frac{w(v)}{2x_v}}(e^{tw(v)}-1)}
\end{align*}

Let $t = \frac{2k}{\eps w(S)}$. Since $x_v \le \eta w(S)$, $t \le \frac{2 \eta k}{\eps x_v}$, which means $tw(v) \le \frac{2\eta k w(v)}{\eps x_v} \le \frac{w(v)}{4x_v}$. This means $e^{-\frac{w(v)}{2x_v}}(e^{tw(v)}-1)< e^{-\frac{w(v)}{4x_v}}$. Since $\frac{w(v)}{x_v} \ge (1/\gamma)$ and $\gamma=\frac{\eps}{10}$, we have $\frac{w(v)}{x_v}>4( \log (w(v))-\log(x_v) + \log (1/\eps))$. So $e^{-\frac{w(v)}{4x_v}}<\frac{\eps x_v}{w(v)}$. Therefore, if $tw(v) > 1$, we have 
$$
    \ex{e^{tx'_v}}<e^{tx_v + e^{-\frac{w(v)}{2x_v}}(e^{tw(v)}-1)}<e^{tx_v + \eps x_v/w(v)}<e^{(1+\eps)tx_v}
$$
On the other hand, if $tw(v) \le 1$, we have $e^{tw(v)-1} < 2tw(v)$. Since $\frac{w(v)}{x_v} \ge (1/\gamma)$ and $\gamma=\frac{\eps}{10}$, we have $\frac{w(v)}{2x_v} > \log w(v) - \log x_v + \log (1/\eps)$, which means $e^{-\frac{w(v)}{2x_v}}<\eps\frac{x_v}{w(v)}$, therefore,
$$
    \ex{e^{tx'_v}}<e^{tx_v + e^{-\frac{w(v)}{2x_v}}(e^{tw(v)}-1)}<e^{tx_v + \eps\frac{x_v}{w(v)} \cdot (tw(v))} < e^{(1+\eps)tx_v}
$$

Therefore on either case, we have $\ex{e^{tx'_v}}<e^{(1+\eps)tx_v}$, which means 
$$\ex{e^{t\sum_{v\in V_2} x'_v}} < e^{(1+\eps)t\sum_{v\in V_2} x_v}$$ 
which means 
$$
\pr{\sum_{v\in V_2} x'_v > \sum_{v\in V_2} x_v + 2\eps w(S)} < e^{(1+\eps)t\sum_{v\in V_2} x_v-t(\sum_{v \in V_2} x_v + 2\eps w(S))}<e^{-\eps t w(S)}<e^{-2k}
$$

Therefore, $\pr{\sum_v x'_v > (1+3\eps)w(S)} < 2^{-2k+1}$, which means the cut size of on $S$ does not increase by a factor of $(1+3\eps)$ with probability at least $1-2^{-2k+1}$, and take the union bound on all terminal cut, with probability at least $1-2^{k-1}$, the sparsifier is indeed a $(1+3\eps)$-cut sparsifier.

\fi
